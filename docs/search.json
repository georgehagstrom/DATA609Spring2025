[
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructor",
    "section": "",
    "text": "George Hagstrom, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Princeton University where I development mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth‚Äôs Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ‚Äôomics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You‚Äôre encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\n\nThis book provides a strong practical introduction to Linear Algebra. We are primarily using the later parts of the book to learn about applications of least squares. However, you are encouraged to use earlier parts of the book to review linear algebra if needed.\n\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\n\nThis is an excellent book on practical applications of convex optimization. We will use the first half of the book, which focuses on identifying convex optimization problems.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\nThis is a good introduction to neural networks and deep learning.\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\n\nThis textbook provides a concise introduction to optimization for machine learning.\n\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\nThis is free textbook covers a large number of topics that are of relevance to data science, often from a slightly different perspective than standard treatments. It provides an excellent introduction to Bayesian Inference and Neural Networks, but applies ideas from information theory too.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "This class will involve a number of computational homework assignments. These can be completed in a variety of different programming languages. I recommend that you use python or Julia to complete the assignments, but it is also possible to use R.\nGuide to Base Languages:\n\nPython\n\nscipy.linalg is a useful package for linear algebra in python\nCVXPY is the implementation of the CVX package in python\npytorch is a package for neural networks in python\n\nJulia\n\nCONVEX.jl is the implementation of the CVX package in Julia\ntorch.jl provides torch functionality for deep learning in julia.\nflux.jl is another good option for deep learning.\n\nR\n\nCVXR is the implementation of the CVX package in R.\ntorch for R Can be used to implement neural networks in R",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "posts/2025-02-02-Week-2-Least-Squares.html",
    "href": "posts/2025-02-02-Week-2-Least-Squares.html",
    "title": "Week 2- Least Squares",
    "section": "",
    "text": "This week we will talk about the most basic and most important class of optimization problems, least squares optimization. Whenever you fit a linear regression, you are solving a least squares problems, and least squares methods have applications across all quantitative fields. Although least squares methods are simple, they should not be dismissed- the fact that we can solve them rapidly, reliably means that it is useful to know how to formulate them in applied situations and how to solve them.\nIn this week we will go over the basics and a few simple applications, while the two weeks after this will explore data fitting examples and how to handle more complicated cases.\nYour first homework assignment will be due next Sunday.\nHere are more details on what you need to do this week:\n\nGo through module 2 and complete the week 2 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nContinue working on Lab 1"
  },
  {
    "objectID": "posts/2025-01-27-Meetup-1-Slides.html",
    "href": "posts/2025-01-27-Meetup-1-Slides.html",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "",
    "text": "Meetup 1 is tonight at 6:45PM. We are going to talk about what optimization is, some of its applications, the course structure, and some very basic facts about its theory.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 1 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "href": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "title": "Coding Vignette- Linear Algebra in Numpy",
    "section": "",
    "text": "I‚Äôm not sure how many of you have had experience with using python for linear algebra, so I decided to make a short video that goes over the basics of numpy (you are free to use R or Julia as well). I also solve a problem where I re-express the formula for the pseudoinverse of a matrix using the singular value decomposition of that matrix. I did it sort of to show how you might write out a long math expression in markdown, and to talk about matrix algebra and the properties of the SVD. This calculation is quite a bit more invovled than what I am asking on the homework, so don‚Äôt be intimidated.\nClick here to watch the video on youtube\nYou may also find the numpy documentation helpful: Click Here\nI have also included the ipython notebook"
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Constrained Least Squares and Applications",
    "section": "",
    "text": "Homework\nHomework 2 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving Least Squares problems with Linear Equality Constraints\nApplications to Portfolio Optimization\nKalman Filters and Recursive State Estimation\n\n\n\nReadings\nChapters 16 and 17 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "4 - Constrained Least Squares and Applications"
    ]
  },
  {
    "objectID": "modules/module13.html",
    "href": "modules/module13.html",
    "title": "Module 13: Local Optimization Algorithms and Neural Networks",
    "section": "",
    "text": "Homework\nHomework 7 is due in two weeks, Sunday at midnight\n\n\nLearning Objectives\n\nNeural Network Learning and Backpropagation\nGradient Descent\nNewton‚Äôs Method\nConvergence Rates and Condition Number\nAcceleration Using Momentum\n\n\n\nReadings\nChapter 9 of Convex Optimization\nChapter 8.1-8.4 of Introduction to Algorithms for Data Mining Machine Learning\nOr alternatively deeplearningbook Chapter 6",
    "crumbs": [
      "Topics",
      "13 - Local Optimization and Neural Networks"
    ]
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3: Statistical Applications of Least Squares",
    "section": "",
    "text": "Deliverables\nHomework 2 will be due two Sundays from now.\n\n\nLearning Objectives\n\nLeast Squares and Linear Regression\nGeneralized Least Squares\nRecursive/Online Least Squares\nRegularization and Multi-Objective Least Squares\n\n\n\nReadings\nChapters 13 and 15 of Introduction to Applied Linear Algebra\nSections 4.2 and 4.5 of Introduction to Algorithms for Data Mining and Machine Learning",
    "crumbs": [
      "Topics",
      "3 - Least Squares and Statistics"
    ]
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Convex Sets II",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nGeneralized Inequalities\nSupporting and Separating Hyperplanes\nDual Cones\n\n\n\n\nSections 2.4-2.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module6.html#homework",
    "href": "modules/module6.html#homework",
    "title": "Module 6 - Convex Sets II",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nGeneralized Inequalities\nSupporting and Separating Hyperplanes\nDual Cones\n\n\n\n\nSections 2.4-2.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, splines, and interpolation\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module10.html#homework",
    "href": "modules/module10.html#homework",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, splines, and interpolation\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9: Convex Optimization Problems and Disciplined Convex Programming",
    "section": "",
    "text": "Homework\nHomework 5 is due next week, Sunday at midnight.\n\n\nLearning Objectives\n\nDefinition of Convex Optimization Problem\nProblem Transformations\nImportant Subclasses of Convex Optimization Problems\nDiscilined Convex Programming and CVX\nVector Optimization and Scalarization\n\n\n\nReadings\nSections 4.1-4.4, 4.6-4.7 of Convex Optimization\nCVX users guide, read one of these: -python -R -julia",
    "crumbs": [
      "Topics",
      "9 - Convex Optimization and Disciplined Convex Programming"
    ]
  },
  {
    "objectID": "modules/module12.html",
    "href": "modules/module12.html",
    "title": "Module 12: Duality and Classifiers",
    "section": "",
    "text": "Learning Objectives\n\nDual Formulation of Optimization Problems\nLagrange Multipliers\nLinear and Nonlinear Classifiers, LDA\nSupport Vector Machines\n\n\n\nReadings\nSections 8.1, 8.2, 8.6, 8.7, 8.8 Convex Optimization",
    "crumbs": [
      "Topics",
      "12 - Duality and Classifiers"
    ]
  },
  {
    "objectID": "modules/module5.html",
    "href": "modules/module5.html",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\nOperations that Preserve Convexity of Sets\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "modules/module5.html#homework",
    "href": "modules/module5.html#homework",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\nOperations that Preserve Convexity of Sets\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "This website would not be possible without the quarto package. This course draws inspiration from several open educational resources on the internet, including two books and accompanying materials by Stephen Boyd and Lieven Vandenberghe: Introduction to Applied Linear Algebra and Convex Optimization",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "meetups/Meetup-2/Untitled.html",
    "href": "meetups/Meetup-2/Untitled.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb = np.random.randn(10)\n\n\na @ b\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nb.T @ a.T\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.zeros((10,10))\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\nnp.linalg.norm(np.linalg.inv(a) @ a - np.eye(10))\n\n2.570038214105557e-15\n\n\n\nA = a\n\n\\[ Ax = b \\]\n\nx = np.linalg.lstsq(A,b)[0]\n\n/tmp/ipykernel_2637623/3825200175.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  x = np.linalg.lstsq(A,b)[0]\n\n\n\nA @ x - b\n\narray([-2.22044605e-15,  3.35842465e-15,  5.55111512e-15, -5.38458167e-15,\n        1.77635684e-15,  2.99760217e-15,  2.66453526e-15, -2.38697950e-15,\n        6.21724894e-15,  4.66293670e-15])\n\n\n\\[\nA = U \\Sigma V^T\n\\]\n\\(A\\) is \\(m\\times n\\), \\(U\\) is \\(m\\times m\\) \\(\\Sigma\\) \\(m \\times n\\) \\(V\\) is \\(n \\times n\\)\n\\(U\\) and \\(V\\) are orthonormal matrices, which means:\n\\(U U^T = I\\), \\(V V^T = I\\)\n\\(\\Sigma\\) is 0 except for the diagonals which are the singular values of \\(A\\)\n\nA = np.random.randn(100,10)\n\n\nsvd_A = np.linalg.svd(A)\n\n\nsvd_A[1]\n\narray([13.1081352 , 11.81959892, 11.15343028, 10.83354228, 10.23738251,\n        9.27330655,  9.07488313,  8.12095293,  7.844576  ,  6.77092389])\n\n\n\nU = svd_A[0]\nV = svd_A[2].T\n\n\nnp.linalg.norm(U @ U.T -  np.eye(100))\n\n4.986008699316736e-15\n\n\n\nnp.linalg.norm(V @ V.T -  np.eye(10))\n\n2.4656943504538092e-15\n\n\n\\[\nA^+ = (A^T A)^{-1} A^T\n\\]\n\\[A = U \\Sigma V^T\\]\n\\[\nA^+ = (V \\Sigma^T U^T U \\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} V^T V \\Sigma^T U^T\n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} \\Sigma^T U^T\n\\]\n\\[\nA^+ = V\\Sigma^+ U^T\n\\]\n\\[\nx = V\\Sigma^+ U^T b\n\\]\n\\[\n\\Sigma^+ U^T b = V^T x\n\\]\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = V^T x\n\\]\n\\(y= V^Tx\\)\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = y\n\\]\nx = V @ np.linalg.solve(S.T @ S, U.T @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#week-summary",
    "href": "meetups/Meetup-2/meetup-2.html#week-summary",
    "title": "Meetup 2: Least Squares",
    "section": "Week Summary",
    "text": "Week Summary\n\nStarting on Least Squares\nChapter 12 of VMLS\nKeep working on Lab 1, due this Sunday at midnight"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "href": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "title": "Meetup 2: Least Squares",
    "section": "System of Linear Equations",
    "text": "System of Linear Equations\n\nWhen can we solve: \\[ A\\mathbf{x} = \\mathbf{b}\\,?\\]\n\\(A\\) is an \\(m\\times n\\) matrix\n\\(\\mathbf{x}\\) is an \\(n\\)-vector\n\\(\\mathbf{b}\\) is an \\(m\\)-vector"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m=n\\) square\nExact solution if rows/columns are independent \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     &    \\cdots   & \\vdots \\\\\n                    \\vdots & \\cdots & \\cdots & \\vdots \\\\\n                    a_{n1} & \\cdots & \\cdots & a_{nn}\n                    \\end{pmatrix}\n                    \\]\n\\(\\mathbf{x} = A^{-1}\\mathbf{b}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&lt;n\\) wide\nGenerally multiple solutions because dependent columns \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     & \\cdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & \\cdots & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]\nCommon ML situation (more parameters than data points)\nParticular solution plus the null space"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&gt;n\\) tall\nGenerally no exact solution \\[ A = \\begin{pmatrix} a_{11} & \\cdots &  a_{1n} \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Inspiration for Least Squares",
    "text": "Inspiration for Least Squares\n\nCan‚Äôt solve? Minimize the error \\[ \\mathbf{r} = A\\mathbf{x}-\\mathbf{b}\\]\nOptimization problem is: \\[\\textrm{Find:}\\quad \\min_{\\mathbf{x}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\\]\nThe \\(\\mathbf{x}\\) that minimizes this objective (the squared residual) is the least squares solution\nLinear regression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that:"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that\n\nmatrixcalculus.org\nTake derivatives, work with indices"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nGet System of Linear Equations: \\[ A^TA\\mathbf{x} = A^T\\mathbf{b} \\]\n\\(A^TA\\) is an \\(n\\times n\\) symmetric matrix called the Gram matrix\nInvertible if columns of \\(A\\) are independent:\n\n\\[ \\mathbf{x} = \\left(A^T A\\right)^{-1}A^T\\mathbf{b}\\]\n\nThis is the exact formula for the least squares solution"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#properties",
    "href": "meetups/Meetup-2/meetup-2.html#properties",
    "title": "Meetup 2: Least Squares",
    "section": "Properties",
    "text": "Properties\n\nMorse-Penrose Pseudoinverse: \\[A^+ = (A^TA)^{-1}A^T\\]\n\\(n\\times m\\) matrix\nIf \\(A\\) is invertible, \\(A^+ = A^{-1}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Linear Regression",
    "text": "Applications: Linear Regression\n\nSuppose you have \\(m\\) observations, each with \\(n\\) features\n\\(\\mathbf{a}_{i}\\) is \\(i\\)th obs\nWant to predict the variable \\(\\mathbf{b}\\) using an Affine function: \\[ \\mathbf{a}_i^T\\mathbf{x} +c = b_i \\]\nor\n\\[\n  \\sum_{j=1}^m a_{ij} x_j + c = b_i\n  \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAugmented matrix \\(A\\) and coefficients \\(\\mathbf{x}\\): \\[\nA = \\begin{pmatrix} a_{11} & \\cdots & a_{1n} & 1 \\\\\n                  \\vdots & \\cdots & \\vdots & 1 \\\\\n                  a_{m1} & \\cdots & a_{mn} & 1\n                  \\end{pmatrix}\n\\] \\[\n\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n & c \\end{pmatrix}^T\n\\]\nNow get standard least squares problem:\n\n\\[ \\min_{\\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n\n\n\n\n\n\nGroup\n\n\n\n\n1\n18-25, play sports\n\n\n2\n26-30, play sports\n\n\n3\n31-35, play sports\n\n\n4\n36-40, play sports\n\n\n5\n41-45, play sports\n\n\n6\n18-25, no sports\n\n\n7\n26-30, no sports\n\n\n8\n31-35, no sports\n\n\n9\n36-40, no sports\n\n\n10\n41-45, no sports"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n4 different marketing channels\n\n\n\n\n\n\nChannel\n\n\n\n\n1\nInstagram\n\n\n2\nWine Mag\n\n\n3\nFacebook\n\n\n4\nUFC"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)\n\n\n\n   Instagram  Wine Mag  Facebook   UFC      Demo\n0        3.0       0.1       0.4  6.00   18-25 S\n1        2.5       0.3       0.6  6.00   26-30 S\n2        2.3       0.4       1.0  5.50   31-35 S\n3        1.8       0.6       1.3  5.00   36-40 S\n4        1.0       0.6       2.0  4.50   41-45 S\n5        3.3       0.8       0.5  0.20  18-25 NS\n6        2.6       1.1       0.8  0.12  26-30 NS\n7        2.2       1.3       1.1  0.10  31-35 NS\n8        1.4       2.3       1.3  0.40  36-40 NS\n9        1.2       4.2       1.7  0.20  41-45 NS"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nProblem: spend money to get \\(\\mathbf{v}_{dem}\\) impressions\nDecision variable \\(\\mathbf{s}\\) is spending in each channel\nSolve least squares problem: \\[\n\\min_{\\mathbf{s}} \\|R\\mathbf{s} - \\mathbf{v}_{dem}\\|^2\n\\]\n\\(R\\mathbf{s}\\) is views"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform\n\n\nv_dem = np.ones(10)\n\ns = np.linalg.lstsq(R,v_dem)[0]\n\ns_dat = pd.DataFrame(s, columns = [\"Spending\"])\ns_dat[\"Channel\"] = ['Instagram','Wine Mag','Facebook','UFC'] \n\n(\n  ggplot(s_dat,aes(x=\"Channel\",y=\"Spending\"))\n  + geom_point(size=5) \n  + theme_bw(base_size=14)\n)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget Issues",
    "text": "Marketing Budget Issues\n\nNeed Constraints\n\nNo negative money in adds: \\(\\mathbf{s} \\geq 0\\)\nNo infinite budget: \\(\\sum_{i}s_i \\leq s_{max}\\)\n\nNeed a better objective function\n\nWeird to target precise \\(\\mathbf{v}_{dem}\\)\nBetter to have different value for each ad impression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\n\n\n\n\nUrban Transit\nInternet\nKnow when people enter and leave\nKnow path they took\nHow long does it take to cross each link?"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nFor trip \\(j\\) vector \\(\\mathbf{r}_j\\)\n\n\\(r_{ij} = 1\\) if \\(i\\)th ‚Äúlink‚Äù traversed on \\(j\\)th trip\n\\(r_{ij} = 0\\) otherwise\nMatrix \\(R\\) contains \\(r_{ij}\\)\n\nFor \\(t_j\\) is the time taken on trip \\(j\\)\nDecision variables \\(d_i\\) is the time taken on the \\(i\\)th link\nTime is \\(\\sum r_{ij}d_i\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nMinimize Least Squares Error:\n\n\\[\n\\textrm{Find:}\\quad \\min_{\\mathbf{d}} \\|R\\mathbf{d} - \\mathbf{t}\\|^2\n\\]\n\nMany similar problems in optimization theory\nThis one is the simplest"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Numerical Issues With Least Squares",
    "text": "Numerical Issues With Least Squares\n\nSuppose you need to solve a system of linear equations: \\[\nA\\mathbf{x} = \\mathbf{b}\n\\]\nWhich of these is better?\n\nnp.linalg.inv(A) @ b\nnp.linalg.solve(A, b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "href": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "title": "Meetup 2: Least Squares",
    "section": "Matrix Inverses",
    "text": "Matrix Inverses\n\nConsider Singular Value Decomposition of \\(A\\): \\[\nA = U\\Sigma V^T\n\\]\n\\(U\\) and \\(V\\) are orthonormal matrices\n\\(\\Sigma\\) contains the singular values \\[\n\\Sigma = \\begin{bmatrix}\n  \\sigma_{1} & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "href": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "title": "Meetup 2: Least Squares",
    "section": "Condition Number and Accuracy",
    "text": "Condition Number and Accuracy\n\nAccuracy of np.linalg.inv(A) @ b related to \\(\\frac{\\sigma_{max}}{\\sigma_{min}}\\)\nThis is called the ‚Äúcondition number‚Äù of the matrix \\(A\\)\n\n\n%timeit np.linalg.inv(A) @ b\n%timeit np.linalg.solve(A,b)\n\nx1 = np.linalg.inv(A) @ b\nx2 = np.linalg.solve(A,b)\n\nprint(\"Matrix Inverse Error: \",np.linalg.norm(A @ x1 - b))\nprint(\"Linear Solve Error: \",np.linalg.norm(A @ x2 - b))\n\n3.24 ms ¬± 394 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n1.67 ms ¬± 362 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\nMatrix Inverse Error:  3.4314559713132e-07\nLinear Solve Error:  2.238534789063854e-15\n\n\n\n100 million times more accurate, 5 times faster"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "href": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "title": "Meetup 2: Least Squares",
    "section": "Issue is worse for \\(A^TA\\)",
    "text": "Issue is worse for \\(A^TA\\)\n\nWhat about \\((A^TA)^{-1}\\)?\nLet‚Äôs look at SVD: \\[ A^TA = V \\Sigma^T\\Sigma U^T\\] \\[ \\Sigma^T\\Sigma = \\begin{bmatrix}\n  \\sigma_{1}^2 & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}^2\n\\end{bmatrix}\n\\]\nCondition number problems üíÄ üíÄ üíÄ üíÄ"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "href": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "title": "Meetup 2: Least Squares",
    "section": "Do Ill-Conditioned Matrices Occur?",
    "text": "Do Ill-Conditioned Matrices Occur?\n\nVery Common\nCollinear predictors in linear regression\nMain issue in Deep Learning\nNoisy Inverse Problems\nSome of the solutions are basically just regularization"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#alternatives",
    "href": "meetups/Meetup-2/meetup-2.html#alternatives",
    "title": "Meetup 2: Least Squares",
    "section": "Alternatives",
    "text": "Alternatives\n\nSoftware packages generally have routines to solve least squares without computing ill-conditioned matrix inverse\nGood example is \\(A = QR\\) factorization\nHere \\(Q\\) is orthogonal and \\(R\\) is upper triangular\nThen \\[\nA^+ = (A^TA)^{-1}A^T = R^{-1} Q\n\\]\nIn practice np.linalg.linsolve(R,Q @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "href": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "title": "Meetup 2: Least Squares",
    "section": "What if \\(A\\) is really big?",
    "text": "What if \\(A\\) is really big?\n\nConstructing \\(A^TA\\) costs \\(mn^2\\) operations\nSolving \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) costs \\(n^3\\) operations\n\\(A^TA\\) could be a TB or larger‚Ä¶..\nSolution 1- variety of iterative methods (HW)\nSolution 2- Randomized Algorithms (Random Kaczmarz)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#thanks",
    "href": "meetups/Meetup-2/meetup-2.html#thanks",
    "title": "Meetup 2: Least Squares",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "images/IntroSlides.html#about-me",
    "href": "images/IntroSlides.html#about-me",
    "title": "Welcome to DATA 609",
    "section": "About Me",
    "text": "About Me\n\n\n\nDoctoral Lecturer at CUNY SPS\nPast research experience:\n\nPlasma physics and Applied Mathematics (NYU)\nEcology and Evolutionary Biology (Princeton)\n\nResearch using Bayesian methods and genomic data to improve global scale ocean and climate models\nTaught math at NYU"
  },
  {
    "objectID": "images/IntroSlides.html#why-optimization",
    "href": "images/IntroSlides.html#why-optimization",
    "title": "Welcome to DATA 609",
    "section": "Why Optimization?",
    "text": "Why Optimization?\n\n\n\nOptimization theory underlies most methods in data science\nThree foci:\n\nLeast Squares Optimization\nConvex Optimization\nLocal Optimization and Stochastic Gradient Descent"
  },
  {
    "objectID": "images/IntroSlides.html#course-websites",
    "href": "images/IntroSlides.html#course-websites",
    "title": "Welcome to DATA 609",
    "section": "Course Website(s)",
    "text": "Course Website(s)\n\n\n\nBrightspace to submit homework\nCourse Website"
  },
  {
    "objectID": "images/IntroSlides.html#course-website-and-meetup",
    "href": "images/IntroSlides.html#course-website-and-meetup",
    "title": "Welcome to DATA 609",
    "section": "Course Website and Meetup",
    "text": "Course Website and Meetup\n\n\n\nBrightspace to submit homework\nCourse Website\nCourse Meetup: Monday 6:45-7:45PM\nZoom Link"
  },
  {
    "objectID": "images/IntroSlides.html#slack-channel",
    "href": "images/IntroSlides.html#slack-channel",
    "title": "Welcome to DATA 609",
    "section": "Slack Channel",
    "text": "Slack Channel\n\n\n\nCourse slack channel for messaging and rapid communications\nhttps://data609-spring-2025.slack.com\nInvite Link"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks",
    "href": "images/IntroSlides.html#textbooks",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks-1",
    "href": "images/IntroSlides.html#textbooks-1",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#assignments",
    "href": "images/IntroSlides.html#assignments",
    "title": "Welcome to DATA 609",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n8 Lab Assignments (80%)\nFinal Project (20%)"
  },
  {
    "objectID": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "href": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "title": "Welcome to DATA 609",
    "section": "Looking forward to a great semester!",
    "text": "Looking forward to a great semester!\nThanks for watching"
  },
  {
    "objectID": "assignments/labs/Lab1.html",
    "href": "assignments/labs/Lab1.html",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#instructions",
    "href": "assignments/labs/Lab1.html#instructions",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "href": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 1: Gradient Descent",
    "text": "Problem 1: Gradient Descent\n\nConsider the mathematical function defined on \\(f: \\mathbb{R}^2\\,\\to\\, \\mathbb{R}\\):\n\n\\[\nf(x,y) = (x-1)^2 + (y+2)^2,\n\\]\nFind the single critical point of this function and show that it is a local minimum (in this case, this will also be a global minimum).\n\nNow consider a new objective function that depends on a parameter \\(b\\): \\[\nf(x,y) = x^2 + by^2\n\\] Here we will look at two different values of \\(b\\), \\(b=3\\) and \\(b=10\\). The global minimum of this function occurs at the point \\(x^* = 0\\), \\(y^*=0\\) no matter what the value of \\(b\\). Suppose that we didn‚Äôt know this and wanted to find the minimum of this function using gradient descent instead of direct calculation.\n\n\nFirst write code to perform the gradient descent algorithm, that is perform the iteration: \\[\n\\mathbf{v}_{n+1} = \\mathbf{v}_n - k \\nabla f(\\mathbf{v}_n),\n\\]\n\nwhere the vector \\(\\mathbf{v} = \\begin{bmatrix} x & y\\end{bmatrix}^T\\) and \\(k\\) is the learning rate.\n\nThen test the performance of your algorithm as a function of the learning rates \\(k\\) by performing 100 iterations of the algorithm for 100 values of \\(k\\) equally spaced between \\(k=0.01\\) and \\(k=0.3\\). Start with an initial guess of \\(\\mathbf{v}_0 = \\begin{bmatrix} b & 1\\end{bmatrix}^T\\). Do this for \\(b=3\\) and \\(b=10\\). Make separate plots for \\(b=3\\) and \\(b=10\\) of the log base 10 of the error (in this case it is \\(\\sqrt{x_{100}^2+y_{100}^2}\\)) for the final value of the iteration versus the value of \\(k\\). How does learning rate relate to the final value of the error? For which value of \\(b\\) does the algorithm have the ability to converge fastest (have the lowest value of the error at the end)?\n\nNote: For some combinations of \\(k\\) and \\(b\\), the algorithm won‚Äôt converge to the right answer, i.e.¬†the error will grow with time. To make your plot easier to read, don‚Äôt plot the error for iterations that didn‚Äôt converge.\n\nAs \\(k\\) increases, for one or both values of \\(b\\), you will observe a point where the trend of final error versus learning rate reverses direction. Pick a value of \\(k\\) very close to the point where this occurs, and make a contour plot of the function \\(f\\) and the trajectory of the iterations for the gradient descent algorithm for that value of \\(k\\) superimposed over the contour plot. What do you observe?\n\nNote: The differences that you observe here are a special case of a more general phenomenon: the speed of convergence of gradient descent depends on something called the condition number of the Hessian matrix (the matrix of the 2nd order partial derivatives) of the target function. The condition number for a symmetric matrix is just the ratio of the largest to smallest eigenvalues, in this case the condition number is \\(b\\) (or 1/\\(b\\)). Gradient descent performs worse and worse the larger the condition number (and large condition numbers are problematic for a wide variety of other numerical methods)."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "href": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 2: Solving Least Squares Problems",
    "text": "Problem 2: Solving Least Squares Problems\nGenerate a random \\(20\\times 10\\) matrix \\(A\\) and a random 20-vector \\(b\\) (use a Gaussian distribution). Then, solve the least squares problem: \\[\n\\min_{\\mathbf{x}\\in \\mathbb{R}^{10}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\n\\] in the following ways:\n\nMultiply \\(\\mathbf{b}\\) by the Morse-Penrose Pseudoinverse \\(A^+\\).\nUse built in functions to solve the least squares problem (i.e.¬†in python numpy.lstsq, in R lm, and in Julia the backslash operator).\nUsing the \\(QR\\) factorization of \\(A\\). This factorization rewrites \\(A\\) as: \\[\nA = \\begin{bmatrix} Q & 0\\end{bmatrix} \\begin{bmatrix} R & 0 \\end{bmatrix}^T,\n\\] where \\(Q\\) is an orthonormal matrix and \\(R\\) is upper triangular. The least squares solution equals: \\[\n\\mathbf{x} = R^{-1}Q^T\\mathbf{b}\n\\]\nVerify that each of these solutions are nearly equal and that the residuals \\(A\\mathbf{x}-\\mathbf{b}\\) are orthogonal to the vector \\(A\\mathbf{x}\\)"
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "href": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 3: Iterative Solutions to Least Squares",
    "text": "Problem 3: Iterative Solutions to Least Squares\nAlthough the pseudoinverse provides an exact formula for the least squares solutions, there are some situations in which using the exact solution is computationally difficult, particularly when the matrix \\(A\\) and vector \\(\\mathbf{b}\\) have a large number of entries. In isn‚Äôt out of the ordinary for \\(A^TA\\) to be more than a terabyte, for example . In these cases it may be better to use an approximate solution instead of the exact formula. There are many different approximate methods for solving least squares problems, here we will use an iterative method developed by Richardson.\nThis method begins with an initial guess \\(\\mathbf{x}^{(0)} = 0\\) and calculates successive approximations as follows:\n\\[\n    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mu A^T\\left(A\\mathbf{x}^{(k)}-\\mathbf{b}\\right)\n\\]\nHere \\(\\mu\\) is a positive paramter that has a similar interpretation to the learning rate for gradient descent. A choice that guarantees convergence is \\(\\mu \\leq \\frac{1}{\\|A\\|}\\). The iteration is terminated when the change in the residual \\(\\|A^T(Ax^{(k)} ‚àí b)\\|\\) after successive steps is below a user determined threshold, which indicates that the least squares optimality conditions are nearly satisfied.\n\nSuppose that \\(\\mathbf{x}\\) is a solution to the least squares problem: \\[\n\\mathbf{x} = A^+\\mathbf{b}\n\\]\n\nShow by substitution of the formula for the pseudoinverse that \\(\\mathbf{x}\\) is a fixed-point of the iteration scheme, i.e.¬†that: \\[\n\\mathbf{x} = \\mathbf{x} - \\mu A^T\\left(A\\mathbf{x}-\\mathbf{b}\\right)\n\\]\n\nGenerate a random 20 √ó 10 matrix \\(A\\) and 20-vector \\(\\mathbf{b}\\), and compute the least squares solution \\(\\mathbf{x} = A^+\\mathbf{b}\\). Then run the Richardson algorithm with \\(\\mu = \\frac{1}{\\|A\\|^2}\\) for 500 iterations, and plot \\(\\|\\mathbf{x}^{(k)}-\\mathbf{x}\\|\\) to verify that \\(\\mathbf{x}^{(k)}\\) is converging to \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "assignments/labs/Lab4.html",
    "href": "assignments/labs/Lab4.html",
    "title": "Homework 3: Convex Functions",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab4.html#instructions",
    "href": "assignments/labs/Lab4.html#instructions",
    "title": "Homework 3: Convex Functions",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab4.html#problem-1",
    "href": "assignments/labs/Lab4.html#problem-1",
    "title": "Homework 3: Convex Functions",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 3.2 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab4.html#problem-2",
    "href": "assignments/labs/Lab4.html#problem-2",
    "title": "Homework 3: Convex Functions",
    "section": "Problem 2:",
    "text": "Problem 2:\nExercise 3.6 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab4.html#problem-3",
    "href": "assignments/labs/Lab4.html#problem-3",
    "title": "Homework 3: Convex Functions",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 3.16 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab4.html#problem-4",
    "href": "assignments/labs/Lab4.html#problem-4",
    "title": "Homework 3: Convex Functions",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 3.22 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab4.html#problem-5",
    "href": "assignments/labs/Lab4.html#problem-5",
    "title": "Homework 3: Convex Functions",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 3.24 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab2.html",
    "href": "assignments/labs/Lab2.html",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#instructions",
    "href": "assignments/labs/Lab2.html#instructions",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "href": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models",
    "text": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models\nMany applications of least squares (and other statistical methods) involve , in which data is collected over a time period and the statistical model is updated as new data arrives. If the quantity of data arriving is very large, it may be inefficient or even impossible to refit the entire model on the entire dataset. Instead, we use techniques (often referred to as which take the current model as a starting point and update them to incorporate the new data.\nThe structure of least squares problems makes them amenable to online updating (sometimes this is called ‚Äúrecursive‚Äù least squares). The structure of the problem is as follows, at time \\(t\\) we receive a vector of observations \\(\\mathbf{a}_t\\) and an observation of our target variable \\(b_t\\).\nThe full set of all observations that we have received up to time \\(t\\) is contained in the following matrix:\n\\[\nA_{(t)} = \\begin{bmatrix} \\mathbf{a}_1^T \\\\ \\mathbf{a}_2^T \\\\ \\vdots \\\\ \\mathbf{a}_t^T \\end{bmatrix}\n\\]\nwhich says that row \\(j\\) of the matrix \\(A_{(t)}\\) is the \\(j\\)th observation \\(\\mathbf{a}_j^T\\).\nThe observations up to the time \\(t\\) of the target variable are similarly encoded in a vector: \\[\n\\mathbf{b}_{(t)} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_t \\end{bmatrix}\n\\]\nHere we assume that the vectors \\(\\mathbf{a}\\) each contain \\(n\\) observations, so that \\(A_{(t)}\\) is a \\(t\\times n\\) matrix and the vector \\(\\mathbf{b}_{(t)}\\) is a \\(t\\)-vector.\nAs long as \\(t&gt;n\\), i.e.¬†the number of time observations is greater than the number of features/data points in each observation, we can fit a linear model predicting the target as a function of the features \\(\\mathbf{a}\\) by solving the least squares problem:\n\\[\n(A^T_{(t)}A_{(t)})\\mathbf{x} = A_{(t)}^T\\mathbf{b}_{(t)}\n\\]\nAs the length of the time series increases, the computational difficulty of solving the problem also increases. However, a remarkable result in linear algebra called the matrix inversion lemma allows us to efficiently update the solution of the least squares problem."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "href": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 2: Weighted Least Squares",
    "text": "Problem 2: Weighted Least Squares\nThe file social-mobility.csv contains data on the fraction of individuals born in the years 1980-1982 to parents in the bottom 20% of the income distribution who reach the top 20% of the income distribution by the time they turn 30 in a large number of municipalities throughout the United States. The dataset also contains additional variables that describe other socio-economic differences between the cities in the dataset.\n\nMake a scatter-plot of mobility versus population (use a log-scale for population). What do you notice about the variance of social mobility as a function of population? This is a common feature of nearly every dataset containing geographic regions with widely different populations.\nAssume that the number of children born in families making below the 20th percentile of the income distribution in each city is linearly proportional to the city population. Write down a formula for how the variance of each measurement of the social mobility based on the measured social mobility and the population. Hint: start with either the formula for the variance of binomial counts or look up the variance of a proportion derived from a binomial distribution. Don‚Äôt worry about constant factors when deriving this formula.\nUse weighted least squares to calculate an estimate of how social mobility depends on commute time and student-teacher ratio, using weights calculated based on the variance estimate derived in (b). Compare the coefficients to those derived from ordinary least squares with no weights.\n\n##Problem 3: Markowitz Portfolio Optimization\nIn this problem you will use Markowitz Portfolio Optimization to construct a set of portfolios that aim to achieve certain target rates of return while minimizing risk. The file prices.csv contains information on daily asset returns from 2020-2024 for a group of assets. The data is divided into two time periods, a training period (2020-2022) and a testing period (2022-2024).\n\nConstruct a vector of annual returns \\(\\mu\\) and return covariance \\(\\Gamma\\). Then solve the following constrained least squares problem to calculate optimal portfolios achieving a fixed rate of return with minimum variance: \\[\n    \\min_{w} x^T\\Gamma x,\\\\\n    w^T\\mu = r\n\\]\n\nCalculate optimal portfolios based on the 2020-2022 data for targeted rates of return \\(r=5\\%\\), \\(r=10\\%\\), and \\(r=20\\%\\).\n\nPlot the cumulative value of each portfolio over time, starting from an initial investment of \\(\\$10000\\), for both the training and test sets of returns.\n\nFor each of the 3 portfolios report:\n\nThe annualized return on the training and test sets;\nThe annualized risk on the training and test sets;\nThe asset with the minimum allocation weight (can be the most negative), and its weight;\nThe asset with the maximum allocation weight, and its weight;\nThe leverage, defined as |w1| + ¬∑ ¬∑ ¬∑ + |wn|. (Several other definitions of leverage are used.) This number is always at least one, and it is exactly one only if the portfolio has no short positions.\n\nComment briefly on your observations about the different portfolios and the difference between their training and testing performance.\n\nIt is well known that optimal portfolios constructed using the Markowitz procedure perform much more poorly out of sample compared to in sample. This is due to a variety of reasons, one of which is that it assumes that future returns are equal to past returns, another that the correlation structure of the market might change over time, and finally, when there are many assets there is the potential for overfitting. Repeat the previous problem but introduce a ridge regression/\\(l_2\\) norm penalty term to the objective function, with a hyperparameter \\(\\lamba\\) governing the size of the penalty term.\n\nSelect 10 positive values of \\(\\lamba\\) on a log scale between \\(1e-1\\) and \\(10\\) and for each value of \\(\\lamba\\) solve the following penalized regression problem:\n\\[\n    \\min_{w} w^T(\\Gamma+\\lambda I) w ,\\\\\n    w^T\\mu = r\n\\]\nfor just the single value of \\(r=20\\%\\). Then calculate the performance of each of these regularized Markowitz strategies on both the training and test datasets and plot the values of:\n\nThe annualized return on the training and test sets;\nThe annualized risk on the training and test sets;\nThe minimum allocation weight;\nThe maximum allocation weight;\nThe leverage\n\nComment on how the different values of \\(\\lambda\\) changed the optimal portfolios and the difference between in-sample and out-of-sample return and variance."
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Project",
    "section": "",
    "text": "The idea of this project is for you to find data and do something interesting with it that relates to the skills you have developed in this course.\nThe project is intentionally open ended- this is an opportunity for you to explore your interests, through using techniques or technologies that branch off those that were introduced in this class, datasets related to problems you face in your career or that you are curious about, or both.\nMy expectation for your project is that it has the length and sophistication of a complete homework assignment, but will all the work focused on the topic. It should include an introduction to contextualize the topic, but otherwise you can structure it how you like.\nThe output of your project should software that allows me to reproduce your work and a research report which describes what you did with the data and your findings, which could be contained within a github repository.\n\n\nThe first step of your project is to submit an initial proposal, which will help to ensure that your project is both worthwhile and feasible in the time frame of the last half of the semester. Your proposal should describe the area that you are interested in, the datasets you are going to use/mathematical problems you want to solve, and the link to the topics of this course. The proposal length should be around one page (excluding figures and references),\nThe proposal is preliminary, and you can include additional data or a different analysis as needs require.\n\n\n\nWhen writing proposals, I‚Äôve found the following set of considerations, called ‚ÄúHeilmeier's Questions‚Äù, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr.¬†George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you‚Äôre successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final ‚Äúexams‚Äù to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#heilmeiers-questions",
    "href": "assignments/project.html#heilmeiers-questions",
    "title": "Project",
    "section": "",
    "text": "When writing proposals, I‚Äôve found the following set of considerations, called ‚ÄúHeilmeier's Questions‚Äù, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr.¬†George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you‚Äôre successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final ‚Äúexams‚Äù to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Writing Math in Markdown\n\n\n\n\n\nClick here for a 20 minute video on writing math in markdown using Latex.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Linear Algebra in Numpy\n\n\n\n\n\nClick here for a video on basic linear algebra in Numpy.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 2: Least Squares Optimization\n\n\n\n\n\nClick here to read more about Meetup 2 and view/download the lecture slides.\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2- Least Squares\n\n\n\n\n\nClick here to read about Week 2.\n\n\n\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 1: Introduction to Optimization\n\n\n\n\n\nClick here to read more about Meetup 1 and view/download the lecture slides.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA 609\n\n\n\n\n\nImportant information on how to get started with this course. Click this post now for instructions on getting started.\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "The homework assignments in this course are lab assignments where you will solve the problem using a combination of mathematics and computer code.\nPlease submit a and either a quarto file or jupyter notebook or other code that generates your homework. Labs should be submitted on Blackboard.\n\n\nGradient Descent and Least Squares (Download)\n\n\nLeast Squares Applications (Download)\n\n\nConvex Sets (Download)\n\n\nConvex Functions (Download)\n\n\nConvex Optimizations Problems (Download)\n\n\nApplications to Statistics and Machine Learning (Download)\n\n\nNonconvex Optimization and Stochastic Gradient Descent (Download)\n\n\nTraining Deep Neural Networks (Download)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "assignments/labs/Lab3.html",
    "href": "assignments/labs/Lab3.html",
    "title": "Homework 3: Convex Sets",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab3.html#instructions",
    "href": "assignments/labs/Lab3.html#instructions",
    "title": "Homework 3: Convex Sets",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab3.html#problem-1",
    "href": "assignments/labs/Lab3.html#problem-1",
    "title": "Homework 3: Convex Sets",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 2.5 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab3.html#problem-2",
    "href": "assignments/labs/Lab3.html#problem-2",
    "title": "Homework 3: Convex Sets",
    "section": "Problem 2:",
    "text": "Problem 2:\nExercise 2.7 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab3.html#problem-3",
    "href": "assignments/labs/Lab3.html#problem-3",
    "title": "Homework 3: Convex Sets",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 2.12 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab3.html#problem-4",
    "href": "assignments/labs/Lab3.html#problem-4",
    "title": "Homework 3: Convex Sets",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 2.15 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab3.html#problem-5",
    "href": "assignments/labs/Lab3.html#problem-5",
    "title": "Homework 3: Convex Sets",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 2.28 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab6.html",
    "href": "assignments/labs/Lab6.html",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab6.html#instructions",
    "href": "assignments/labs/Lab6.html#instructions",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-1",
    "href": "assignments/labs/Lab6.html#problem-1",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 7.4 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-2",
    "href": "assignments/labs/Lab6.html#problem-2",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 2:",
    "text": "Problem 2:\nExercise 7.6 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-3",
    "href": "assignments/labs/Lab6.html#problem-3",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 7.14 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-4",
    "href": "assignments/labs/Lab6.html#problem-4",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 7.48 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-5",
    "href": "assignments/labs/Lab6.html#problem-5",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 8.6 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab5.html",
    "href": "assignments/labs/Lab5.html",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab5.html#instructions",
    "href": "assignments/labs/Lab5.html#instructions",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-1",
    "href": "assignments/labs/Lab5.html#problem-1",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 4.2 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-2",
    "href": "assignments/labs/Lab5.html#problem-2",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 2:",
    "text": "Problem 2:\n4.2 ‚ÄòHello World‚Äô in CVX. Use CVX to verify the optimal values you obtained (analytically) for exercise 4.2 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-3",
    "href": "assignments/labs/Lab5.html#problem-3",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 4.60 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-4",
    "href": "assignments/labs/Lab5.html#problem-4",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 6.4 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-5",
    "href": "assignments/labs/Lab5.html#problem-5",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 6.13 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/tbd.html",
    "href": "assignments/labs/tbd.html",
    "title": "Lab TBD",
    "section": "",
    "text": "Check back later\nThis assignment is not available to view yet. Please check back closer to the assignment due date/week of the module this assignment is based on."
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is Optimization?",
    "text": "What is Optimization?\n\nWhat is the decision that leads to the best outcome?\nWhat is the value of our decision variable \\(\\mathbf{x}\\) that minimizes our objective function f()?\n\n\\[\n\\textrm{minimize}\\quad f(\\mathbf{x}), \\\\\n\\textrm{subject to}\\quad g_i(\\mathbf{x}) \\leq 0, \\\\\n\\textrm{and}\\quad h_j(\\mathbf{x}) = 0,\\\\\n\\mathbf{x}\\in \\mathbb{R}^n, \\quad f,\\, g_i,\\, h_j: \\mathbb{R}^n\\mapsto \\mathbb{R}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Constraint Terminology",
    "text": "Constraint Terminology\n\nA feasible point \\(\\mathbf{x}\\) for the optimization problem is a point that satisfies all the constraints: \\[\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]\nTechnically we don‚Äôt need the \\(h_j\\) terms\nCan get greater than constraints with \\(-g_i\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimum Terminology",
    "text": "Optimum Terminology\n\n\\(\\mathbf{x}\\) is called a local minimum if there is some region \\(\\Gamma\\) surrounding \\(\\mathbf{x}\\) where \\(f(\\mathbf{x})\\leq f(\\mathbf{x'})\\) for all \\(\\mathbf{x}'\\) in \\(\\Gamma\\).\n\\(\\mathbf{x}\\) is a global minimum if there is no feasible point \\(x'\\) with \\(f(\\mathbf{x'})&lt;f(\\mathbf{x})\\)\nCan turn a maximization problem into a minimization problem by replacing the objective function \\(f\\) with \\(-f\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(\\mathbf{x}\\) represents something we control:\n\nCoefficients of a linear regression\nWeights of a neural network\nAllocation of $ to advertising channels\nInvestment allocations in a portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(f\\) models how the decision \\(\\mathbf{x}\\) impacts us\n\nRMS error of the regression\ncross-entropy plus a penalty for the weights\nAd impressions on target demographic\nExpected variance of your portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(g_i\\) and \\(h_j\\) constrain your decisions \\(\\mathbf{x}\\) so they are realistic\n\nCan‚Äôt put negative $ in an ad channel\nRegulatory limits on leverage, short positions\nValid probability distribution\nPrior constraints on parameters"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization underlies nearly all data science algorithms",
    "text": "Optimization underlies nearly all data science algorithms\nGoing in depth brings some benefits:\n\nRecognize what problem you are facing, can enable you to solve it 1000 times easier in some cases\nUnderstand how the tools you use work and what can go wrong\nAbsolutely indispensable for some cutting edge methods (i.e.¬†deep learning)\nUseful non-stats applications"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\n\n\\(i\\)th observation is vector \\(\\mathbf{a}_i\\)\nObs of target function \\(b_i\\)\nLinear model \\(b \\sim a^t x\\)\n\n\\[\n\\textrm{minimize RMS error:}\\quad \\min_{x\\in\\mathbb{R}^n} \\sum_{i} \\left(\\mathbf{a}_i^t \\mathbf{x}-b_i\\right)^2 \\] or let \\(A\\) be the matrix whose rows are the vectors \\(\\mathbf{a}_i^t\\): \\[\\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 \\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\nAdd penalty terms for regularization or constraints\n\\[ \\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 +k\\|x\\|^2 \\\\\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "href": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Maximum Likelihood",
    "text": "Examples: Maximum Likelihood\n\n\\(p(\\mathbf{y}|\\mathbf{x})\\) is likelihood of data given parameters\nmaximum likelihood is the optimization problem\n\n\\[\n\\textrm{find:}\\quad \\min_{\\mathbf{x}\\in\\mathbb{R}^n} -\\log\\left(p(\\mathbf{y}|\\mathbf{x})\\right) \\\\\n\\textrm{subject to:}\\quad g_i(\\mathbf{x}) \\leq 0 \\\\\n\\textrm{and:}\\quad h_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "href": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Example: Deep Learning",
    "text": "Example: Deep Learning\n\nTraining examples \\(\\mathbf{x}_i\\) and labels \\(y_i\\)\nNeural network defined by a function \\(\\phi\\) that depends on weights \\(\\mathbf{w}\\)\nLearning problem:\n\n\\[ \\min_w \\sum_i C(\\phi(\\mathbf{x}_i,\\mathbf{w}),y_i), \\]\n\n\\(C\\) is a cost function"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization outside data sciece",
    "text": "Optimization outside data sciece\n\nOrgs and Companies face constant optimization problems\nLearning how to handle them can be a superpower"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLeast Squares\n\n\nOldest, easiest, most mature technologies, first four weeks\nPrimary book is VMLS"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nConvex Optimization\n\n\nNon-negative curvature\nIncredibly useful, difficult, weeks 5-12\nSubsumes many special cases (least squares, linear programs, etc)\nGrowing field, especially with advent of AI"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nNonconvex Optimization\n\n\nCan‚Äôt solve these but can come close\nUseful for things like deep learning\nLast 3 weeks\nMore based on lectures but also https://www.deeplearningbook.org/"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "href": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Other Textbook",
    "text": "Other Textbook\n\n\n\nWe have one more good book, unforunately not free\nAlso doesn‚Äôt cover anything in depth\nGreat for overall picture"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "href": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nDue every two weeks\nMostly solving problems with data but some math\nPreferred format: quarto file accompanied with rendered pdf\nChoice of language, Recommended python, Julia, R\n80% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#final-project",
    "href": "meetups/Meetup-1/meetup-1.html#final-project",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Final Project",
    "text": "Final Project\n\nFinal Project will be to explore an aspect of the course in a little more depth.\nIntentionally open ended, but could be as simple as picking an application area of interest to you and designing and solving a problem in that area\n20% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "href": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Website and Slack Channel",
    "text": "Course Website and Slack Channel\n\nKey annoucements, homework, readings, etc will be posted to: https://georgehagstrom.github.io/DATA609Spring2025/\nCourse will have a slack channel to enable rapid communication: invite link\nTurn in assignments on Brightspace"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "href": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\n\nAI is useful, you can ask it questions, have it generate code, etc\nIt is great at easy problems, but that is a trap\nIt makes lots of mistakes\nResearch shows it helps people who know the material well much more than those who don‚Äôt\nPolicy: You should understand everything that you turn in completely. I‚Äôll will be generous with grades for genuine effort but will penalize AI-driven mistakes harshly"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "href": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf needed review linear algebra:\n\nEssential Linear Algebra for Machine Learning -Appendix A1, A3-A5 of Convex Optimization\n\nStart Lab 1\n\nProblem 1 this week, 2 and 3 next week"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization: Needles in a haystack?",
    "text": "Optimization: Needles in a haystack?\n\nGeneral optimization is a hard problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError proportional to # boxes sampled: \\[\\mathrm{Num Boxes} \\sim \\frac{C}{\\epsilon^N}\\]\nN is num. parameters\nModest N, problem takes longer than age of universe"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "href": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Only Local Optimization is Possible",
    "text": "Only Local Optimization is Possible\n\n\n\nCan‚Äôt find lowest valley, but can find a valley\nOften, this is still very useful in practice\nNo guarantees, sensitive to initial guess\nFor convex problems, only one valley!"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nIn calculus need two things: \\[\n\\frac{df}{dx} = 0,\n\\frac{d^2f}{dx^2} &gt; 0\n\\]\nFunction is ‚Äúflat‚Äù\nCurvature is ‚Äúup‚Äù"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nHigher dimensions \\[\n\\nabla f(\\mathbf{x}) = 0 \\\\\n(\\nabla^2 f)_{ij} = \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j} \\succ 0\n\\]\nVanishing gradient\nPositive Definite ‚ÄúHessian‚Äù"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\nIterative methods:\n\nGradient Descent, Newton‚Äôs Method, ‚Ä¶.\n\nStart with initial guess \\(\\mathbf{x}_0\\)\nCalculate gradient \\(\\nabla f\\)\nTake a ‚Äústep‚Äù in direction of \\(\\nabla f\\) \\[ \\mathbf{x}_{n+1} = \\mathbf{x}_n - k\\nabla f(\\mathbf{x}_{n})\\]\n\\(k\\) is step size or learning rate"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f(x,y) = \\exp\\left(-x^2-3y^2\\right)\\)\n\\(k=0.1\\)\n100 steps"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#advert",
    "href": "meetups/Meetup-1/meetup-1.html#advert",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Advert",
    "text": "Advert\n\nOnce per month (Tuesday, Wedneday, Thursday) ‚Äúfield trips‚Äù to New York Open Statistical Computing Meetup\nSend me a DM I‚Äôll add you to the email list\nnyhackr.org"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#thanks",
    "href": "meetups/Meetup-1/meetup-1.html#thanks",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "href": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb= np.random.randn(10)\n\n\na @ b\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nb.T @ a.T\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.linalg.inv(a) @ a\n\narray([[ 1.00000000e+00, -3.65595152e-16, -1.37241160e-16,\n        -1.67938840e-15,  9.33527437e-16,  1.44199349e-15,\n         1.15089786e-16, -1.55696214e-15,  1.11022302e-15,\n        -2.63677968e-16],\n       [ 1.26550255e-15,  1.00000000e+00, -2.68494358e-16,\n         1.42303383e-15, -9.79693103e-16, -4.51521472e-15,\n         2.63044668e-15,  1.74407151e-15, -2.66453526e-15,\n         4.44089210e-16],\n       [-1.64423122e-15, -1.66173972e-16,  1.00000000e+00,\n         2.75288470e-15,  1.74102465e-16,  1.54544175e-15,\n        -2.79739957e-15, -2.04740762e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [-3.51858931e-16,  6.23676926e-17,  2.66458886e-16,\n         1.00000000e+00,  9.38263196e-17,  2.61301662e-16,\n         2.45064178e-16, -1.53357911e-16,  4.44089210e-16,\n        -4.44089210e-16],\n       [ 7.84703105e-16, -1.04920005e-15,  4.13020606e-16,\n        -2.12483741e-15,  1.00000000e+00, -4.53483867e-15,\n         4.18017106e-15,  1.71820802e-15,  0.00000000e+00,\n         0.00000000e+00],\n       [ 9.37535921e-16,  3.43043467e-16,  9.65347805e-17,\n         6.68706576e-16, -4.20016708e-16,  1.00000000e+00,\n        -1.09267442e-15,  1.01169627e-16,  0.00000000e+00,\n         4.44089210e-16],\n       [-8.71025446e-16,  1.09123436e-15, -7.64517688e-16,\n         9.73897302e-16, -6.40358430e-16,  3.68586912e-15,\n         1.00000000e+00, -1.54120216e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [ 2.86959469e-15,  2.45431198e-16,  5.06260913e-16,\n        -5.81240373e-16, -5.42632247e-16, -2.38411883e-15,\n         1.09159912e-15,  1.00000000e+00, -8.88178420e-16,\n        -4.44089210e-16],\n       [-3.57782462e-17, -5.09336718e-16,  7.90198623e-16,\n        -2.53964484e-15, -7.71687051e-17, -1.71387168e-15,\n         1.28935306e-15,  1.41638585e-15,  1.00000000e+00,\n        -8.88178420e-16],\n       [-5.10166112e-16,  3.36418889e-16,  3.54329540e-16,\n        -1.34491999e-15, -3.29604245e-16, -7.44603616e-16,\n         1.18536447e-15,  5.30313254e-16,  4.44089210e-16,\n         1.00000000e+00]])\n\n\n\nnp.linalg.svd(a).Vh\n\narray([[ 0.28383114,  0.13031739,  0.08488282,  0.26915419, -0.05833712,\n        -0.55745972,  0.04622785,  0.52583554, -0.47933131, -0.01565541],\n       [-0.7299212 ,  0.04346431, -0.15481374,  0.1199476 ,  0.28639583,\n        -0.35620311,  0.24101023, -0.22640736, -0.21810744, -0.24728702],\n       [ 0.07621821,  0.43546203,  0.2088927 ,  0.28230896, -0.17741055,\n        -0.17934521,  0.42879952, -0.01233035,  0.62390845, -0.21049244],\n       [-0.39305518,  0.11448225,  0.09008748,  0.41564326, -0.41297868,\n         0.10835773, -0.0692628 , -0.00268073, -0.05982432,  0.67885942],\n       [ 0.06933558,  0.34860056, -0.39022658,  0.26430987, -0.41367073,\n         0.19057494, -0.35422431, -0.22534478, -0.22225331, -0.46737544],\n       [-0.05118823, -0.68217798, -0.03157757,  0.58681909,  0.00875722,\n         0.07532819, -0.10790011,  0.20278167,  0.24018863, -0.26543311],\n       [ 0.31944117, -0.2503526 , -0.62925149,  0.04300488, -0.15611409,\n        -0.20160406,  0.45924193, -0.30071194, -0.00703338,  0.26660451],\n       [ 0.25406731,  0.17492689,  0.15182522,  0.44334045,  0.50247181,\n         0.45816405,  0.28794845, -0.19810239, -0.31166484,  0.05968817],\n       [ 0.22725739, -0.10877031,  0.36247981,  0.15032739,  0.06499115,\n        -0.45411298, -0.38287566, -0.64660084,  0.0248702 ,  0.08237576],\n       [ 0.02620891,  0.29393688, -0.46195436,  0.15967492,  0.51017133,\n        -0.14029136, -0.41663193,  0.18003309,  0.35346832,  0.25124871]])\n\n\n\nnp.diagonal?\n\n\nSignature:       np.diagonal(a, offset=0, axis1=0, axis2=1)\nCall signature:  np.diagonal(*args, **kwargs)\nType:            _ArrayFunctionDispatcher\nString form:     &lt;function diagonal at 0x7b3729c832e0&gt;\nFile:            ~/miniforge3/envs/stan/lib/python3.11/site-packages/numpy/core/fromnumeric.py\nDocstring:      \nReturn specified diagonals.\nIf `a` is 2-D, returns the diagonal of `a` with the given offset,\ni.e., the collection of elements of the form ``a[i, i+offset]``.  If\n`a` has more than two dimensions, then the axes specified by `axis1`\nand `axis2` are used to determine the 2-D sub-array whose diagonal is\nreturned.  The shape of the resulting array can be determined by\nremoving `axis1` and `axis2` and appending an index to the right equal\nto the size of the resulting diagonals.\nIn versions of NumPy prior to 1.7, this function always returned a new,\nindependent array containing a copy of the values in the diagonal.\nIn NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\nbut depending on this fact is deprecated. Writing to the resulting\narray continues to work as it used to, but a FutureWarning is issued.\nStarting in NumPy 1.9 it returns a read-only view on the original array.\nAttempting to write to the resulting array will produce an error.\nIn some future release, it will return a read/write view and writing to\nthe returned array will alter your original array.  The returned array\nwill have the same type as the input array.\nIf you don't write to the array returned by this function, then you can\njust ignore all of the above.\nIf you depend on the current behavior, then we suggest copying the\nreturned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\nof just ``np.diagonal(a)``. This will work with both past and future\nversions of NumPy.\nParameters\n----------\na : array_like\n    Array from which the diagonals are taken.\noffset : int, optional\n    Offset of the diagonal from the main diagonal.  Can be positive or\n    negative.  Defaults to main diagonal (0).\naxis1 : int, optional\n    Axis to be used as the first axis of the 2-D sub-arrays from which\n    the diagonals should be taken.  Defaults to first axis (0).\naxis2 : int, optional\n    Axis to be used as the second axis of the 2-D sub-arrays from\n    which the diagonals should be taken. Defaults to second axis (1).\nReturns\n-------\narray_of_diagonals : ndarray\n    If `a` is 2-D, then a 1-D array containing the diagonal and of the\n    same type as `a` is returned unless `a` is a `matrix`, in which case\n    a 1-D array rather than a (2-D) `matrix` is returned in order to\n    maintain backward compatibility.\n    If ``a.ndim &gt; 2``, then the dimensions specified by `axis1` and `axis2`\n    are removed, and a new axis inserted at the end corresponding to the\n    diagonal.\nRaises\n------\nValueError\n    If the dimension of `a` is less than 2.\nSee Also\n--------\ndiag : MATLAB work-a-like for 1-D and 2-D arrays.\ndiagflat : Create diagonal arrays.\ntrace : Sum along diagonals.\nExamples\n--------\n&gt;&gt;&gt; a = np.arange(4).reshape(2,2)\n&gt;&gt;&gt; a\narray([[0, 1],\n       [2, 3]])\n&gt;&gt;&gt; a.diagonal()\narray([0, 3])\n&gt;&gt;&gt; a.diagonal(1)\narray([1])\nA 3-D example:\n&gt;&gt;&gt; a = np.arange(8).reshape(2,2,2); a\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n&gt;&gt;&gt; a.diagonal(0,  # Main diagonals of two arrays created by skipping\n...            0,  # across the outer(left)-most axis last and\n...            1)  # the \"middle\" (row) axis first.\narray([[0, 6],\n       [1, 7]])\nThe sub-arrays whose main diagonals we just obtained; note that each\ncorresponds to fixing the right-most (column) axis, and that the\ndiagonals are \"packed\" in rows.\n&gt;&gt;&gt; a[:,:,0]  # main diagonal is [0 6]\narray([[0, 2],\n       [4, 6]])\n&gt;&gt;&gt; a[:,:,1]  # main diagonal is [1 7]\narray([[1, 3],\n       [5, 7]])\nThe anti-diagonal can be obtained by reversing the order of elements\nusing either `numpy.flipud` or `numpy.fliplr`.\n&gt;&gt;&gt; a = np.arange(9).reshape(3, 3)\n&gt;&gt;&gt; a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n&gt;&gt;&gt; np.fliplr(a).diagonal()  # Horizontal flip\narray([2, 4, 6])\n&gt;&gt;&gt; np.flipud(a).diagonal()  # Vertical flip\narray([6, 4, 2])\nNote that the order in which the diagonal is retrieved varies depending\non the flip function.\nClass docstring:\nClass to wrap functions with checks for __array_function__ overrides.\nAll arguments are required, and can only be passed by position.\nParameters\n----------\ndispatcher : function or None\n    The dispatcher function that returns a single sequence-like object\n    of all arguments relevant.  It must have the same signature (except\n    the default values) as the actual implementation.\n    If ``None``, this is a ``like=`` dispatcher and the\n    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n    first (additional and positional) argument.\nimplementation : function\n    Function that implements the operation on NumPy arrays without\n    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n    will be forwarded to this (and the ``dispatcher``) as if using\n    ``*args, **kwargs``.\nAttributes\n----------\n_implementation : function\n    The original implementation passed in.\n\n\n\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nb = np.random.randn(100,2)\n\n\nnp.linalg.svd(b)\n\nSVDResult(U=array([[ 3.18048731e-05,  6.74648048e-02, -6.41579788e-02, ...,\n        -4.70143275e-02, -1.17012062e-02,  5.72707204e-02],\n       [-1.67854420e-01,  1.70642880e-01, -1.21850722e-01, ...,\n        -1.51615802e-01,  2.00743368e-02, -1.09802273e-01],\n       [-7.00863302e-02,  1.18377452e-01,  9.84604342e-01, ...,\n        -1.72681740e-02,  1.25486957e-03, -7.25125082e-03],\n       ...,\n       [-1.04494452e-01,  1.19440600e-01, -1.73568100e-02, ...,\n         9.79733632e-01,  1.95664282e-03, -1.09753255e-02],\n       [ 2.32789965e-02, -5.98960947e-04,  1.31502367e-03, ...,\n         2.01688869e-03,  9.99525031e-01,  2.51988849e-03],\n       [-1.23784387e-01,  8.99047891e-03, -7.56209658e-03, ...,\n        -1.12822312e-02,  2.51689274e-03,  9.86617378e-01]]), S=array([9.7586975 , 8.78873925]), Vh=array([[-0.46515128, -0.8852312 ],\n       [-0.8852312 ,  0.46515128]]))\n\n\n\\[ A = U\\Sigma V^T\\] \\[ (A^TA)^{-1} A = (V\\Sigma^T U^T U \\Sigma^T V^T)^{-1} U\\Sigma V^T    \\] \\[A^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V\\Sigma^T U^T \\] \\[ A^+ = V (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T \\] \\[ A^+ = V\\Sigma^+ U^T \\]\n\nU = np.linalg.svd(b)[0]\n\n\nU.T @ U\n\narray([[ 1.00000000e+00, -1.24938610e-16,  5.63615904e-17, ...,\n         2.77555756e-17, -5.20417043e-18,  0.00000000e+00],\n       [-1.24938610e-16,  1.00000000e+00, -3.15292394e-17, ...,\n        -3.46944695e-17,  6.50521303e-18, -9.54097912e-18],\n       [ 5.63615904e-17, -3.15292394e-17,  1.00000000e+00, ...,\n        -3.46944695e-18, -1.30104261e-18,  5.20417043e-18],\n       ...,\n       [ 2.77555756e-17, -3.46944695e-17, -3.46944695e-18, ...,\n         1.00000000e+00,  4.82754810e-19, -4.24892617e-18],\n       [-5.20417043e-18,  6.50521303e-18, -1.30104261e-18, ...,\n         4.82754810e-19,  1.00000000e+00,  3.18478586e-19],\n       [ 0.00000000e+00, -9.54097912e-18,  5.20417043e-18, ...,\n        -4.24892617e-18,  3.18478586e-19,  1.00000000e+00]])\n\n\n\n$$ V(\\Sigma^T\\Sigma)^{-1}\\Sigma^TU^T b = x $$\n$$ (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b  = V^T x$$\n$$ (\\Sigma^T U^T b) = (\\Sigma^T\\Sigma) (V^T x)$$\n\n`V @ np.linalg.solve(S^T @ S, U.T @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/LatexTutorial.html",
    "href": "meetups/Meetup-2/LatexTutorial.html",
    "title": "This is a markdown cell",
    "section": "",
    "text": "Here is a math expression \\[\nx+1 = 0\n\\]\nHere is a more complex\n\\[\nx^2_0 + y_1 = 0\n\\]\nHere is how you exert greater control over the sub and superscripts\n\\[\nx^{2y_1-2}_{opt}\n\\]\nHere is how we do a sum\n\\[\n\\sum_{i=1}^{100} x_i\n\\]\nHere is how to write an integral:\n\\[\n\\int_{0}^{\\infty} dx \\exp(-x^2)\n\\]\n\\[\n\\int_{0}^{\\infty} dx e^{-x^2}\n\\]\n\\[\nf(x,y) = \\frac{\\sin(xy) }{1 + \\exp(x+y) }\n\\]\n\\[\nf(x,y) = x + y^2\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = 1\n\\]\n\\[\n\\frac{\\partial f}{\\partial y} = 2y\n\\]\n\\[\n\\nabla f\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\n\\end{bmatrix} ^T\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n\\]\nThe Identity in 3D\n\\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\\n                    0 & 1 & 0 \\\\\n                    0 & 0 & 1\n    \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (‚ÄúCreative Commons‚Äù) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an ‚Äúas-is‚Äù basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor ‚Äì Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter‚Äôs License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter‚Äôs License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter‚Äôs License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter‚Äôs License You apply.\n\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "modules/module11.html",
    "href": "modules/module11.html",
    "title": "Module 11 - Convex Optimization Problems in Statistics",
    "section": "",
    "text": "Homework\nHomework 6 is due in two weeks, next Sunday at midnight\n\n\nLearning Objectives\n\nMaximum Likelihood and Maximum Entropy Methods\nMaximum Posterior Probability and Bayesian Inference\nLogistic Regression\nExperiment Design\n\n\n\nReadings\nSections 7.1, 7.2, and 7.5 of Convex Optimization",
    "crumbs": [
      "Topics",
      "11 - Convex Optimization in Statistics"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8: Convex Functions II",
    "section": "",
    "text": "Learning Objectives\n\nQuasiconvex Functions\nLog-Convexity/Concavity\nGeneralized Inequalities and Convexity\n\n\n\nReadings\nSections 3.4-3.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "8 - Convex Functions II"
    ]
  },
  {
    "objectID": "modules/module14.html",
    "href": "modules/module14.html",
    "title": "Module 14: Stochastic Gradient Descent",
    "section": "",
    "text": "Homework\nHomework 7 is due this Sunday at midnight\n\n\nLearning Objectives\n\nStochastic Gradient Descent\nHyperparameter Tuning\nSaddle Points and Convergence\n\n\n\nReadings\nSection 8.5 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 8 of deeplearningbook",
    "crumbs": [
      "Topics",
      "14 - Stochastic Gradient Descent"
    ]
  },
  {
    "objectID": "modules/module15.html",
    "href": "modules/module15.html",
    "title": "Module 15 - Deep Learning Applications",
    "section": "",
    "text": "Learning Objectives\n\nTraining Deep Neural Networks\n\n\n\nReadings\nGoodfellow Chapter 11 of the deeplearningbook",
    "crumbs": [
      "Topics",
      "15 - Training Deep Neural Networks"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Introduction to Least Squares Problems",
    "section": "",
    "text": "Overview and Deliverables\nHomework 1 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving unconstrained least squares problems\n\nProperties of the Gram matrix and the pseudoinverse\nBasic Numerical Linear Algebra Considerations\n\nApplications:\n\nOptimization\nLinear Regression\n\n\n\n\nReadings\nChapter 12 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "2 - Least Squares Optimization"
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7: Convex Functions",
    "section": "",
    "text": "Learning Objectives\n\nDefinitions of Convex Functions\nImportant Examples of Convex Functions\nJensen‚Äôs Inequality\nOperations that Preserve Convexity\nConvex Conjugates\n\n\n\nReadings\nSections 3.1-2.3 of Convex Optimization",
    "crumbs": [
      "Topics",
      "7 - Convex Functions"
    ]
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to Optimization Problems",
    "section": "",
    "text": "Overview and Deliverables\n\nLearning Objectives\n\nYou will learn about how optimization problems are formalized and the terminology used to describe them\nUbiquity of Optimization problems in data science\nWe will define several important classes of optimization problems:\n\nUnconstrained versus Constrained\nConvex versus Non-Convex\nGlobal versus Local\n\nWhy optimization is hard\n\n\n\nReadings\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf you want to brush up on your linear algebra skills:\nEssential Linear Algebra for Machine Learning\nAppendix A1, A3-A5 of Convex Optimization\nThe mathematical level of those appendices are above what I expect from you in the course, I think the essential linear algebra notes are less intimidating.",
    "crumbs": [
      "Topics",
      "1 - Introduction to Optimization Theory "
    ]
  },
  {
    "objectID": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "href": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "title": "Coding Vignette- Writing Math in Markdown",
    "section": "",
    "text": "When working as a data scientist, and in particular, communicating your results and work in a technical document, you will need to be able to write mathematical expressions. One powerful tool for doing this is called LaTeX. LaTex is a its own programming language designed for rendering technical publications. It is commonly used in math and scientific fields, but a small subset of the language has been adopted in other environments including Microsoft Word and markdown, allowing you to render math using LaTeX commands. Knowing how to do this will be helpful for your homeworks, so I‚Äôve decided to make a short video showing you the very basics of how this works.\nClick here to watch the video on youtube\nThere are a number of documents on the internet which explain the finer details of latex, and most of the lecture slides I shared use it, so you can see examples of how it works. I have also included the ipython notebook"
  },
  {
    "objectID": "posts/2025-02-03-Meetup-2-Slides.html",
    "href": "posts/2025-02-03-Meetup-2-Slides.html",
    "title": "Meetup 2: Least Squares Optimization",
    "section": "",
    "text": "Meetup 2 is tonight at 6:45PM. We will introduce least squares optimization, which is the most commonly used and possibly most important type of optimization problem. We will go through a few applications to illustrate least squares in its simplest formulation, and discuss some practical issues with solving least squares problems.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 2 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "href": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "title": "Welcome to DATA 609",
    "section": "",
    "text": "Welcome to DATA 609, Math Modeling Techniques.\nClick this post now to get started!\nHere are the first steps:\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nJoin our Slack channel:\n\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\n\nI recorded a short video which introduces me and gives you more background on the course:\n\nClick here for slides"
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: George Hagstrom, Ph.D. Class Meetup: Monday 7:00-8:00 Eastern Office Hours: By appointment Email: george.hagstrom@cuny.edu\n\nDescription\nOptimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.\nIn this course you will learn optimization theory through its practical applications to statistics and machine learning. The first half of the course will cover convex optimization (including least squares and linear programming as special cases), and the second half will cover methods for non-convex problems, primarily stochastic gradient descent and Markov-Chain Monte Carlo Methods. Applications will include large-scale linear and logistic regression, regularization, maximum likelihood methods, support vector machines, neural networks, Bayesian statistics, and optimization problems arising in an operations research or other business context.\n\n\nCourse Learning Outcomes\nBy the end of the course, students should be able to:\n\nUnderstand how to formulate major statistical and machine learning algorithms as optimization problems\nLearn how to recognize and solve least squares, linear programming, and convex optimization problems.\nLearn how to represent convex optimization problems in the CVX package\nUnderstand the basics of algorithmic complexity theory and use it to understand how quickly different algorithms will converge to the solution of an optimization problem\nImplement stochastic gradient descent for neural networks and other non-convex problems, understand trade-offs in algorithm design\n\n\n\nProgram Learning Outcomes\n\nBusiness Understanding. Apply frameworks and processes to build out data analytics solutions from understanding of business goals.\nSolid foundational data programming skills, using industry standard tools, essential algorithms, and design patterns for working with structured data, unstructured data and big data.\nSolid foundational math and statistics skills, with emphasis on linear algebra, probability, Bayesian statistics, and numerical methods.\nData understanding. Collect, describe, model, explore and verify data.\nData preparation. Selecting, cleaning, constructing, integrating, and formatting data.\nOptimization Modeling. Selecting optimization modeling techniques, generating test designs, building and assessing models.\nModel implementation and deployment.\nCommunicating results.\n\n\n\nGrading\n\nLabs (80%)\nProject (20%)\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. A homework assignment will be due every other week, see the schedule for details). There will also be a final project required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. I highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Schedule page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course.\n\n\nTextbooks and Course Materials\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\n\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Brightspace, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University‚Äôs policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students‚Äô personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "title: ‚ÄúDATA609 - Math Modeling Techniques‚Äù tbl-colwidths: [10,10,20,5,5,5,5,5,5,5] editor_options: chunk_output_type: console",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#meetup-link",
    "href": "course/schedule.html#meetup-link",
    "title": "DATA 609 Spring 2025",
    "section": "Meetup Link:",
    "text": "Meetup Link:\nClick Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\nDate\nStart Time\nModule\nSlides\nVideo\n\n\n\n\nJan 27\n06:45PM\nIntroduction to Optimization Theory\nMeetup 1 Slides\nMeetup 1 Video\n\n\nFeb 3\n06:45PM\nLeast Squares Optimization\nMeetup 2 Slides\nMeetup 2 Video\n\n\nFeb 10\n06:45PM\nLeast Squares and Statistics\n\n\n\n\nFeb 18\n06:45PM\nConstrained Least Squares and Applications\n\n\n\n\nFeb 24\n06:45PM\nConvex Sets\n\n\n\n\nMar 3\n06:45PM\nConvex Sets II\n\n\n\n\nMar 10\n06:45PM\nConvex Functions\n\n\n\n\nMar 17\n06:45PM\nConvex Functions II\n\n\n\n\nMar 24\n07:0PM\nConvex Optimization Problems\n\n\n\n\nMar 31\n06:45PM\nData Fitting\n\n\n\n\nApr 7\n06:45PM\nApplications to Statistics and Machine Learning\n\n\n\n\nApr 14\n06:45PM\nDuality and Classifiers\n\n\n\n\nApr 21\n\nNo Meetup (Spring Break)\n\n\n\n\nApr 28\n06:45PM\nGradient Descent and Neural Networks\nLab 7\n\n\n\nMay 5\n06:45PM\nStochastic Gradient Descent\n\n\n\n\nMay 12\n06:45PM\nTraining Deep Neural Networks",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA 609 - Math Modeling Techniques",
    "section": "",
    "text": "Optimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]