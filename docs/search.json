[
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructor",
    "section": "",
    "text": "George Hagstrom, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Princeton University where I development mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth’s Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ’omics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You’re encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\n\nThis book provides a strong practical introduction to Linear Algebra. We are primarily using the later parts of the book to learn about applications of least squares. However, you are encouraged to use earlier parts of the book to review linear algebra if needed.\n\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\n\nThis is an excellent book on practical applications of convex optimization. We will use the first half of the book, which focuses on identifying convex optimization problems.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\nThis is a good introduction to neural networks and deep learning.\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\n\nThis textbook provides a concise introduction to optimization for machine learning.\n\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\nThis is free textbook covers a large number of topics that are of relevance to data science, often from a slightly different perspective than standard treatments. It provides an excellent introduction to Bayesian Inference and Neural Networks, but applies ideas from information theory too.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "This class will involve a number of computational homework assignments. These can be completed in a variety of different programming languages. I recommend that you use python or Julia to complete the assignments, but it is also possible to use R.\nGuide to Base Languages:\n\nPython\n\nscipy.linalg is a useful package for linear algebra in python\nCVXPY is the implementation of the CVX package in python\npytorch is a package for neural networks in python\n\nJulia\n\nCONVEX.jl is the implementation of the CVX package in Julia\ntorch.jl provides torch functionality for deep learning in julia.\nflux.jl is another good option for deep learning.\n\nR\n\nCVXR is the implementation of the CVX package in R.\ntorch for R Can be used to implement neural networks in R",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "posts/2025-03-03-Week-6-Constructing-Convex-Sets.html",
    "href": "posts/2025-03-03-Week-6-Constructing-Convex-Sets.html",
    "title": "Week 6- Constructing Convex Sets",
    "section": "",
    "text": "During this week, we will learn how to construct more complex convex sets from the simple examples that we learned about in Week 5. We will use operations that preserve convexity, including intersection, cartesian product, and transformations by affine, perspective, or linear fractional functions.\nWe will also introduce the cvx software and the concept of disciplined convex programming.\nYour third homework assignment is due this Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 6 and complete the week 6 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 3"
  },
  {
    "objectID": "posts/2025-02-14-Linear-Algebra-In_R.html",
    "href": "posts/2025-02-14-Linear-Algebra-In_R.html",
    "title": "Coding Vignette- Linear Algebra in R",
    "section": "",
    "text": "For those of you that prefer to work in R, I decided to make a video going through the basics of matrix manipulations and linear algebra in R. At the end I redo one of the examples from the last meetup. Although matrices aren’t commonly used in introductory R classes, they do become quite prominent for more advanced applications (consider tidymodels which can return matrices where the entries are different models).\nClick here to watch the video on youtube\nThe linear algebra routines are all in base R: Click Here for a Tutorial\nI have also included the R script with the code that I entered"
  },
  {
    "objectID": "posts/2025-02-24-Meetup-5-Slides.html",
    "href": "posts/2025-02-24-Meetup-5-Slides.html",
    "title": "Meetup 5: Introduction to Convex Sets",
    "section": "",
    "text": "I have uploaded the slides for Meetup 5, which happened tonight at 6:45PM\nWe will learn about convex optimization and examples of convex sets, which are used as constraints in convex optimization problems.\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-24-Week-5-Convex-Sets.html",
    "href": "posts/2025-02-24-Week-5-Convex-Sets.html",
    "title": "Week 5-Convex Sets",
    "section": "",
    "text": "During this week we will begin learning the tools we need to formulate and solve convex optimization problems. We will talk more about general optimization problems and look at convexity from the perspective of constraints. We will present the definition of a convex set and give several example, and discuss how these examples arise when formulating problems.\nYour third homework assignment has been posted and is going to be due two Sundays from now.\nHere are more details on what you need to do this week:\n\nGo through module 5 and complete the week 4 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 3"
  },
  {
    "objectID": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "href": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "title": "Welcome to DATA 609",
    "section": "",
    "text": "Welcome to DATA 609, Math Modeling Techniques.\nClick this post now to get started!\nHere are the first steps:\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nJoin our Slack channel:\n\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\n\nI recorded a short video which introduces me and gives you more background on the course:\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-01-27-Meetup-1-Slides.html",
    "href": "posts/2025-01-27-Meetup-1-Slides.html",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "",
    "text": "Meetup 1 is tonight at 6:45PM. We are going to talk about what optimization is, some of its applications, the course structure, and some very basic facts about its theory.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 1 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "href": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "title": "Coding Vignette- Linear Algebra in Numpy",
    "section": "",
    "text": "I’m not sure how many of you have had experience with using python for linear algebra, so I decided to make a short video that goes over the basics of numpy (you are free to use R or Julia as well). I also solve a problem where I re-express the formula for the pseudoinverse of a matrix using the singular value decomposition of that matrix. I did it sort of to show how you might write out a long math expression in markdown, and to talk about matrix algebra and the properties of the SVD. This calculation is quite a bit more invovled than what I am asking on the homework, so don’t be intimidated.\nClick here to watch the video on youtube\nYou may also find the numpy documentation helpful: Click Here\nI have also included the ipython notebook"
  },
  {
    "objectID": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "href": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "title": "Coding Vignette- Writing Math in Markdown",
    "section": "",
    "text": "When working as a data scientist, and in particular, communicating your results and work in a technical document, you will need to be able to write mathematical expressions. One powerful tool for doing this is called LaTeX. LaTex is a its own programming language designed for rendering technical publications. It is commonly used in math and scientific fields, but a small subset of the language has been adopted in other environments including Microsoft Word and markdown, allowing you to render math using LaTeX commands. Knowing how to do this will be helpful for your homeworks, so I’ve decided to make a short video showing you the very basics of how this works.\nClick here to watch the video on youtube\nThere are a number of documents on the internet which explain the finer details of latex, and most of the lecture slides I shared use it, so you can see examples of how it works. I have also included the ipython notebook"
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to Optimization Problems",
    "section": "",
    "text": "Overview and Deliverables\n\nLearning Objectives\n\nYou will learn about how optimization problems are formalized and the terminology used to describe them\nUbiquity of Optimization problems in data science\nWe will define several important classes of optimization problems:\n\nUnconstrained versus Constrained\nConvex versus Non-Convex\nGlobal versus Local\n\nWhy optimization is hard\n\n\n\nReadings\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf you want to brush up on your linear algebra skills:\nEssential Linear Algebra for Machine Learning\nAppendix A1, A3-A5 of Convex Optimization\nThe mathematical level of those appendices are above what I expect from you in the course, I think the essential linear algebra notes are less intimidating.",
    "crumbs": [
      "Topics",
      "1 - Introduction to Optimization Theory "
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7: Convex Functions",
    "section": "",
    "text": "Learning Objectives\n\nDefinitions of Convex Functions\nImportant Examples of Convex Functions\nJensen’s Inequality\nOperations that Preserve Convexity\nConvex Conjugates\n\n\n\nReadings\nSections 3.1-2.3 of Convex Optimization",
    "crumbs": [
      "Topics",
      "7 - Convex Functions"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Introduction to Least Squares Problems",
    "section": "",
    "text": "Overview and Deliverables\nHomework 1 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving unconstrained least squares problems\n\nProperties of the Gram matrix and the pseudoinverse\nBasic Numerical Linear Algebra Considerations\n\nApplications:\n\nOptimization\nLinear Regression\n\n\n\n\nReadings\nChapter 12 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "2 - Least Squares Optimization"
    ]
  },
  {
    "objectID": "modules/module15.html",
    "href": "modules/module15.html",
    "title": "Module 15 - Deep Learning Applications",
    "section": "",
    "text": "Learning Objectives\n\nTraining Deep Neural Networks\n\n\n\nReadings\nGoodfellow Chapter 11 of the deeplearningbook",
    "crumbs": [
      "Topics",
      "15 - Training Deep Neural Networks"
    ]
  },
  {
    "objectID": "modules/module14.html",
    "href": "modules/module14.html",
    "title": "Module 14: Stochastic Gradient Descent",
    "section": "",
    "text": "Homework\nHomework 7 is due this Sunday at midnight\n\n\nLearning Objectives\n\nStochastic Gradient Descent\nHyperparameter Tuning\nSaddle Points and Convergence\n\n\n\nReadings\nSection 8.5 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 8 of deeplearningbook",
    "crumbs": [
      "Topics",
      "14 - Stochastic Gradient Descent"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8: Convex Functions II",
    "section": "",
    "text": "Learning Objectives\n\nQuasiconvex Functions\nLog-Convexity/Concavity\nGeneralized Inequalities and Convexity\n\n\n\nReadings\nSections 3.4-3.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "8 - Convex Functions II"
    ]
  },
  {
    "objectID": "modules/module11.html",
    "href": "modules/module11.html",
    "title": "Module 11 - Convex Optimization Problems in Statistics",
    "section": "",
    "text": "Homework\nHomework 6 is due in two weeks, next Sunday at midnight\n\n\nLearning Objectives\n\nMaximum Likelihood and Maximum Entropy Methods\nMaximum Posterior Probability and Bayesian Inference\nLogistic Regression\nExperiment Design\n\n\n\nReadings\nSections 7.1, 7.2, and 7.5 of Convex Optimization",
    "crumbs": [
      "Topics",
      "11 - Convex Optimization in Statistics"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Extending Linear Regression",
    "text": "Case Study: Extending Linear Regression\nWhat are the validity conditions for linear regression?\n\nLinear Relationship\nIndependent Residuals\nNormal Residuals\n?????"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression-1",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression-1",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Extending Linear Regression",
    "text": "Case Study: Extending Linear Regression\nWhat are the validity conditions for linear regression?\n\nLinear Relationship\nIndependent Residuals\nNormal Residuals\nHomoscedasticity i.e. constant variance"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-homoscedasticity",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-homoscedasticity",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Homoscedasticity",
    "text": "Case Study: Homoscedasticity"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#unequal-variance-common",
    "href": "meetups/Meetup-3/meetup-3.html#unequal-variance-common",
    "title": "DATA 609 Meetup 3",
    "section": "Unequal Variance Common",
    "text": "Unequal Variance Common\n\nTypical for Variance to be proportional to magnitude in some way\n\n\nCould try a data transformation, but isn’t satisfying\n\n\nHeterogeneous detectors\n\n\nHave sensor network with mix of types"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#maximum-likelihood",
    "href": "meetups/Meetup-3/meetup-3.html#maximum-likelihood",
    "title": "DATA 609 Meetup 3",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nConsider following statistical model for \\(y_i\\) in terms of measurements \\(\\mathbf{x}_i\\): \\[\ny_i \\sim \\mathrm{Normal}\\left(\\mathbf{x}_i^T\\mathbf{\\theta},\\sigma_i\\right)\n\\]\n\\(\\sigma_i^2\\) is variance of measurement of \\(y_i\\)\nLikelihood: \\[\n\\log p(\\mathbf{y}|X,\\theta) = \\log\\left(\\Pi_{i=1}^n \\frac{1}{2\\pi\\sigma_i}\\exp\\left(-\\frac{\\left(y_i-\\mathbf{x}_i^T\\mathbf{\\theta}\\right)^2}{2\\sigma_i^2}  \\right) \\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#maximum-likelihood-1",
    "href": "meetups/Meetup-3/meetup-3.html#maximum-likelihood-1",
    "title": "DATA 609 Meetup 3",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nConsider following statistical model for \\(y_i\\) in terms of measurements \\(\\mathbf{x}_i\\): \\[\ny_i \\sim \\mathrm{Normal}\\left(\\mathbf{x}_i^T\\mathbf{\\theta},\\sigma_i\\right)\n\\]\n\\(\\sigma_i^2\\) is variance of measurement of \\(y_i\\)\nLikelihood: \\[\n\\log p(\\mathbf{y}|X,\\theta) = -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^n\\log(\\sigma_i) -\n\\left\\|\\mathrm{diag}\\left(1/\\sigma_i\\right)\\left( X \\mathbf{\\theta} - \\mathbf{y}\\right)\\right\\|^2\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#weighted-least-squares",
    "href": "meetups/Meetup-3/meetup-3.html#weighted-least-squares",
    "title": "DATA 609 Meetup 3",
    "section": "Weighted Least Squares",
    "text": "Weighted Least Squares\n\nMaximum Likelihood Estimate for the Unequal Variance Problem is a Weighted Least Squares optimization:\n\n\\[\n\\min_{\\mathbf{\\theta}} \\|W^{1/2}(X\\mathbf{\\theta} - \\mathbf{y})\\|^2\n\\]\n\nHere the matrix \\(W\\) is a diagonal matrix with \\(\\frac{1}{\\sigma_i^2}\\) on the diagonal entries\nLarge \\(\\sigma_i\\) means error of that term is less important \\[\nX^TWX \\mathbf{\\theta} = X^TW\\mathbf{y}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#gauss-markov-theorem",
    "href": "meetups/Meetup-3/meetup-3.html#gauss-markov-theorem",
    "title": "DATA 609 Meetup 3",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nCan extend this result to scenario when measurement errors have a given covariance matrix \\(\\Gamma\\)\nThen \\(W=\\Gamma^{-1}\\) and Weighted Least Squares provides maximum likelihood estimate\nBest Linear Uniased Estimator"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#when-is-wls-useful",
    "href": "meetups/Meetup-3/meetup-3.html#when-is-wls-useful",
    "title": "DATA 609 Meetup 3",
    "section": "When is WLS useful?",
    "text": "When is WLS useful?\n\nSample more of one group than another\nErrors vary based on detector or value\nCare about some predictions on some data more than others"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#weekly-summary",
    "href": "meetups/Meetup-3/meetup-3.html#weekly-summary",
    "title": "DATA 609 Meetup 3",
    "section": "Weekly Summary",
    "text": "Weekly Summary\n\nReading: Chapter 13 of VMLS\nHW 2 Available, due in 2 weeks\n\nProblem 1 and 2 based on this week\nProblem 3 next week\n\nWill release another numpy coding video, lmk if you prefer a different language"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#data-fitting",
    "href": "meetups/Meetup-3/meetup-3.html#data-fitting",
    "title": "DATA 609 Meetup 3",
    "section": "Data Fitting",
    "text": "Data Fitting\n\nHave some observations \\(y_i\\) and \\(\\mathbf{x}_i\\) and want to fit a regression\n\\(m\\) observations, have \\(n\\) basis functions \\(f\\) \\[\ny_i \\sim \\mathrm{Normal}\\left(\\sum_{j=1}^{n} \\theta_j f_j(\\mathbf{x}_i),\\sigma\\right)\n\\]\nNormal errors gives justification for least squares"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#data-fitting-solution",
    "href": "meetups/Meetup-3/meetup-3.html#data-fitting-solution",
    "title": "DATA 609 Meetup 3",
    "section": "Data Fitting Solution",
    "text": "Data Fitting Solution\nCan form matrix \\(A\\): \\[\nA = \\begin{bmatrix}\nf_1(\\mathbf{x}_1) & f_2(\\mathbf{x}_1) & \\cdots & f_n(\\mathbf{x}_1) \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nf_1(\\mathbf{x}_m) & f_2(\\mathbf{x}_m) & \\cdots & f_n(\\mathbf{x}_m)\n\\end{bmatrix}\n\\]\n\nAnd find: \\[\n\\min_{\\theta} \\|A\\mathbf{\\theta} - \\mathbf{y}\\|^2\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series",
    "href": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Autoregressive Time Series",
    "text": "Example: Autoregressive Time Series\n\nSuppose we have observations \\(y_t\\) and want to predict new \\(y\\) based on previous observations \\[\ny_t = \\theta_1 y_{t-1} + \\theta_2 y_{t-2}+\\cdots+\\theta_n y_{t-n}\n\\]\nHow to write this in our framework?"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series-1",
    "href": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series-1",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Autoregressive Time Series",
    "text": "Example: Autoregressive Time Series\n\nSuppose we have observations \\(y_t\\) and want to predict new \\(y\\) based on previous observations \\[\ny_t = \\theta_1 y_{t-1} + \\theta_2 y_{t-2}+\\cdots+\\theta_n y_{t-n}\n\\]\nHow to write this in our framework?\nDefine \\(\\mathbf{x}_i = \\begin{bmatrix} y_{t-1} & y_{t-2} & \\cdots & y_{t-n} \\end{bmatrix}\\)"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-central-park-temperature",
    "href": "meetups/Meetup-3/meetup-3.html#example-central-park-temperature",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Central Park Temperature",
    "text": "Example: Central Park Temperature\n\nHourly Central Park Temperature from Jan-March 2023"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#how-to-predict",
    "href": "meetups/Meetup-3/meetup-3.html#how-to-predict",
    "title": "DATA 609 Meetup 3",
    "section": "How to Predict",
    "text": "How to Predict\n\nBaseline Mean and Variance:\n\n\\({T}_{\\mathrm{mean}} = 42.5\\), \\(\\sigma_T = 8.57\\)\n\nGuess Temperature from 24 hours before\n\n\\(\\sigma_{lag} = 1.41\\)\n\nHow about using the past 8 hours?: \\[\nT_t = \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#setting-up-the-model",
    "href": "meetups/Meetup-3/meetup-3.html#setting-up-the-model",
    "title": "DATA 609 Meetup 3",
    "section": "Setting up the model",
    "text": "Setting up the model\n\nA = np.array([central_park[\"temp\"].values[i:i+8].T \nfor i in  range(len(central_park)-8)])\nx = central_park[\"temp\"].values[8:]\ntheta = np.linalg.lstsq(A,x)[0]\n\n\nSlight Improvement \\(\\sigma = 1.25\\)"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#predictions",
    "href": "meetups/Meetup-3/meetup-3.html#predictions",
    "title": "DATA 609 Meetup 3",
    "section": "Predictions",
    "text": "Predictions\n\n\nText(0, 0.5, 'Temperature')"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#more-time-series",
    "href": "meetups/Meetup-3/meetup-3.html#more-time-series",
    "title": "DATA 609 Meetup 3",
    "section": "More Time Series",
    "text": "More Time Series\n\nCan incorporate trends: \\[\nT_t = \\theta_0(i-i_0) + \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]\nOther variables or functions \\[\nT_t = \\theta_{m}\\mathrm{Month}(i) + \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#generalization-and-validation",
    "href": "meetups/Meetup-3/meetup-3.html#generalization-and-validation",
    "title": "DATA 609 Meetup 3",
    "section": "Generalization and Validation",
    "text": "Generalization and Validation\n\nGoal usually isn’t to find model with absolute lowest RMS error\nInstead want a model that predicts well on new data\nTwo goals often in conflict"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-polynomial-interpolation",
    "href": "meetups/Meetup-3/meetup-3.html#example-polynomial-interpolation",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Polynomial Interpolation",
    "text": "Example: Polynomial Interpolation\n\nFitting high degree polynomials famously leads to overfitting \\[\ny_i = \\sum_{j=1}^n \\theta_j x^{j-1}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#fit-results",
    "href": "meetups/Meetup-3/meetup-3.html#fit-results",
    "title": "DATA 609 Meetup 3",
    "section": "Fit Results",
    "text": "Fit Results\n\nCurve develops unnecessary features at higher order"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#generalization-error",
    "href": "meetups/Meetup-3/meetup-3.html#generalization-error",
    "title": "DATA 609 Meetup 3",
    "section": "Generalization Error",
    "text": "Generalization Error\n\nAt high order, generalization fails"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#regularization",
    "href": "meetups/Meetup-3/meetup-3.html#regularization",
    "title": "DATA 609 Meetup 3",
    "section": "Regularization",
    "text": "Regularization\n\nAnother way to handle overfitting is with regularization\nAdd term proportional to \\(\\|\\mathbf{\\theta}\\|^2\\) \\[\n\\min_{\\mathbf{\\theta}} \\|A\\mathbf{\\theta}-\\mathbf{y}\\|^2 + \\lambda\\|\\mathbf{\\theta}\\|^2\n\\]\nUse if \\(A\\) is ill-conditioned or many parameters."
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nName comes from geometry\nConsider objective function with a “ridge”"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression-1",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression-1",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nName comes from geometry\nRegularizer turns ridge to peak"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#bayesian-linear-regression",
    "href": "meetups/Meetup-3/meetup-3.html#bayesian-linear-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\n\\[\n\\mathbf{y} \\sim \\mathrm{Normal}\\left(X\\theta,\\sigma^2 I\\right) \\\\\n\\mathbf{\\theta} \\sim \\mathrm{Normal}\\left(0, \\frac{1}{\\lambda} I\\right)\n\\]\n\n\\(\\frac{1}{\\lambda}\\) is variance of prior distribution on \\(\\theta\\)\n\\(p(\\theta|X)\\) is normal\n\\(E(\\theta|X)\\) is least squares ridge regression solution"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression-solution",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression-solution",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression Solution",
    "text": "Ridge Regression Solution\n\nEquivalent to: \\[\n\\left(A^TA + \\lambda I\\right)\\mathbf{\\theta} = A^T\\mathbf{y}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#feature-engineering",
    "href": "meetups/Meetup-3/meetup-3.html#feature-engineering",
    "title": "DATA 609 Meetup 3",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nCan use domain knowledge to create custom features in your data\nNew feature vectors: \\(f_1(\\mathbf{x})\\), \\(f_2(\\mathbf{x}), \\cdots\\)\nStandard data transforms, log, z-score\nThresholding: \\(f(x) = \\max(0,x)\\)\nDomain Knowledge: \\[ \\mathrm{BMI} =\\frac{\\left(\\mathrm{Weight}\\right)}{\\left(\\mathrm{Height}\\right)^2}\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#thanks",
    "href": "meetups/Meetup-3/meetup-3.html#thanks",
    "title": "DATA 609 Meetup 3",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#week-summary",
    "href": "meetups/Meetup-6/meetup-6.html#week-summary",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Week Summary",
    "text": "Week Summary\n\nCovering Section 2.3 of cvxbook\nMaterial is unusual, normal to feel confused\nWe will use cvx for first time\n\ncvxpy\ncvxr\n\nLab 3 Due this Sunday"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#start-thinking-about-miniproject",
    "href": "meetups/Meetup-6/meetup-6.html#start-thinking-about-miniproject",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Start Thinking About Miniproject",
    "text": "Start Thinking About Miniproject\n\nOver next month, start thinking about how your interests relate with something we have covered or are going to cover\nLeast Squares, Convex Optimization, or Deep Learning/non-convex optimization\nAsk me if you want suggestions\nTarget complexity is homework assignment"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#addendum-on-cones",
    "href": "meetups/Meetup-6/meetup-6.html#addendum-on-cones",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Addendum on Cones",
    "text": "Addendum on Cones\n\nNot all cones are convex!"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-1",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-2",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex?",
    "text": "How to show a set is convex?\n\nSuppose you have a problem and some constraints\n\n\nBrute force math:\n\n\nTry to prove that for every \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in set, \\(\\theta\\mathbf{x}_1+(1-\\theta)\\mathbf{x}_2\\) is in the set, if \\(0\\leq\\theta\\leq1\\)\nOnly for simplest sets"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-1",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex",
    "text": "How to show a set is convex\n\nUsing convex functions (next weeks)\n\n\nGraph of a convex function\nTechnically how CVX works"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-2",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex",
    "text": "How to show a set is convex\n\nShow how to build your set from simpler convex sets\n\n\nIntersection\nTransformations: Linear/Affine, Perspective, Linear Fractional Mapping"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-not-convex",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-not-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is (not) convex",
    "text": "How to show a set is (not) convex\n\nIf you are stuck, brute force numerics\n\n\nGenerate random points in set\nCheck random convex combinations\nWait a long time"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection",
    "href": "meetups/Meetup-6/meetup-6.html#intersection",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection",
    "text": "Intersection\n\nIntersection is shared points\n\n\n\n\\[ C_3 = C_1 \\cap C_2 \\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersections-of-convex-are-convex",
    "href": "meetups/Meetup-6/meetup-6.html#intersections-of-convex-are-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersections of Convex are Convex",
    "text": "Intersections of Convex are Convex\n\n\n\n\n\nEven an infinite intersection"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nSuppose we can several locations where we are allowed to build some quantity of renewable energy plants."
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-1",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\n\\(p_i(t)\\) power function for each plant over a typical day"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-2",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\n\\(p_i(t)\\) power function for each plant over a typical day"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-3",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-3",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nLet \\(w_i\\) stand for the size of renewable plant we build at location \\(i\\)."
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-4",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-4",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nThe total power generated at time \\(t\\): \\[\np_{tot}(t) = \\mathbf{p}(t)^T\\mathbf{w}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints",
    "text": "Infinite Constraints\n\nGuarantee that the level of power above demand level:\n\n\\[\n\\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-1",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints",
    "text": "Infinite Constraints\n\nFor each individual time point \\(t\\), this inequality describes a convex set\nBecause \\(\\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t)\\) linear in \\(\\mathbf{w}\\)\nTake intersection over \\(t\\) \\[\nC = \\bigcap_t \\{\\mathbf{w}\\,|\\, \\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t) \\}\n\\]\n\\(C\\) is a convex set"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nCan apply a function \\(f\\) to an entire set to generate a new set\nDoesn’t have to be in the same space: \\(f:\\, \\mathbb{R}^m \\mapsto \\mathbb{R}^n\\)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set-1",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nImage of \\(C\\) under \\(f\\):\n\n\\[f(C) = \\{x\\, |\\, x = f(y)\\, \\mathrm{for}\\, y \\in C\\}\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set-2",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nOther Direction: Preimage of \\(C\\) under \\(f\\):\n\n\\[ f^{-1}(C) = \\{x\\, |\\, f(x) \\in C\\} \\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#linearaffine-transformations",
    "href": "meetups/Meetup-6/meetup-6.html#linearaffine-transformations",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Linear/Affine Transformations",
    "text": "Linear/Affine Transformations\n\nIf \\(C\\) is convex, and \\(f\\) is affine \\(f(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b}\\)\n\n\\(f(C)\\) is convex\n\\(f^{-1}(C)\\) is convex\n\nCan scale and shift convex sets"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#implication",
    "href": "meetups/Meetup-6/meetup-6.html#implication",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Implication:",
    "text": "Implication:\n\nConsider following set of constraints:\n\n\\[\nA\\mathbf{x} \\leq \\mathbf{d} \\\\\nx_1^2 + x_2^2 \\leq 4\n\\]\n\nFirst constraint is convex set\nSecond one describes a set where \\(x_1\\) and \\(x_2\\) are inside a circle, but rest of coordinates unconstrained"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#implication-1",
    "href": "meetups/Meetup-6/meetup-6.html#implication-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Implication:",
    "text": "Implication:\n\nDefine operator \\(P\\) that is identity on \\(x_1\\) and \\(x_2\\) and ignores the rest:\n\n\\[\nP = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\\n                    0 & 1 & 0 & \\cdots & 0 \\end{bmatrix}\n\\]\n\nCircle of radius \\(2\\) in \\(\\mathbb{R}^2\\) is convex\nPreimage of circle under \\(P\\) is also convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#cartesian-product",
    "href": "meetups/Meetup-6/meetup-6.html#cartesian-product",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Cartesian Product",
    "text": "Cartesian Product\n\nHave two sets of variables \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\).\nHave two convex sets \\(C_1\\) and \\(C_2\\), over each set of variables\nCartesian product of \\(C_1\\) and \\(C_2\\) is convex: \\[\nC_1\\times C_2 = \\{ \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\end{bmatrix} \\, | \\, \\mathbf{x}_1 \\in C_1, \\mathbf{x}_2 \\in C_2 \\}\n\\]\nCan freely combine convex sets in different groups of variables"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line",
    "href": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection with an arbitrary line",
    "text": "Intersection with an arbitrary line\n\n\\(C\\) is convex if and only if \\(C \\cap L\\) is convex for every line \\(L\\)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line-1",
    "href": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection with an arbitrary line",
    "text": "Intersection with an arbitrary line\n\n\\(C\\) is convex if and only if \\(C \\cap L\\) is convex for every line \\(L\\)\nIf \\(C\\) is convex, then it is true by intersection\nIf all intersections are convex, then all intersections are segments so \\(C\\) is convex by the definition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#application-quadratic-inequality",
    "href": "meetups/Meetup-6/meetup-6.html#application-quadratic-inequality",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Application: Quadratic Inequality",
    "text": "Application: Quadratic Inequality\n\nConsider \\(A\\succeq 0\\)\n\\[\n\\{\\mathbf{x}\\, |\\, \\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\}\n\\]\nWhy intuitively should this be convex?\n\n\\(\\mathbf{x}^TA\\mathbf{x} \\leq \\alpha\\) is an ellipsoid"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality\n\nConsider \\(A\\succeq 0\\)\n\\[\n\\{\\mathbf{x}\\, |\\, \\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\}\n\\]\nDefine line \\(L = \\{\\mathbf{x}\\, |\\, \\mathbf{x} = t\\mathbf{v} + \\mathbf{v}_0,\\, t\\in\\mathbb{R} \\}\\)\nSubstitute: \\[\nt^2 (\\mathbf{v}^TA\\mathbf{v} ) + t(2\\mathbf{v}^TA\\mathbf{v_0} + \\mathbf{v}^T\\mathbf{b}) \\leq \\alpha - \\mathbf{v_0}^T A\\mathbf{v_0} - \\mathbf{v}_0^T\\mathbf{b}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-1",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality\n\nTwo possibilities:\n\\(\\mathbf{v}^T A \\mathbf{v} = 0\\)\n\nThen it is convex because polyhedron\n\n\\(\\mathbf{v}^TA\\mathbf{v} \\geq 0\\)\n\\(\\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\) is convex\nDoesn’t work if inequality is in other direction"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-2",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-problem",
    "href": "meetups/Meetup-6/meetup-6.html#example-problem",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example Problem",
    "text": "Example Problem\n\nSuppose we have a variable \\(x\\) which takes values \\(a_i\\) for \\(i=1,\\cdots,n\\)\n\\(p(x=a_i) = p_i\\) is a probability distribution\nIs the set \\(\\{\\mathbf{p}\\, |\\, \\mathrm{var}(x) \\geq \\alpha\\}\\) convex?"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-approach",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-approach",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to approach?",
    "text": "How to approach?\n\n\\(\\mathrm{var}(x) = \\sum_{i} p_i(a_i - \\bar{x})^2\\)\nExpand it out more: \\[\n\\mathrm{var}(x) = \\sum_{i} \\left(p_i a_i^2 \\right) - \\left(\\sum_i p_i a_i\\right)^2 \\geq \\alpha\n\\]\nThis is a quadratic inequality with matrix \\(A=\\mathrm{diag}(a_i^2)\\)\nThus convex\nThen intersection with simplex also convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-problem-1",
    "href": "meetups/Meetup-6/meetup-6.html#example-problem-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example Problem",
    "text": "Example Problem\n\nWhat if we wanted variance lower than \\(\\alpha\\)?\nNot convex in general"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#perspective-function",
    "href": "meetups/Meetup-6/meetup-6.html#perspective-function",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Perspective Function",
    "text": "Perspective Function\n\n\\(f(\\mathbf{x},t) = \\frac{\\mathbf{x}}{t}\\), for \\(t&gt;0\\)\nInterpretation: Image taken through a pinhole camera with hole at origin, surface at \\(t=-1\\)\n\\(f(C)\\) and \\(f^{-1}(C)\\) both convex if \\(C\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#linear-fractional-function",
    "href": "meetups/Meetup-6/meetup-6.html#linear-fractional-function",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Linear Fractional Function",
    "text": "Linear Fractional Function\n\nPerspective is special case\n\\(f(\\mathbf{x}) = \\frac{A\\mathbf{x}+\\mathbf{b}}{\\mathbf{c}^T\\mathbf{x}+\\mathbf{d}}\\)\n\nfor \\(\\mathbf{c}^T\\mathbf{x} + \\mathbf{d} \\gt 0\\)\n\n\\(f(C)\\) and \\(f^{-1}(C)\\) both convex if \\(C\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-conditional-probability",
    "href": "meetups/Meetup-6/meetup-6.html#example-conditional-probability",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example: Conditional Probability",
    "text": "Example: Conditional Probability\n\nConsider discrete probability distribution over two variables: \\[p(i,j) = \\mathrm{prob(x=i,y=j)}\\]\nConditional probability \\(p(i|j) = \\frac{p(i,j)}{\\sum_{i'}p(i',j)}\\)\nLinear fractional function of joint probabilities\nA convex set of joint probabilities becomes a convex set of conditional probabilities"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nCVX is software that solves convex optimization problems\nBased on disciplined convex programming or DCP\nHas library of functions with known curvature\nCVX verifies convexity of objectives and constraints"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-1",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nBuild expressions out of variables and constants\n\n\nimport cvxpy as cvx\n\n# 6 dimensional vector of unknowns\nx = cvx.Variable(6)\n\n# 12x6 dimensional matrix of unknowns\nA = cvx.Variable((12,6))\n\n# Scalar variable\ny = cvx.Variable()\n\n# Constants are numpy ndarrays\n\nx_mean = np.random.randn(6)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-2",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\natomic functions define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check convexity?\nobj = cvx.Minimize(cvx.norm(x))\nprint(obj.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-3",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-3",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check convexity of constraints?\n\nprint((x - x_mean &gt;= - margin).is_dcp())\nprint((x - x_mean &lt;=   margin).is_dcp())\n\nTrue\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-4",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-4",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check everything at once?\n\nprint(problem.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-5",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-5",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check everything at once?\n\nprint(problem.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-6",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-6",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nSolve using problem.solve()\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Solve the problem\n\nsol = problem.solve()\nprint(sol) # objective\nx.value # optimum\n\n0.981112635570473\n\n\narray([-1.93168495e-01, -9.68625134e-02, -6.18882600e-05, -9.43753927e-01,\n       -1.58789509e-01, -4.98297019e-05])"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-7",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-7",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nHow about weighted least squares?\n\n\nmargin = 0.2\nw = np.sqrt(np.random.random(6))\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(np.diag(w) @ x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n\\[\n\\min_{x} \\|\\mathrm{diag}(w_i)\\mathbf{x}\\| \\\\\n\\mathbf{x}-\\mathbf{x}_0 \\geq -\\sigma \\\\\n\\mathbf{x} - \\mathbf{x}_0 \\leq \\sigma\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-8",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-8",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\n# Solve the problem\n\nsol = problem.solve()\nprint(sol) # objective\nx.value # optimum\n\n0.5503561437710429\n\n\narray([-1.93168497e-01, -9.68625162e-02, -2.20648824e-06, -9.43753929e-01,\n       -1.58789511e-01, -1.49039973e-05])"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#thanks",
    "href": "meetups/Meetup-6/meetup-6.html#thanks",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nWhy can’t you build a car with:\n\nSpeed of Ferrari\nPracticality of minivan\n\n\n\nFerrari Concept"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-1",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nSize brings weight and poor aerodynamics\nPower hurts durability\nCost through the roof\n\n\nFerrari Concept"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-2",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nThis theme applies throughout life and is at the heart of optimization\nYour marketing team gives you a budget\nYour investors don’t want to risk everything\nYour rocketship only has so much fuel"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-3",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-3",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nWithout constraints, your optimization solution may be meaningless\nFiguring out the important constraints is a key part of modeling\nToday we will start to deal with a simple type of constraint in the context of least squares"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#weekly-summary",
    "href": "meetups/Meetup-4/meetup-4.html#weekly-summary",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Weekly Summary",
    "text": "Weekly Summary\n\nHomework 2 due next Sunday 2/23 at midnight\nReadings for the week are Chapter 16 and 17 of VMLS\nWe will talk about multi-objective problems later (skipped chapter 15)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week!",
    "text": "Events Happening this Week!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-1",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week",
    "text": "Events Happening this Week\n\nCareer Readiness Bootcamp\n\n\nWednesday Februar 19th, Room 407 319 West 31st Street\n\nIn person link\nVirtual link"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-2",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening This Week!",
    "text": "Events Happening This Week!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-3",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-3",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week",
    "text": "Events Happening this Week\n\nNew York Open Statistical Computing Meetup\n\n\n6:30-8:30+, Pless Hall NYU\n$7 tickets must RSVP\nWe have a group that attends these monthly meetups\nI won’t be there this week not sure how the attendance will be\n\nRegister by Clicking Here"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#constrained-least-squares",
    "href": "meetups/Meetup-4/meetup-4.html#constrained-least-squares",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Constrained Least Squares",
    "text": "Constrained Least Squares\n\nWe will learn how to solve least squares with linear equality constraints today\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 \\\\\nC\\mathbf{x} = \\mathbf{d}\n\\]\n\nHere \\(C\\) is a wide matrix\n\\(\\mathbf{x}\\) restricted to hyperplane"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#alternative-formulation",
    "href": "meetups/Meetup-4/meetup-4.html#alternative-formulation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Alternative Formulation",
    "text": "Alternative Formulation\n\nCan also think of this as having \\(p\\) linear constraints\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2, \\\\\n\\mathbf{c}_i^T\\mathbf{x} = d_i,\\quad i=1,...,p\n\\]\n\nHere \\(C\\) is a wide matrix\n\\(\\mathbf{x}\\) restricted to hyperplane"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#why-just-equality",
    "href": "meetups/Meetup-4/meetup-4.html#why-just-equality",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Why Just Equality?",
    "text": "Why Just Equality?\n\nInequality constraints are useful too:\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 \\\\\n\\mathbf{c}_i^T\\mathbf{x} \\leq d_i,\\quad i=1,...,p\n\\]\n\nCould do it for one inequality constraint\nBut for more than one you have a complicated region\nNeed methods from later in the semester"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-b-splines",
    "href": "meetups/Meetup-4/meetup-4.html#example-b-splines",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example: B-Splines",
    "text": "Example: B-Splines\n\nSuppose you have a noisy dataset and you want to fit it with a smoother curve\nTalked about polynomials last week \\[\nf(x) = \\sum_{i=0}^p \\theta_i x^i\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features",
    "href": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Polynomials are Bad Features",
    "text": "Polynomials are Bad Features\n\nPolynomials have strong symmetries\nPolynomials “blow-up” at edge of domain"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features-1",
    "href": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Polynomials are Bad Features",
    "text": "Polynomials are Bad Features\n\n\nWouldn’t it make more sense to use features with more local structure?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-b-splines-1",
    "href": "meetups/Meetup-4/meetup-4.html#example-b-splines-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example B-Splines",
    "text": "Example B-Splines\n\nIdea of B-Splines is to have local features\nDivide Domain into regions"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region",
    "href": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Do a fit within each region",
    "text": "Do a fit within each region\n\nHave a small number of basis functions within each region\nPiecewise Constant: Basis is \\(1\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region-1",
    "href": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Do a fit within each region",
    "text": "Do a fit within each region\n\nB-Spline uses more polynomials\nCubic is most common \\(1\\), \\(x^2\\), \\(x^3\\), \\(x^4\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#piecewise-cubic-fit",
    "href": "meetups/Meetup-4/meetup-4.html#piecewise-cubic-fit",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Piecewise Cubic Fit",
    "text": "Piecewise Cubic Fit\n\n\nNot the jumps at each of the knots\nTo make the fit function “smooth” we need to add constraints"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#problem-that-was-solved",
    "href": "meetups/Meetup-4/meetup-4.html#problem-that-was-solved",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Problem that was Solved",
    "text": "Problem that was Solved\n\nProblem so far: \\[f_{ij}(x) = x^{i-1},\\quad \\mathrm{if: }\\quad  \\mathrm{knot}_j \\leq x &lt; \\mathrm{knot}_{j+1},\\\\\nf_{ij}(x) = 0,\\quad \\mathrm{otherwise} \\]\n\\(i \\in 1\\,...\\,4\\), \\(j \\in 1\\, ... \\mathrm{number\\, of\\, knots}\\) \\[ A_{k,(4(j-1)+i)} = f_{ij}(x_k) \\] \\[ \\min_{\\theta} \\|A\\mathbf{\\theta} - \\mathbf{x}\\|^2\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#adding-constraints",
    "href": "meetups/Meetup-4/meetup-4.html#adding-constraints",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Adding Constraints",
    "text": "Adding Constraints\n\nWant the predictions to match at each knot: \\[ \\sum_{i=1}^{4} \\theta_{ij}\\left(\\mathrm{knot}_j\\right)^{i-1} = \\sum_{i=1}^{4} \\theta_{i,j+1}\\left(\\mathrm{knot_{j}}\\right)^{i-1} \\]\nAlso want derivatives to match at each knot: \\[\n\\sum_{i=2}^{4} (i-1)\\theta_{ij}\\left(\\mathrm{knot}_j\\right)^{i-2} = \\sum_{i=2}^{4} (i-1)\\theta_{i,j+1}\\left(\\mathrm{knot_{j}}\\right)^{i-2}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#full-problem",
    "href": "meetups/Meetup-4/meetup-4.html#full-problem",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Full Problem",
    "text": "Full Problem\n\nEach knot produces two linear equations\nCan write overall as \\(C\\mathbf{\\theta} = \\mathbf{d}\\)\nFull Problem:\n\n\\[\\mathrm{find:}\\quad \\min_{\\mathbf{\\theta}}\\|A\\mathbf{\\theta} - \\mathbf{x}\\|^2 \\\\\n\\mathrm{subject\\, to:\\quad C\\mathbf{\\theta} = \\mathbf{d}}\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#how-to-solve-it",
    "href": "meetups/Meetup-4/meetup-4.html#how-to-solve-it",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "How to solve it?",
    "text": "How to solve it?\n\nBrute force way\n\n\\(C\\mathbf{\\theta} = \\mathbf{d}\\) can be solved\nGet a “hyperplane” of solutions because underdetermined\nReformulate least squares and solve\n\nOr use Lagrange Multipliers"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#lagrange-multipliers",
    "href": "meetups/Meetup-4/meetup-4.html#lagrange-multipliers",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Lagrange Multipliers",
    "text": "Lagrange Multipliers\n\nConstrained problem into unconstrained\n\n\\[\\mathrm{find:}\\quad \\min_{\\mathrm{\\theta}} \\mathbf{f}(\\mathbf{\\theta}) \\\\\n\\mathrm{subject\\, to:} \\quad \\mathbf{g}(\\mathbf{\\theta}) = 0 \\]\n\nLagrangian: \\[ L(\\mathbf{\\theta},\\mathbf{z}) = f(\\mathbf{\\theta})-\\mathbf{z}^Tg(\\mathbf{\\theta})\\]\nCritical points of \\(L\\) are potential minima of the constrained problem"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#why-does-it-work",
    "href": "meetups/Meetup-4/meetup-4.html#why-does-it-work",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Why does it work?",
    "text": "Why does it work?\n\nCritical points of \\(L\\): \\[\n\\frac{\\partial L(\\mathbf{\\theta},\\mathbf{z})}{\\partial\\theta} = \\nabla \\mathbf{f}(\\theta) - \\mathbf{z}^T\\nabla \\mathbf{g}(\\theta) = 0 \\\\\n\\frac{\\partial L(\\mathbf{\\theta},\\mathbf{z})}{\\partial\\theta} = \\mathbf{g}(\\theta) = 0\n\\]\nGet back the constraint equations\nFirst equation says that gradient of objective has to be made parallel to gradients of constraints"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#geometric-explanation",
    "href": "meetups/Meetup-4/meetup-4.html#geometric-explanation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Geometric Explanation",
    "text": "Geometric Explanation\n\n\\(\\mathbf{g(\\theta)} = 0\\) defines a lower dimensional shape\n\nThe more equations the less dimensionality of the shape\n\\(n\\) equations gives points (0D)\n\\(n-1\\) equations gives curves (1D)\n\\(n-2\\) equations gives surfaces (2D)\n\n\\(\\mathbf{z}^T\\nabla\\mathbf{g}(\\theta)\\) is a linear combination of constraint gradients"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#geometric-explanation-1",
    "href": "meetups/Meetup-4/meetup-4.html#geometric-explanation-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Geometric Explanation",
    "text": "Geometric Explanation"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#lagrange-multiplier-for-least-squares",
    "href": "meetups/Meetup-4/meetup-4.html#lagrange-multiplier-for-least-squares",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Lagrange Multiplier For Least Squares",
    "text": "Lagrange Multiplier For Least Squares\n\\[\nL(\\mathbf{\\theta},\\mathbf{z}) = \\|A\\mathbf{\\theta}-\\mathbf{x}\\|^2 + \\mathbf{z}^T\\left(C\\mathbf{\\theta}-\\mathbf{d}\\right)\n\\]\n\nGives us:\n\n\\[\n\\frac{\\partial L(\\theta,\\mathbf{z})}{\\partial \\theta} =\n  2A^T\\left(A\\mathbf{\\theta} - \\mathbf{x}\\right) + C^T\\mathbf{z} = 0,\\\\\n  \\frac{\\partial L(\\theta,\\mathbf{z})}{\\partial \\theta} = C\\theta -\\mathbf{d} = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#kkt-matrix",
    "href": "meetups/Meetup-4/meetup-4.html#kkt-matrix",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "KKT Matrix",
    "text": "KKT Matrix\n\nLagrange multiplier equations are linear, can write a matrix equation: \\[\n\\begin{bmatrix}\n2A^TA & C^T \\\\\nC & 0\n\\end{bmatrix}\n\\begin{bmatrix} \\mathbf{\\theta} \\\\ \\mathbf{z}  \\end{bmatrix}\n=\n\\begin{bmatrix} 2A^T\\mathbf{x} \\\\ \\mathbf{d} \\end{bmatrix}\n\\]\nBottom row implements the constraints\nMatrix is called KKT matrix"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#conditions-on-a-and-c",
    "href": "meetups/Meetup-4/meetup-4.html#conditions-on-a-and-c",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Conditions on A and C",
    "text": "Conditions on A and C\n\nWhen is KKT matrix invertible?\n\\(n+p\\times n+p\\)\n\\(C\\) has indepdendent\n\nOtherwise can’t find \\(\\mathbf{z}\\)\nImplies \\(C\\) is wide\n\n\\(\\begin{bmatrix} A \\\\ C\\end{bmatrix}\\) has independent columns\n\nGo wrong if not enough data"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices",
    "href": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "\\(\\mathbf{z}\\) are Shadow Prices",
    "text": "\\(\\mathbf{z}\\) are Shadow Prices\n\nWhat is the meaning of the \\(\\mathbf{z}\\) from the solution?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices-1",
    "href": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "\\(\\mathbf{z}\\) are Shadow Prices",
    "text": "\\(\\mathbf{z}\\) are Shadow Prices\n\nWhat is the meaning of the \\(\\mathbf{z}\\) from the solution?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#shadow-prices",
    "href": "meetups/Meetup-4/meetup-4.html#shadow-prices",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Shadow Prices",
    "text": "Shadow Prices\n\nLet’s say your constraint is negotiable\nChange your constraint to \\(C\\theta = \\mathbf{d}+\\mathbf{\\delta}\\)\nObjective changes by: \\[ \\|A\\theta_{new}-\\mathbf{x}\\|^2 - \\|A\\theta_{old} - \\mathbf{x}\\|^2 \\approx \\mathbf{z}^T\\delta \\]\nIf your objective function was a cost then \\(\\mathbf{z}\\) can be interpreted as a price per unit of constraint."
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-knots",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-knots",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Splines: Knots",
    "text": "Implementing the B-Splines: Knots\n\nnum_knots = 15\nknot_list &lt;- quantile(cherry_blossoms_2$year,\n                      probs = seq(from = 0, to = 1, \n                                length.out = num_knots))\nregions = num_knots\nnum_spline = 4\n\ncherry_blossoms_2 = cherry_blossoms_2 |&gt; mutate(region = cut(year,breaks = knot_list,right=FALSE,labels=FALSE)) |&gt; mutate(\n  region = if_else(is.na(region),num_knots,region))\n\ncherry_blossoms_2 = cherry_blossoms_2 |&gt; mutate(\n  adj_year = 2*(year - min(year))/(max(year)-min(year)) - 1)\n\n\nnormalizing the year variable is important for the condition number of resulting Gram matrix and constraints!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-a",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-a",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Splines: A",
    "text": "Implementing the B-Splines: A\n\nA = matrix(0,nrow = nrow(cherry_blossoms_2),ncol = num_spline*(regions))\n\nfor (n in 1:nrow(cherry_blossoms_2)){\n  region = cherry_blossoms_2$region[n]\n  year = cherry_blossoms_2$year[n]\n  adj_year = cherry_blossoms_2$adj_year[n]\n  A[n,(4*(region-1) + 1):(4*(region))] = c(1,adj_year,adj_year^2,adj_year^3) \n}\n\n\nFor each data point, we find the corresponding region\nFor each region for values of theta\nCalculate basis functions"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-spline-c",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-spline-c",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Spline: C",
    "text": "Implementing the B-Spline: C\n\nC = matrix(0,nrow = 2*(regions-1),ncol = 4*regions)\n\nmin_year = min(cherry_blossoms_2$year)\nmax_year = max(cherry_blossoms_2$year)\n\nadj_knot_list = 2*(knot_list-min_year)/(max_year-min_year) - 1\n\nfor (i in 1:(regions-1)){\n  C[2*i-1,(4*(i-1)+1):(4*i+4) ] = c(1,adj_knot_list[i+1],\n      adj_knot_list[i+1]^2, adj_knot_list[i+1]^3,\n  -1, -adj_knot_list[i+1], -adj_knot_list[i+1]^2,\n  -adj_knot_list[i+1]^3)\n\n    C[2*i,(4*(i-1)+1):(4*i+4)] = c(0,1,2*adj_knot_list[i+1],\n    3*adj_knot_list[i+1]^2, 0, -1, -2*adj_knot_list[i+1],\n    -3*adj_knot_list[i+1]^2)\n}"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#forming-the-kkt-matrix-and-rhs",
    "href": "meetups/Meetup-4/meetup-4.html#forming-the-kkt-matrix-and-rhs",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Forming the KKT Matrix and RHS",
    "text": "Forming the KKT Matrix and RHS\n\nn_thetas = ncol(A)\np_constraints = nrow(C)\nKKT = matrix(0,nrow = n_thetas+p_constraints ,n_thetas+p_constraints)\n\nKKT[1:n_thetas,1:n_thetas] = 2*t(A) %*% A\nKKT[1:n_thetas,(n_thetas+1):(n_thetas+p_constraints)] = t(C)\nKKT[(n_thetas+1):(n_thetas+p_constraints),1:n_thetas] = C\n\nrhs_ext = matrix(0,nrow = n_thetas+p_constraints,ncol = 1)\n\ndoy = cherry_blossoms_2$doy\n\nrhs_ext[1:n_thetas,1] = 2*t(A) %*% doy"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#solving-the-problem",
    "href": "meetups/Meetup-4/meetup-4.html#solving-the-problem",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\ntheta_ext = solve(KKT,rhs_ext)\ntheta_sol = theta_ext[1:n_thetas]\nz_sol = theta_ext[(n_thetas+1):(n_thetas+p_constraints)]\n\ndoy_pred = A %*% theta_sol\n\ncherry_blossoms_2 |&gt; mutate(doy_pred = doy_pred) |&gt; \n  ggplot(aes(x=year,y=doy)) + geom_point(color=\"red\",alpha=0.2) +\n  geom_vline(data = knot_df,mapping = aes(xintercept=knots)) +\n  geom_line(mapping = aes(x= year, y = doy_pred), color = \"blue\") +\n    labs(title=\"B-Spline Fit\") +\n    ylab(\"Day of Year\") +\n  theme_clean(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#solving-the-problem-1",
    "href": "meetups/Meetup-4/meetup-4.html#solving-the-problem-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\n\nJumps are gone!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-portfolio-optimization",
    "href": "meetups/Meetup-4/meetup-4.html#example-portfolio-optimization",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example: Portfolio Optimization",
    "text": "Example: Portfolio Optimization\n\nYou have a budget of \\(d\\) dollars to invest in financial assets\nPortfolio allocation weights \\(\\mathbf{w}\\) describe how much you invest in each asset\nYou will hold the portfolio over some well defined period"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-terminology",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-terminology",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Terminology",
    "text": "Portfolio Terminology\n\nShort positions, \\(w_i &lt; 0\\) for some \\(i\\)\nLeverage = \\(\\sum_{i=1}^n |w_i|\\)\n\nMeasures how much money you have borrowed\nHave to pay interest in real world\n\nRisk Free Asset\n\nUsually one asset is a cash or treasury equivalent with low to no risk and return"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-returns",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-returns",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Returns",
    "text": "Portfolio Returns\n\nOn day \\(t\\), value of asset \\(i\\) changes by \\(R_{it}\\)\n\\(R_{it} = 1.05\\) is a \\(5\\%\\) gain, \\(0.91\\) a \\(9\\%\\) loss, \\(0\\) a 100% loss, etc\nAfter an investment period of \\(T\\) days, overall portfolio value: \\[\nd_T = d\\sum_{i=1}^n\\Pi_{t=1}^T R_{it}w_i\n\\]\nNote that returns are multiplicative over time\nOften annualize return: \\(\\rho_{A} = \\rho_T^{T_y/T}\\), where \\(T_y\\) trading days per year"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-risk",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-risk",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Risk",
    "text": "Portfolio Risk\n\nDay to day variability of portfolio is called risk\nDefine \\(\\bar{\\rho} = \\frac{1}{T}\\sum_{t=1}^T \\sum_{i=1}^n R_{it}w_t\\)\nCompute variance \\[\n\\mathrm{var}\\left(\\rho\\right) = \\frac{1}{T}\\sum_{t=1}^T\\left(\\sum_{i=1}^n R_{it}w_i - \\bar\\rho \\right)^2\n\\]\nAnnualized risk defined as \\(\\sqrt{T_{y}}\\sqrt{\\mathrm{var(\\rho)}}\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#covariance-formulation",
    "href": "meetups/Meetup-4/meetup-4.html#covariance-formulation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Covariance Formulation",
    "text": "Covariance Formulation\n\nCan also measure risk based on covariance matrix between daily returns of each asset: \\[\n\\Gamma_{ij} = \\frac{1}{T}\\sum_{i=1}^T \\left(R_{it} -\\bar{\\rho}_i\\right)\\left(R_{jt} - \\bar{\\rho}_j\\right)\n\\]\nThen: \\[\n\\mathrm{var}(\\rho) = \\mathbf{w}^T\\Gamma\\mathbf{w}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#markowitz-portfolio-optimization",
    "href": "meetups/Meetup-4/meetup-4.html#markowitz-portfolio-optimization",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Markowitz Portfolio Optimization",
    "text": "Markowitz Portfolio Optimization\n\nGoal: Use past data to estimate future returns \\(\\rho_i\\) and correlations \\(\\Gamma\\)\nCalculate portfolio weights \\(\\mathbf{w}\\) to minimize risk while achieving a target return\n\n\\[\n\\mathrm{find:}\\quad \\min_{\\mathbf{w}} \\mathbf{w}^T\\Gamma\\mathbf{w} \\\\\n\\mathrm{subject\\, to:}\\quad \\sum_{i=1}^n w_i = 1 \\\\\n\\sum_{i=1}^n \\rho_i w_i = \\rho_{g}\n\\]\n\nThis is a least squares (actually least norm) problem with \\(\\Gamma\\) taking role of Gram matrix \\(A^TA\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nWhich asset would you pick?\n\n\n\nAndrew Lo AM"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-1",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nMedium reward “no risk” asset was Bernie Madoff\n\n\n\nAndrew Lo AM"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-2",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nMinimizing risk at a high return target leads to high leverage\nRisk to go bust out of sample"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#thanks",
    "href": "meetups/Meetup-4/meetup-4.html#thanks",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-2/Untitled.html",
    "href": "meetups/Meetup-2/Untitled.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb = np.random.randn(10)\n\n\na @ b\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nb.T @ a.T\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.zeros((10,10))\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\nnp.linalg.norm(np.linalg.inv(a) @ a - np.eye(10))\n\n2.570038214105557e-15\n\n\n\nA = a\n\n\\[ Ax = b \\]\n\nx = np.linalg.lstsq(A,b)[0]\n\n/tmp/ipykernel_2637623/3825200175.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  x = np.linalg.lstsq(A,b)[0]\n\n\n\nA @ x - b\n\narray([-2.22044605e-15,  3.35842465e-15,  5.55111512e-15, -5.38458167e-15,\n        1.77635684e-15,  2.99760217e-15,  2.66453526e-15, -2.38697950e-15,\n        6.21724894e-15,  4.66293670e-15])\n\n\n\\[\nA = U \\Sigma V^T\n\\]\n\\(A\\) is \\(m\\times n\\), \\(U\\) is \\(m\\times m\\) \\(\\Sigma\\) \\(m \\times n\\) \\(V\\) is \\(n \\times n\\)\n\\(U\\) and \\(V\\) are orthonormal matrices, which means:\n\\(U U^T = I\\), \\(V V^T = I\\)\n\\(\\Sigma\\) is 0 except for the diagonals which are the singular values of \\(A\\)\n\nA = np.random.randn(100,10)\n\n\nsvd_A = np.linalg.svd(A)\n\n\nsvd_A[1]\n\narray([13.1081352 , 11.81959892, 11.15343028, 10.83354228, 10.23738251,\n        9.27330655,  9.07488313,  8.12095293,  7.844576  ,  6.77092389])\n\n\n\nU = svd_A[0]\nV = svd_A[2].T\n\n\nnp.linalg.norm(U @ U.T -  np.eye(100))\n\n4.986008699316736e-15\n\n\n\nnp.linalg.norm(V @ V.T -  np.eye(10))\n\n2.4656943504538092e-15\n\n\n\\[\nA^+ = (A^T A)^{-1} A^T\n\\]\n\\[A = U \\Sigma V^T\\]\n\\[\nA^+ = (V \\Sigma^T U^T U \\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} V^T V \\Sigma^T U^T\n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} \\Sigma^T U^T\n\\]\n\\[\nA^+ = V\\Sigma^+ U^T\n\\]\n\\[\nx = V\\Sigma^+ U^T b\n\\]\n\\[\n\\Sigma^+ U^T b = V^T x\n\\]\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = V^T x\n\\]\n\\(y= V^Tx\\)\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = y\n\\]\nx = V @ np.linalg.solve(S.T @ S, U.T @ b)\n\n?np.random.randn\n\n\nDocstring:\nrandn(d0, d1, ..., dn)\nReturn a sample (or samples) from the \"standard normal\" distribution.\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n.. note::\n    New code should use the\n    `~numpy.random.Generator.standard_normal`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\nNotes\n-----\nFor random samples from the normal distribution with mean ``mu`` and\nstandard deviation ``sigma``, use::\n    sigma * np.random.randn(...) + mu\nExamples\n--------\n&gt;&gt;&gt; np.random.randn()\n2.1923875335537315  # random\nTwo-by-four array of samples from the normal distribution with\nmean 3 and standard deviation 2.5:\n&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\nType:      builtin_function_or_method\n\n\n\n\nimport matplotlib.pyplot as plt\nx1 = np.linspace(0,1,1000)\ny1 = 3*x1 + np.random.randn(1000)*(x1+0.1) \ny2 = 3*x1 + np.random.randn(1000)*0.5\n\nplt.subplot(1,2,1)\nplt.scatter(x1,y2,s=0.1)\nplt.title(\"Homoscedastic\")\nplt.subplot(1,2,2)\nplt.scatter(x1,y1,s=0.1)\nplt.title(\"Heteroscedastic\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\ncentral_park\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 central_park\n\nNameError: name 'central_park' is not defined\n\n\n\n\nimport sklearn\n\n\nsklearn.datasets\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 sklearn.datasets\n\nAttributeError: module 'sklearn' has no attribute 'datasets'\n\n\n\n\nimport scipy\n\n\nscipy.s"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#week-summary",
    "href": "meetups/Meetup-2/meetup-2.html#week-summary",
    "title": "Meetup 2: Least Squares",
    "section": "Week Summary",
    "text": "Week Summary\n\nStarting on Least Squares\nChapter 12 of VMLS\nKeep working on Lab 1, due this Sunday at midnight"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "href": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "title": "Meetup 2: Least Squares",
    "section": "System of Linear Equations",
    "text": "System of Linear Equations\n\nWhen can we solve: \\[ A\\mathbf{x} = \\mathbf{b}\\,?\\]\n\\(A\\) is an \\(m\\times n\\) matrix\n\\(\\mathbf{x}\\) is an \\(n\\)-vector\n\\(\\mathbf{b}\\) is an \\(m\\)-vector"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m=n\\) square\nExact solution if rows/columns are independent \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     &    \\cdots   & \\vdots \\\\\n                    \\vdots & \\cdots & \\cdots & \\vdots \\\\\n                    a_{n1} & \\cdots & \\cdots & a_{nn}\n                    \\end{pmatrix}\n                    \\]\n\\(\\mathbf{x} = A^{-1}\\mathbf{b}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&lt;n\\) wide\nGenerally multiple solutions because dependent columns \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     & \\cdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & \\cdots & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]\nCommon ML situation (more parameters than data points)\nParticular solution plus the null space"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&gt;n\\) tall\nGenerally no exact solution \\[ A = \\begin{pmatrix} a_{11} & \\cdots &  a_{1n} \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Inspiration for Least Squares",
    "text": "Inspiration for Least Squares\n\nCan’t solve? Minimize the error \\[ \\mathbf{r} = A\\mathbf{x}-\\mathbf{b}\\]\nOptimization problem is: \\[\\textrm{Find:}\\quad \\min_{\\mathbf{x}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\\]\nThe \\(\\mathbf{x}\\) that minimizes this objective (the squared residual) is the least squares solution\nLinear regression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that:"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that\n\nmatrixcalculus.org\nTake derivatives, work with indices"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nGet System of Linear Equations: \\[ A^TA\\mathbf{x} = A^T\\mathbf{b} \\]\n\\(A^TA\\) is an \\(n\\times n\\) symmetric matrix called the Gram matrix\nInvertible if columns of \\(A\\) are independent:\n\n\\[ \\mathbf{x} = \\left(A^T A\\right)^{-1}A^T\\mathbf{b}\\]\n\nThis is the exact formula for the least squares solution"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#properties",
    "href": "meetups/Meetup-2/meetup-2.html#properties",
    "title": "Meetup 2: Least Squares",
    "section": "Properties",
    "text": "Properties\n\nMorse-Penrose Pseudoinverse: \\[A^+ = (A^TA)^{-1}A^T\\]\n\\(n\\times m\\) matrix\nIf \\(A\\) is invertible, \\(A^+ = A^{-1}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Linear Regression",
    "text": "Applications: Linear Regression\n\nSuppose you have \\(m\\) observations, each with \\(n\\) features\n\\(\\mathbf{a}_{i}\\) is \\(i\\)th obs\nWant to predict the variable \\(\\mathbf{b}\\) using an Affine function: \\[ \\mathbf{a}_i^T\\mathbf{x} +c = b_i \\]\nor\n\\[\n  \\sum_{j=1}^m a_{ij} x_j + c = b_i\n  \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAugmented matrix \\(A\\) and coefficients \\(\\mathbf{x}\\): \\[\nA = \\begin{pmatrix} a_{11} & \\cdots & a_{1n} & 1 \\\\\n                  \\vdots & \\cdots & \\vdots & 1 \\\\\n                  a_{m1} & \\cdots & a_{mn} & 1\n                  \\end{pmatrix}\n\\] \\[\n\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n & c \\end{pmatrix}^T\n\\]\nNow get standard least squares problem:\n\n\\[ \\min_{\\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n\n\n\n\n\n\nGroup\n\n\n\n\n1\n18-25, play sports\n\n\n2\n26-30, play sports\n\n\n3\n31-35, play sports\n\n\n4\n36-40, play sports\n\n\n5\n41-45, play sports\n\n\n6\n18-25, no sports\n\n\n7\n26-30, no sports\n\n\n8\n31-35, no sports\n\n\n9\n36-40, no sports\n\n\n10\n41-45, no sports"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n4 different marketing channels\n\n\n\n\n\n\nChannel\n\n\n\n\n1\nInstagram\n\n\n2\nWine Mag\n\n\n3\nFacebook\n\n\n4\nUFC"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)\n\n\n\n   Instagram  Wine Mag  Facebook   UFC      Demo\n0        3.0       0.1       0.4  6.00   18-25 S\n1        2.5       0.3       0.6  6.00   26-30 S\n2        2.3       0.4       1.0  5.50   31-35 S\n3        1.8       0.6       1.3  5.00   36-40 S\n4        1.0       0.6       2.0  4.50   41-45 S\n5        3.3       0.8       0.5  0.20  18-25 NS\n6        2.6       1.1       0.8  0.12  26-30 NS\n7        2.2       1.3       1.1  0.10  31-35 NS\n8        1.4       2.3       1.3  0.40  36-40 NS\n9        1.2       4.2       1.7  0.20  41-45 NS"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nProblem: spend money to get \\(\\mathbf{v}_{dem}\\) impressions\nDecision variable \\(\\mathbf{s}\\) is spending in each channel\nSolve least squares problem: \\[\n\\min_{\\mathbf{s}} \\|R\\mathbf{s} - \\mathbf{v}_{dem}\\|^2\n\\]\n\\(R\\mathbf{s}\\) is views"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform\n\n\nv_dem = np.ones(10)\n\ns = np.linalg.lstsq(R,v_dem)[0]\n\ns_dat = pd.DataFrame(s, columns = [\"Spending\"])\ns_dat[\"Channel\"] = ['Instagram','Wine Mag','Facebook','UFC'] \n\n(\n  ggplot(s_dat,aes(x=\"Channel\",y=\"Spending\"))\n  + geom_point(size=5) \n  + theme_bw(base_size=14)\n)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget Issues",
    "text": "Marketing Budget Issues\n\nNeed Constraints\n\nNo negative money in adds: \\(\\mathbf{s} \\geq 0\\)\nNo infinite budget: \\(\\sum_{i}s_i \\leq s_{max}\\)\n\nNeed a better objective function\n\nWeird to target precise \\(\\mathbf{v}_{dem}\\)\nBetter to have different value for each ad impression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\n\n\n\n\nUrban Transit\nInternet\nKnow when people enter and leave\nKnow path they took\nHow long does it take to cross each link?"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nFor trip \\(j\\) vector \\(\\mathbf{r}_j\\)\n\n\\(r_{ij} = 1\\) if \\(i\\)th “link” traversed on \\(j\\)th trip\n\\(r_{ij} = 0\\) otherwise\nMatrix \\(R\\) contains \\(r_{ij}\\)\n\nFor \\(t_j\\) is the time taken on trip \\(j\\)\nDecision variables \\(d_i\\) is the time taken on the \\(i\\)th link\nTime is \\(\\sum r_{ij}d_i\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nMinimize Least Squares Error:\n\n\\[\n\\textrm{Find:}\\quad \\min_{\\mathbf{d}} \\|R\\mathbf{d} - \\mathbf{t}\\|^2\n\\]\n\nMany similar problems in optimization theory\nThis one is the simplest"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Numerical Issues With Least Squares",
    "text": "Numerical Issues With Least Squares\n\nSuppose you need to solve a system of linear equations: \\[\nA\\mathbf{x} = \\mathbf{b}\n\\]\nWhich of these is better?\n\nnp.linalg.inv(A) @ b\nnp.linalg.solve(A, b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "href": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "title": "Meetup 2: Least Squares",
    "section": "Matrix Inverses",
    "text": "Matrix Inverses\n\nConsider Singular Value Decomposition of \\(A\\): \\[\nA = U\\Sigma V^T\n\\]\n\\(U\\) and \\(V\\) are orthonormal matrices\n\\(\\Sigma\\) contains the singular values \\[\n\\Sigma = \\begin{bmatrix}\n  \\sigma_{1} & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "href": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "title": "Meetup 2: Least Squares",
    "section": "Condition Number and Accuracy",
    "text": "Condition Number and Accuracy\n\nAccuracy of np.linalg.inv(A) @ b related to \\(\\frac{\\sigma_{max}}{\\sigma_{min}}\\)\nThis is called the “condition number” of the matrix \\(A\\)\n\n\n%timeit np.linalg.inv(A) @ b\n%timeit np.linalg.solve(A,b)\n\nx1 = np.linalg.inv(A) @ b\nx2 = np.linalg.solve(A,b)\n\nprint(\"Matrix Inverse Error: \",np.linalg.norm(A @ x1 - b))\nprint(\"Linear Solve Error: \",np.linalg.norm(A @ x2 - b))\n\n3.99 ms ± 676 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n1.01 ms ± 252 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nMatrix Inverse Error:  3.0824002798186794e-07\nLinear Solve Error:  2.3634282010204213e-15\n\n\n\n100 million times more accurate, 5 times faster"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "href": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "title": "Meetup 2: Least Squares",
    "section": "Issue is worse for \\(A^TA\\)",
    "text": "Issue is worse for \\(A^TA\\)\n\nWhat about \\((A^TA)^{-1}\\)?\nLet’s look at SVD: \\[ A^TA = V \\Sigma^T\\Sigma U^T\\] \\[ \\Sigma^T\\Sigma = \\begin{bmatrix}\n  \\sigma_{1}^2 & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}^2\n\\end{bmatrix}\n\\]\nCondition number problems 💀 💀 💀 💀"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "href": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "title": "Meetup 2: Least Squares",
    "section": "Do Ill-Conditioned Matrices Occur?",
    "text": "Do Ill-Conditioned Matrices Occur?\n\nVery Common\nCollinear predictors in linear regression\nMain issue in Deep Learning\nNoisy Inverse Problems\nSome of the solutions are basically just regularization"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#alternatives",
    "href": "meetups/Meetup-2/meetup-2.html#alternatives",
    "title": "Meetup 2: Least Squares",
    "section": "Alternatives",
    "text": "Alternatives\n\nSoftware packages generally have routines to solve least squares without computing ill-conditioned matrix inverse\nGood example is \\(A = QR\\) factorization\nHere \\(Q\\) is orthogonal and \\(R\\) is upper triangular\nThen \\[\nA^+ = (A^TA)^{-1}A^T = R^{-1} Q\n\\]\nIn practice np.linalg.linsolve(R,Q @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "href": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "title": "Meetup 2: Least Squares",
    "section": "What if \\(A\\) is really big?",
    "text": "What if \\(A\\) is really big?\n\nConstructing \\(A^TA\\) costs \\(mn^2\\) operations\nSolving \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) costs \\(n^3\\) operations\n\\(A^TA\\) could be a TB or larger…..\nSolution 1- variety of iterative methods (HW)\nSolution 2- Randomized Algorithms (Random Kaczmarz)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#thanks",
    "href": "meetups/Meetup-2/meetup-2.html#thanks",
    "title": "Meetup 2: Least Squares",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is Optimization?",
    "text": "What is Optimization?\n\nWhat is the decision that leads to the best outcome?\nWhat is the value of our decision variable \\(\\mathbf{x}\\) that minimizes our objective function f()?\n\n\\[\n\\textrm{minimize}\\quad f(\\mathbf{x}), \\\\\n\\textrm{subject to}\\quad g_i(\\mathbf{x}) \\leq 0, \\\\\n\\textrm{and}\\quad h_j(\\mathbf{x}) = 0,\\\\\n\\mathbf{x}\\in \\mathbb{R}^n, \\quad f,\\, g_i,\\, h_j: \\mathbb{R}^n\\mapsto \\mathbb{R}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Constraint Terminology",
    "text": "Constraint Terminology\n\nA feasible point \\(\\mathbf{x}\\) for the optimization problem is a point that satisfies all the constraints: \\[\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]\nTechnically we don’t need the \\(h_j\\) terms\nCan get greater than constraints with \\(-g_i\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimum Terminology",
    "text": "Optimum Terminology\n\n\\(\\mathbf{x}\\) is called a local minimum if there is some region \\(\\Gamma\\) surrounding \\(\\mathbf{x}\\) where \\(f(\\mathbf{x})\\leq f(\\mathbf{x'})\\) for all \\(\\mathbf{x}'\\) in \\(\\Gamma\\).\n\\(\\mathbf{x}\\) is a global minimum if there is no feasible point \\(x'\\) with \\(f(\\mathbf{x'})&lt;f(\\mathbf{x})\\)\nCan turn a maximization problem into a minimization problem by replacing the objective function \\(f\\) with \\(-f\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(\\mathbf{x}\\) represents something we control:\n\nCoefficients of a linear regression\nWeights of a neural network\nAllocation of $ to advertising channels\nInvestment allocations in a portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(f\\) models how the decision \\(\\mathbf{x}\\) impacts us\n\nRMS error of the regression\ncross-entropy plus a penalty for the weights\nAd impressions on target demographic\nExpected variance of your portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(g_i\\) and \\(h_j\\) constrain your decisions \\(\\mathbf{x}\\) so they are realistic\n\nCan’t put negative $ in an ad channel\nRegulatory limits on leverage, short positions\nValid probability distribution\nPrior constraints on parameters"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization underlies nearly all data science algorithms",
    "text": "Optimization underlies nearly all data science algorithms\nGoing in depth brings some benefits:\n\nRecognize what problem you are facing, can enable you to solve it 1000 times easier in some cases\nUnderstand how the tools you use work and what can go wrong\nAbsolutely indispensable for some cutting edge methods (i.e. deep learning)\nUseful non-stats applications"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\n\n\\(i\\)th observation is vector \\(\\mathbf{a}_i\\)\nObs of target function \\(b_i\\)\nLinear model \\(b \\sim a^t x\\)\n\n\\[\n\\textrm{minimize RMS error:}\\quad \\min_{x\\in\\mathbb{R}^n} \\sum_{i} \\left(\\mathbf{a}_i^t \\mathbf{x}-b_i\\right)^2 \\] or let \\(A\\) be the matrix whose rows are the vectors \\(\\mathbf{a}_i^t\\): \\[\\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 \\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\nAdd penalty terms for regularization or constraints\n\\[ \\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 +k\\|x\\|^2 \\\\\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "href": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Maximum Likelihood",
    "text": "Examples: Maximum Likelihood\n\n\\(p(\\mathbf{y}|\\mathbf{x})\\) is likelihood of data given parameters\nmaximum likelihood is the optimization problem\n\n\\[\n\\textrm{find:}\\quad \\min_{\\mathbf{x}\\in\\mathbb{R}^n} -\\log\\left(p(\\mathbf{y}|\\mathbf{x})\\right) \\\\\n\\textrm{subject to:}\\quad g_i(\\mathbf{x}) \\leq 0 \\\\\n\\textrm{and:}\\quad h_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "href": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Example: Deep Learning",
    "text": "Example: Deep Learning\n\nTraining examples \\(\\mathbf{x}_i\\) and labels \\(y_i\\)\nNeural network defined by a function \\(\\phi\\) that depends on weights \\(\\mathbf{w}\\)\nLearning problem:\n\n\\[ \\min_w \\sum_i C(\\phi(\\mathbf{x}_i,\\mathbf{w}),y_i), \\]\n\n\\(C\\) is a cost function"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization outside data sciece",
    "text": "Optimization outside data sciece\n\nOrgs and Companies face constant optimization problems\nLearning how to handle them can be a superpower"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLeast Squares\n\n\nOldest, easiest, most mature technologies, first four weeks\nPrimary book is VMLS"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nConvex Optimization\n\n\nNon-negative curvature\nIncredibly useful, difficult, weeks 5-12\nSubsumes many special cases (least squares, linear programs, etc)\nGrowing field, especially with advent of AI"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nNonconvex Optimization\n\n\nCan’t solve these but can come close\nUseful for things like deep learning\nLast 3 weeks\nMore based on lectures but also https://www.deeplearningbook.org/"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "href": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Other Textbook",
    "text": "Other Textbook\n\n\n\nWe have one more good book, unforunately not free\nAlso doesn’t cover anything in depth\nGreat for overall picture"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "href": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nDue every two weeks\nMostly solving problems with data but some math\nPreferred format: quarto file accompanied with rendered pdf\nChoice of language, Recommended python, Julia, R\n80% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#final-project",
    "href": "meetups/Meetup-1/meetup-1.html#final-project",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Final Project",
    "text": "Final Project\n\nFinal Project will be to explore an aspect of the course in a little more depth.\nIntentionally open ended, but could be as simple as picking an application area of interest to you and designing and solving a problem in that area\n20% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "href": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Website and Slack Channel",
    "text": "Course Website and Slack Channel\n\nKey annoucements, homework, readings, etc will be posted to: https://georgehagstrom.github.io/DATA609Spring2025/\nCourse will have a slack channel to enable rapid communication: invite link\nTurn in assignments on Brightspace"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "href": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\n\nAI is useful, you can ask it questions, have it generate code, etc\nIt is great at easy problems, but that is a trap\nIt makes lots of mistakes\nResearch shows it helps people who know the material well much more than those who don’t\nPolicy: You should understand everything that you turn in completely. I’ll will be generous with grades for genuine effort but will penalize AI-driven mistakes harshly"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "href": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf needed review linear algebra:\n\nEssential Linear Algebra for Machine Learning -Appendix A1, A3-A5 of Convex Optimization\n\nStart Lab 1\n\nProblem 1 this week, 2 and 3 next week"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization: Needles in a haystack?",
    "text": "Optimization: Needles in a haystack?\n\nGeneral optimization is a hard problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError proportional to # boxes sampled: \\[\\mathrm{Num Boxes} \\sim \\frac{C}{\\epsilon^N}\\]\nN is num. parameters\nModest N, problem takes longer than age of universe"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "href": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Only Local Optimization is Possible",
    "text": "Only Local Optimization is Possible\n\n\n\nCan’t find lowest valley, but can find a valley\nOften, this is still very useful in practice\nNo guarantees, sensitive to initial guess\nFor convex problems, only one valley!"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nIn calculus need two things: \\[\n\\frac{df}{dx} = 0,\n\\frac{d^2f}{dx^2} &gt; 0\n\\]\nFunction is “flat”\nCurvature is “up”"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nHigher dimensions \\[\n\\nabla f(\\mathbf{x}) = 0 \\\\\n(\\nabla^2 f)_{ij} = \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j} \\succ 0\n\\]\nVanishing gradient\nPositive Definite “Hessian”"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\nIterative methods:\n\nGradient Descent, Newton’s Method, ….\n\nStart with initial guess \\(\\mathbf{x}_0\\)\nCalculate gradient \\(\\nabla f\\)\nTake a “step” in direction of \\(\\nabla f\\) \\[ \\mathbf{x}_{n+1} = \\mathbf{x}_n - k\\nabla f(\\mathbf{x}_{n})\\]\n\\(k\\) is step size or learning rate"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f(x,y) = \\exp\\left(-x^2-3y^2\\right)\\)\n\\(k=0.1\\)\n100 steps"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#advert",
    "href": "meetups/Meetup-1/meetup-1.html#advert",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Advert",
    "text": "Advert\n\nOnce per month (Tuesday, Wedneday, Thursday) “field trips” to New York Open Statistical Computing Meetup\nSend me a DM I’ll add you to the email list\nnyhackr.org"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#thanks",
    "href": "meetups/Meetup-1/meetup-1.html#thanks",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "assignments/labs/tbd.html",
    "href": "assignments/labs/tbd.html",
    "title": "Lab TBD",
    "section": "",
    "text": "Check back later\nThis assignment is not available to view yet. Please check back closer to the assignment due date/week of the module this assignment is based on."
  },
  {
    "objectID": "assignments/labs/Lab5.html",
    "href": "assignments/labs/Lab5.html",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab5.html#instructions",
    "href": "assignments/labs/Lab5.html#instructions",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-1",
    "href": "assignments/labs/Lab5.html#problem-1",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 4.2 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-2",
    "href": "assignments/labs/Lab5.html#problem-2",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 2:",
    "text": "Problem 2:\n4.2 ‘Hello World’ in CVX. Use CVX to verify the optimal values you obtained (analytically) for exercise 4.2 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-3",
    "href": "assignments/labs/Lab5.html#problem-3",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 4.60 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-4",
    "href": "assignments/labs/Lab5.html#problem-4",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 6.4 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab5.html#problem-5",
    "href": "assignments/labs/Lab5.html#problem-5",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 6.13 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html",
    "href": "assignments/labs/Lab6.html",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab6.html#instructions",
    "href": "assignments/labs/Lab6.html#instructions",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-1",
    "href": "assignments/labs/Lab6.html#problem-1",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 1:",
    "text": "Problem 1:\nExercise 7.4 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-2",
    "href": "assignments/labs/Lab6.html#problem-2",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 2:",
    "text": "Problem 2:\nExercise 7.6 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-3",
    "href": "assignments/labs/Lab6.html#problem-3",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 3:",
    "text": "Problem 3:\nExercise 7.14 in Convex Optimization"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-4",
    "href": "assignments/labs/Lab6.html#problem-4",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 4:",
    "text": "Problem 4:\nExercise 7.48 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs/Lab6.html#problem-5",
    "href": "assignments/labs/Lab6.html#problem-5",
    "title": "Homework 5: Disciplined Convex Programming and Data Fitting",
    "section": "Problem 5:",
    "text": "Problem 5:\nExercise 8.6 in Convex Optimization Extended Exercises"
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "The homework assignments in this course are lab assignments where you will solve the problem using a combination of mathematics and computer code.\nPlease submit a and either a quarto file or jupyter notebook or other code that generates your homework. Labs should be submitted on Blackboard.\n\n\nGradient Descent and Least Squares (Download)\n\n\nLeast Squares Applications (Download)\n\n\nConvex Sets (PDF) (Quarto Download)\n\n\nConvex Sets (PDF) (Quarto Download)\n\n\nConvex Optimizations Problems (Download)\n\n\nApplications to Statistics and Machine Learning (Download)\n\n\nNonconvex Optimization and Stochastic Gradient Descent (Download)\n\n\nTraining Deep Neural Networks (Download)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 6: Introduction to Convex Sets\n\n\n\n\n\nClick here to read more about Meetup 6 and view/download the lecture slides.\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6- Constructing Convex Sets\n\n\n\n\n\nClick here to read about Week 6.\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5-Convex Sets\n\n\n\n\n\nClick here to read about Week 5.\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 5: Introduction to Convex Sets\n\n\n\n\n\nClick here to read more about Meetup 5 and view/download the lecture slides.\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 4: Least Squares Data Fitting\n\n\n\n\n\nClick here to read more about Meetup 4 and view/download the lecture slides.\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4- Constrained Least Squares\n\n\n\n\n\nClick here to read about Week 3.\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Linear Algebra in R\n\n\n\n\n\nClick here for a video on basic linear algebra in R.\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 3: Least Squares Data Fitting\n\n\n\n\n\nClick here to read more about Meetup 3 and view/download the lecture slides.\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3- Data Fitting\n\n\n\n\n\nClick here to read about Week 3.\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Writing Math in Markdown\n\n\n\n\n\nClick here for a 20 minute video on writing math in markdown using Latex.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Linear Algebra in Numpy\n\n\n\n\n\nClick here for a video on basic linear algebra in Numpy.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 2: Least Squares Optimization\n\n\n\n\n\nClick here to read more about Meetup 2 and view/download the lecture slides.\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2- Least Squares\n\n\n\n\n\nClick here to read about Week 2.\n\n\n\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 1: Introduction to Optimization\n\n\n\n\n\nClick here to read more about Meetup 1 and view/download the lecture slides.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA 609\n\n\n\n\n\nImportant information on how to get started with this course. Click this post now for instructions on getting started.\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Project",
    "section": "",
    "text": "The idea of this project is for you to find data and do something interesting with it that relates to the skills you have developed in this course.\nThe project is intentionally open ended- this is an opportunity for you to explore your interests, through using techniques or technologies that branch off those that were introduced in this class, datasets related to problems you face in your career or that you are curious about, or both.\nMy expectation for your project is that it has the length and sophistication of a complete homework assignment, but will all the work focused on the topic. It should include an introduction to contextualize the topic, but otherwise you can structure it how you like.\nThe output of your project should software that allows me to reproduce your work and a research report which describes what you did with the data and your findings, which could be contained within a github repository.\n\n\nThe first step of your project is to submit an initial proposal, which will help to ensure that your project is both worthwhile and feasible in the time frame of the last half of the semester. Your proposal should describe the area that you are interested in, the datasets you are going to use/mathematical problems you want to solve, and the link to the topics of this course. The proposal length should be around one page (excluding figures and references),\nThe proposal is preliminary, and you can include additional data or a different analysis as needs require.\n\n\n\nWhen writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#heilmeiers-questions",
    "href": "assignments/project.html#heilmeiers-questions",
    "title": "Project",
    "section": "",
    "text": "When writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/labs/Lab2.html",
    "href": "assignments/labs/Lab2.html",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#instructions",
    "href": "assignments/labs/Lab2.html#instructions",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "href": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models",
    "text": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models\nMany applications of least squares (and other statistical methods) involve , in which data is collected over a time period and the statistical model is updated as new data arrives. If the quantity of data arriving is very large, it may be inefficient or even impossible to refit the entire model on the entire dataset. Instead, we use techniques (often referred to as which take the current model as a starting point and update them to incorporate the new data.\nThe structure of least squares problems makes them amenable to online updating (sometimes this is called “recursive” least squares). The structure of the problem is as follows, at time \\(t\\) we receive a vector of observations \\(\\mathbf{x}_t\\) and an observation of our target variable \\(y_t\\).\nThe full set of all observations and target variable data that we have received up to time \\(t\\) is contained in the following matrix and vector:\n\\[\nX_{(t)} = \\begin{bmatrix} \\cdots\\, \\mathbf{x}_1^T\\, \\cdots \\\\\n\\cdots\\, \\mathbf{x}_2^T\\, \\cdots \\\\ \\vdots \\\\ \\cdots\\, \\mathbf{x}_t^T\\, \\cdots \\end{bmatrix},\\quad\n\\mathbf{y}_{(t)} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_t \\end{bmatrix}\n\\]\nwhich says that row \\(j\\) of the matrix \\(X_{(t)}\\) is the \\(j\\)th observation \\(\\mathbf{x}_j^T\\), and the \\(j\\)th entry of \\(\\mathbf{y}_{(t)}\\) is \\(b_j\\).\nHere we assume that the vectors \\(\\mathbf{x}\\) each contain \\(n\\) observations, so that \\(X_{(t)}\\) is a \\(t\\times n\\) matrix and the vector \\(\\mathbf{y}_{(t)}\\) is a \\(t\\)-vector.\nAs long as \\(t&gt;n\\), i.e. the number of time observations is greater than the number of features/data points in each observation, we can fit a linear model predicting the target as a function of the features \\(\\mathbf{x}\\) by solving the following system of equations:\n\\[\n(X^T_{(t)}X_{(t)})\\mathbf{x}_{(t)} = X_{(t)}^T\\mathbf{y}_{(t)}\n\\]\nAs the length of the time series increases, the computational difficulty of solving this problem also increases. However, it is possible to re-use work done on the previous time step to avoid solving the full system at each step.\nThis algorithm is based on the fact that the Gram Matrix \\(X^T_{(t+1)}X_{(t+1)}\\) can be calculated from the Gram matrix \\(X^T_{(t)}X_{(t)}\\) from the previous time step. \\[\nX^T_{(t+1)}X_{(t+1)} = X^T_{(t)}X_{(t)} + \\mathbf{x}_{t+1}\\mathbf{x}_{t+1}^T\n\\] Similarly, the product \\(X^T_{(t+1)} \\mathbf{y}_{(t+1)}\\) can also be updated from the value on the previous time step: \\[\nX^T_{(t+1)}\\mathbf{y}_{(t+1)} = X^T_{(t)}\\mathbf{y}_{(t)} + y_{t+1}\\mathbf{x}_{t+1}\n\\]\nWe can write an efficient algorithm to compute the updated least squares solution as follows:\n\nStep 1: Pick an initial time \\(t\\) such that \\(X_{t}\\) is square or tall so that the least squares problem can be solved (i.e. wait for enough data to have built up before your start) and then calculate the Gram matrix and the product \\(X^T_{t} \\mathbf{y}_t\\) \\[\nG_{(t)} = X^T_{(t)}X_{(t)}, \\quad \\mathbf{h}_{(t)} = X^T_{t}\\mathbf{y}_t\n\\]\nStep 2: Find the least squares solution at time \\(t\\) by solving the linear system: \\[\nG_{(t)}\\mathbf{\\theta}_{(t)} = \\mathbf{h}_{(t+1)}\n\\]\nStep 3: When the next data points \\(\\mathbf{x}_{t+1}\\) and \\(y_{t+1}\\), update \\(G_{(t+1)}\\) and \\(\\mathbf{h}_{(t+1)}\\):\n\n\\[ G_{(t+1)} = G_{(t)} + \\mathbf{x}_{t+1}\\mathbf{x}^T_{t+1},\\quad\n\\mathbf{h}_{(t+1)} = \\mathbf{h}_{(t)} + y_{t+1}\\mathbf{x}_{t+1}\n\\] Then you can repeat Step 2 to find \\(\\mathbf{theta}_{t+1}\\). This algorithm can be improved upon slightly using the Matrix Inversion Lemma/Woodbury Formula, which could be a topic for a project (see note at the end which mentions Kalman filters).\n\nYou are going to use this algorithm to make a linear, autoregressive model that predicts total day-ahead citibike trips from the daily high temperature and the number of daily citibike trips taken each of the past 7 days. The data is contained in the file daily_citibike_trips.csv.\n\nSpecifically, for each time point \\(t&gt;7\\) , fit the following model as a least squares estimation problem: \\[\nN_{trips,\\tau} = \\sum_{i=1}^7 \\theta_i N_{trips,\\tau-i} + \\theta_{i+1} T,\n\\] Here, each \\(N_{trips,\\tau}\\) stands for the number of citibike trips on the \\(t\\)th day of the time series, \\(T\\) stands for the forecast high temperature in New York City that day, and the coefficients \\(\\theta_i\\) are the decision variables.\nFind the coefficients \\(\\theta_{i,t}\\) that minimize the mean square errors on all the observed citibike trips prior to time \\(t\\). Use the recursive least squares optimization outlined in the preamble to this problem to calculate the coefficients for each time point, and plot how they and the \\(R^2\\) of the model change over time.\nWhat patterns do you notice in how the regression coefficients and \\(R^2\\) change over time?\nTip: Be very cautious when coding about the dimensionality of matrices and arrays. In python, x @ x.T will be an 8x8 matrix if a.shape = (8,1). However, by default x.shape = (8,), indicating that a is not being treated as either a row or column vector. For this problem it is important that the vectors are either row or column vectors, and not arrays without such an orientation. In python, you can use numpy.reshape to adjust.\n\nWe have included temperature as a variable because it probably influences the decision to ride a citibike. However, the relationship could be nonlinear, as both extreme high and low temperatures make bike riding less comfortable. With this idea in mind, create a new feature by choosing a nonlinear function of the temperature \\(T\\) that represents the potential for both high and low values of \\(T\\) to the same impact on predicted ridership. Use least squares optimization to fit an autoregressive time series model, replacing \\(T\\) with the value of your new feature \\(f(T)\\) in the time series. How does the temperature dependence coefficient differ between this model and the one you fit in (a)? Does the accuracy of the model improve or get worse using the new feature?"
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "href": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 2: Weighted Least Squares",
    "text": "Problem 2: Weighted Least Squares\nThe file social-mobility.csv contains data on the fraction of individuals born in the years 1980-1982 to parents in the bottom 20% of the income distribution who reach the top 20% of the income distribution by the time they turn 30 in a large number of municipalities throughout the United States. The dataset also contains additional variables that describe other socio-economic differences between the cities in the dataset.\n\nMake a scatter-plot of mobility versus population (use a log-scale for population). What do you notice about the variance of social mobility as a function of population? This is a common feature of nearly every dataset containing geographic regions with widely different populations.\nAssume that the number of children born in families making below the 20th percentile of the income distribution in each city is linearly proportional to the city population. Write down a formula for how the variance of each measurement of the social mobility should depend on the measured social mobility and the population. Hint: start with either the formula for the variance of binomial counts or look up the variance of a proportion derived from a binomial distribution. Don’t worry about constant factors when deriving this formula.\nUse weighted least squares to calculate an estimate of how social mobility depends on commute time and student-teacher ratio, using weights calculated based on the variance estimate derived in (b). Compare the coefficients to those derived from ordinary least squares with no weights."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-3-markowitz-portfolio-optimization",
    "href": "assignments/labs/Lab2.html#problem-3-markowitz-portfolio-optimization",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 3: Markowitz Portfolio Optimization",
    "text": "Problem 3: Markowitz Portfolio Optimization\nIn this problem you will use Markowitz Portfolio Optimization to construct a set of portfolios that aim to achieve target expected rates of return while minimizing risk. The file stock_returns.csv contains information on daily asset returns from 2020-2024 for a group of assets, consisting mostly of large-cap stocks but also a handful of exchange traded funds that correspond to US Treasury Bonds and Notes with varying maturities.\nYou will divide the data into two time periods, a training period (2020-2022) and a testing period (2022-2024). The data in the stock_returns.csv is stored in a long format, with the following variables:\n\nCompany - The ticker symbol that identifies the stock\ndate - The date to which the data corresponds\nadjusted- the closing price of the stock, adjusted for special events like dividends and stock splits\nreturn - the ratio of the current adjusted close to the adjusted close on the previous trading day\nlog_return- the natural logarithm of the return\n\n\nConstruct a vector (we call it \\(\\mu\\)) containing the annualized rate of return over the training period (for the \\(i\\)th stock, you can use the formula: \\(\\mu_i = \\exp\\left(0.5\\sum_{t} \\mathrm{log\\_return}_i(t)\\right)\\)), or the square root of the total return over the first two years of data, and daily return covariance \\(\\Gamma\\).\n\nHint: Possible workflow if working in python: convert your data to a wide format using pandas, extract the values into a numpy ndarray, and then use the function np.cov.\nThen solve the following constrained least squares problem to calculate optimal portfolios achieving a fixed rate of return with minimum variance:\n\\[\n\\begin{aligned}\n    \\min_{w} \\mathbf{x}^T\\Gamma \\mathbf{x}, \\\\\n    \\mathbf{w}^T\\mathbf{\\mu} = r, \\\\\n    \\sum_{i} w_i = 1\n\\end{aligned}\n\\] Here \\(\\mathbf{w}\\) is a vector containing the investment allocations into different assets, and \\(r\\) is the target rate of return.\nCalculate optimal portfolios based on the 2020-2022 data for \\(r=1.05\\), \\(r=1.10\\), and \\(r=1.20\\).\n\nPlot the cumulative value of each portfolio over time, assuming that an initial investment is made at the start of the period and that there is no rebalancing of the portfolio, i.e. \\(r_{T} = \\sum_{i=1}^n w_i\\Pi_{t=1}^T r_{it}\\), where \\(r_{it}\\) is the return of asset \\(i\\) on trading day \\(t\\), (hint: np.cumprod allows you to efficiently calculate this quantity). Make the plot for both the training and test sets of returns.\n\nFor each of the 3 portfolios also report:\n\nThe annualized return on the training and test sets;\nThe risk on the training and test sets, defined as the realized variance of the daily return;\nThe asset with the maximum allocation weight, and its weight;\nThe initial leverage, defined as \\(\\sum_{i=1}^n |w_i|\\). This number is always at least one, and it is exactly one only if the portfolio has no short positions.\n\nComment briefly on your observations about the different portfolios and the difference between their training and testing performance.\n\nIt is well known that optimal portfolios constructed using the Markowitz procedure perform much more poorly out of sample compared to in sample. This is due to a variety of reasons, one of which is that the procedure assumes that future returns are equal to past returns, another that the correlation structure of the market might change over time, and finally, when there are many assets there is the potential for overfitting. Repeat the previous problem but introduce a ridge regression/\\(l_2\\) norm penalty term to the objective function, with a hyperparameter \\(\\lambda\\) governing the size of the penalty term.\n\nSpecifically, you will select 10 positive values of \\(\\lambda\\) on a log scale between \\(1e-1\\) and \\(10\\) and for each value of \\(\\lambda\\) solve the following penalized regression problem:\n\\[\n\\begin{aligned}\n    \\min_{w} w^T(\\Gamma+\\lambda I) w ,\\\\\n    w^T\\mu = r,\\\\\n    \\sum_{i=1}^n w_i = 1\n\\end{aligned}\n\\]\nfor just the single value of \\(r=20\\%\\). Then calculate the performance of each of these regularized Markowitz strategies on both the training and test datasets and plot the the return of the portfolios over both the training and testing period.\nFor each portfolio, also report:\n\nThe annualized return on the training and test sets;\nThe risk on the training and test sets;\nThe maximum allocation weight and the asset with maximum allocation;\nThe initial leverage\n\nComment on how the different values of \\(\\lambda\\) changed the optimal portfolios and the difference between in-sample and out-of-sample return and variance."
  },
  {
    "objectID": "assignments/labs/Lab2.html#potential-projects-based-on-this-homework",
    "href": "assignments/labs/Lab2.html#potential-projects-based-on-this-homework",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Potential Projects Based on this Homework:",
    "text": "Potential Projects Based on this Homework:\nRecursive least squares is a gateway to several important techniques that would lead to good projects. The Kalman Filter or Bayes Filter is an algorithm that recursively estimates a statistical model while allowing the underlying coefficients (or state) of that model to undergo dynamical evolution (as a simple example, think of correcting noisy measurements of the GPS location of a drone using Newtownian physics). These are some of the most useful prediction algorithms and can be applied to many areas, including econometrics, web traffic prediction, and vehicle location. For a more math related project, studying the accuracy of the Woodbury formula could be interesting."
  },
  {
    "objectID": "assignments/labs/Lab1.html",
    "href": "assignments/labs/Lab1.html",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#instructions",
    "href": "assignments/labs/Lab1.html#instructions",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "href": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 1: Gradient Descent",
    "text": "Problem 1: Gradient Descent\n\nConsider the mathematical function defined on \\(f: \\mathbb{R}^2\\,\\to\\, \\mathbb{R}\\):\n\n\\[\nf(x,y) = (x-1)^2 + (y+2)^2,\n\\]\nFind the single critical point of this function and show that it is a local minimum (in this case, this will also be a global minimum).\n\nNow consider a new objective function that depends on a parameter \\(b\\): \\[\nf(x,y) = x^2 + by^2\n\\] Here we will look at two different values of \\(b\\), \\(b=3\\) and \\(b=10\\). The global minimum of this function occurs at the point \\(x^* = 0\\), \\(y^*=0\\) no matter what the value of \\(b\\). Suppose that we didn’t know this and wanted to find the minimum of this function using gradient descent instead of direct calculation.\n\n\nFirst write code to perform the gradient descent algorithm, that is perform the iteration: \\[\n\\mathbf{v}_{n+1} = \\mathbf{v}_n - k \\nabla f(\\mathbf{v}_n),\n\\]\n\nwhere the vector \\(\\mathbf{v} = \\begin{bmatrix} x & y\\end{bmatrix}^T\\) and \\(k\\) is the learning rate.\n\nThen test the performance of your algorithm as a function of the learning rates \\(k\\) by performing 100 iterations of the algorithm for 100 values of \\(k\\) equally spaced between \\(k=0.01\\) and \\(k=0.3\\). Start with an initial guess of \\(\\mathbf{v}_0 = \\begin{bmatrix} b & 1\\end{bmatrix}^T\\). Do this for \\(b=3\\) and \\(b=10\\). Make separate plots for \\(b=3\\) and \\(b=10\\) of the log base 10 of the error (in this case it is \\(\\sqrt{x_{100}^2+y_{100}^2}\\)) for the final value of the iteration versus the value of \\(k\\). How does learning rate relate to the final value of the error? For which value of \\(b\\) does the algorithm have the ability to converge fastest (have the lowest value of the error at the end)?\n\nNote: For some combinations of \\(k\\) and \\(b\\), the algorithm won’t converge to the right answer, i.e. the error will grow with time. To make your plot easier to read, don’t plot the error for iterations that didn’t converge.\n\nAs \\(k\\) increases, for one or both values of \\(b\\), you will observe a point where the trend of final error versus learning rate reverses direction. Pick a value of \\(k\\) very close to the point where this occurs, and make a contour plot of the function \\(f\\) and the trajectory of the iterations for the gradient descent algorithm for that value of \\(k\\) superimposed over the contour plot. What do you observe?\n\nNote: The differences that you observe here are a special case of a more general phenomenon: the speed of convergence of gradient descent depends on something called the condition number of the Hessian matrix (the matrix of the 2nd order partial derivatives) of the target function. The condition number for a symmetric matrix is just the ratio of the largest to smallest eigenvalues, in this case the condition number is \\(b\\) (or 1/\\(b\\)). Gradient descent performs worse and worse the larger the condition number (and large condition numbers are problematic for a wide variety of other numerical methods)."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "href": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 2: Solving Least Squares Problems",
    "text": "Problem 2: Solving Least Squares Problems\nGenerate a random \\(20\\times 10\\) matrix \\(A\\) and a random 20-vector \\(b\\) (use a Gaussian distribution). Then, solve the least squares problem: \\[\n\\min_{\\mathbf{x}\\in \\mathbb{R}^{10}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\n\\] in the following ways:\n\nMultiply \\(\\mathbf{b}\\) by the Morse-Penrose Pseudoinverse \\(A^+\\).\nUse built in functions to solve the least squares problem (i.e. in python numpy.lstsq, in R lm, and in Julia the backslash operator).\nUsing the \\(QR\\) factorization of \\(A\\). This factorization rewrites \\(A\\) as: \\[\nA = \\begin{bmatrix} Q & 0\\end{bmatrix} \\begin{bmatrix} R & 0 \\end{bmatrix}^T,\n\\] where \\(Q\\) is an orthonormal matrix and \\(R\\) is upper triangular. The least squares solution equals: \\[\n\\mathbf{x} = R^{-1}Q^T\\mathbf{b}\n\\]\nVerify that each of these solutions are nearly equal and that the residuals \\(A\\mathbf{x}-\\mathbf{b}\\) are orthogonal to the vector \\(A\\mathbf{x}\\)"
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "href": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 3: Iterative Solutions to Least Squares",
    "text": "Problem 3: Iterative Solutions to Least Squares\nAlthough the pseudoinverse provides an exact formula for the least squares solutions, there are some situations in which using the exact solution is computationally difficult, particularly when the matrix \\(A\\) and vector \\(\\mathbf{b}\\) have a large number of entries. In isn’t out of the ordinary for \\(A^TA\\) to be more than a terabyte, for example . In these cases it may be better to use an approximate solution instead of the exact formula. There are many different approximate methods for solving least squares problems, here we will use an iterative method developed by Richardson.\nThis method begins with an initial guess \\(\\mathbf{x}^{(0)} = 0\\) and calculates successive approximations as follows:\n\\[\n    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mu A^T\\left(A\\mathbf{x}^{(k)}-\\mathbf{b}\\right)\n\\]\nHere \\(\\mu\\) is a positive paramter that has a similar interpretation to the learning rate for gradient descent. A choice that guarantees convergence is \\(\\mu \\leq \\frac{1}{\\|A\\|}\\). The iteration is terminated when the change in the residual \\(\\|A^T(Ax^{(k)} − b)\\|\\) after successive steps is below a user determined threshold, which indicates that the least squares optimality conditions are nearly satisfied.\n\nSuppose that \\(\\mathbf{x}\\) is a solution to the least squares problem: \\[\n\\mathbf{x} = A^+\\mathbf{b}\n\\]\n\nShow by substitution of the formula for the pseudoinverse that \\(\\mathbf{x}\\) is a fixed-point of the iteration scheme, i.e. that: \\[\n\\mathbf{x} = \\mathbf{x} - \\mu A^T\\left(A\\mathbf{x}-\\mathbf{b}\\right)\n\\]\n\nGenerate a random 20 × 10 matrix \\(A\\) and 20-vector \\(\\mathbf{b}\\), and compute the least squares solution \\(\\mathbf{x} = A^+\\mathbf{b}\\). Then run the Richardson algorithm with \\(\\mu = \\frac{1}{\\|A\\|^2}\\) for 500 iterations, and plot \\(\\|\\mathbf{x}^{(k)}-\\mathbf{x}\\|\\) to verify that \\(\\mathbf{x}^{(k)}\\) is converging to \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "images/IntroSlides.html#about-me",
    "href": "images/IntroSlides.html#about-me",
    "title": "Welcome to DATA 609",
    "section": "About Me",
    "text": "About Me\n\n\n\nDoctoral Lecturer at CUNY SPS\nPast research experience:\n\nPlasma physics and Applied Mathematics (NYU)\nEcology and Evolutionary Biology (Princeton)\n\nResearch using Bayesian methods and genomic data to improve global scale ocean and climate models\nTaught math at NYU"
  },
  {
    "objectID": "images/IntroSlides.html#why-optimization",
    "href": "images/IntroSlides.html#why-optimization",
    "title": "Welcome to DATA 609",
    "section": "Why Optimization?",
    "text": "Why Optimization?\n\n\n\nOptimization theory underlies most methods in data science\nThree foci:\n\nLeast Squares Optimization\nConvex Optimization\nLocal Optimization and Stochastic Gradient Descent"
  },
  {
    "objectID": "images/IntroSlides.html#course-websites",
    "href": "images/IntroSlides.html#course-websites",
    "title": "Welcome to DATA 609",
    "section": "Course Website(s)",
    "text": "Course Website(s)\n\n\n\nBrightspace to submit homework\nCourse Website"
  },
  {
    "objectID": "images/IntroSlides.html#course-website-and-meetup",
    "href": "images/IntroSlides.html#course-website-and-meetup",
    "title": "Welcome to DATA 609",
    "section": "Course Website and Meetup",
    "text": "Course Website and Meetup\n\n\n\nBrightspace to submit homework\nCourse Website\nCourse Meetup: Monday 6:45-7:45PM\nZoom Link"
  },
  {
    "objectID": "images/IntroSlides.html#slack-channel",
    "href": "images/IntroSlides.html#slack-channel",
    "title": "Welcome to DATA 609",
    "section": "Slack Channel",
    "text": "Slack Channel\n\n\n\nCourse slack channel for messaging and rapid communications\nhttps://data609-spring-2025.slack.com\nInvite Link"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks",
    "href": "images/IntroSlides.html#textbooks",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks-1",
    "href": "images/IntroSlides.html#textbooks-1",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#assignments",
    "href": "images/IntroSlides.html#assignments",
    "title": "Welcome to DATA 609",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n8 Lab Assignments (80%)\nFinal Project (20%)"
  },
  {
    "objectID": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "href": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "title": "Welcome to DATA 609",
    "section": "Looking forward to a great semester!",
    "text": "Looking forward to a great semester!\nThanks for watching"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-of-optimization-problems",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-of-optimization-problems",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology of Optimization Problems",
    "text": "Zoology of Optimization Problems\n\\[ \\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\\ng_i(\\mathbf{x}) \\leq 0, i=1, \\cdots, p\n\\]\n\nDifferent “names” for optimization problems based on what \\(f\\) and \\(g\\) are"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-linear-programs-lp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-linear-programs-lp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Linear Programs (LP)",
    "text": "Zoology: Linear Programs (LP)\n\\[ \\min_{\\mathbf{x}} \\mathbf{c}^T\\mathbf{x} \\\\\nA\\mathbf{x} \\leq \\mathbf{d}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-qp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-qp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Quadratic Programming (QP)",
    "text": "Zoology: Quadratic Programming (QP)\n\\[ \\min_{\\mathbf{x}} \\frac{1}{2} \\mathbf{x}^T P \\mathbf{x} + \\mathbf{c}^T\\mathbf{x} \\\\\nA\\mathbf{x} \\leq \\mathbf{d}\n\\]\n\n\\(P\\) positive (semi)-definite"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-quadratic-constraints-qcqp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-quadratic-constraints-qcqp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Quadratic Programming Quadratic Constraints (QCQP)",
    "text": "Zoology: Quadratic Programming Quadratic Constraints (QCQP)\n\\[ \\min_{\\mathbf{x}} \\frac{1}{2} \\mathbf{x}^T P \\mathbf{x} + \\mathbf{c}^T\\mathbf{x} \\\\\n\\frac{1}{2} \\mathbf{x}^T P_i \\mathbf{x} + \\mathbf{c}_i^T\\mathbf{x} \\leq \\mathbf{d}_i \\\\\nA\\mathbf{x} = \\mathbf{b}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-others",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-others",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Others…",
    "text": "Zoology: Others…\n\nSemidefinite Programming\nConic Optimization\nSecond Order Cone Programming"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development",
    "href": "meetups/Meetup-5/meetup-5.html#development",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nThese problems were studied sequentially starting in the 1800s\nDevelopment of Computer in the 1940s led to wide application\nChange in perspective:\n\nPre 1970s, Linear problems are easy, linear hard\nPost 1970s, Convex problems are easy, non-convex hard"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development-1",
    "href": "meetups/Meetup-5/meetup-5.html#development-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nRobert Vanderbei/NY Times 1984"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development-2",
    "href": "meetups/Meetup-5/meetup-5.html#development-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nwikipedia"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#weeks-ahead",
    "href": "meetups/Meetup-5/meetup-5.html#weeks-ahead",
    "title": "Meetup 5: Convex Sets",
    "section": "Week(s) Ahead",
    "text": "Week(s) Ahead\n\nWe begin convex optimization today\nWe will learn how to recognize and formulate convex problems\nAt first it will seem weird, but stick with it\nNext two weeks: Understanding whether constraint sets are convex"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#how-will-we-solve-convex-problems",
    "href": "meetups/Meetup-5/meetup-5.html#how-will-we-solve-convex-problems",
    "title": "Meetup 5: Convex Sets",
    "section": "How will we solve Convex Problems?",
    "text": "How will we solve Convex Problems?\n\nFor us \\(f\\) will be convex and so will be the constraint set\nThe hard part is recognizing the convexity, algorithms are very well developed\n\n\nimport cvxpy as cp\nx = cp.Variable()\ny = cp.Variable()\n\n# DCP problems.\nprob1 = cp.Problem(cp.Minimize(cp.square(x - y)),\n                    [x + y &gt;= 0,\n                    x &gt;= 4,\n                    y &lt;= 1])\nprob1.solve()"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Convex Set",
    "text": "What is a Convex Set\n\nIf you draw a line between any two points in the set, the entire line will be in the set"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set-1",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set-1",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Convex Set",
    "text": "What is a Convex Set\n\nIf even one pair of points exists where this fails, the set is not convex"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\nConvex optimization: local minimum always coincides with global minimum\nLocal minimum is findable"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-1",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex region\nSingle Local Min"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-2",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonconvex\nMany local min"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#convex-combinations",
    "href": "meetups/Meetup-5/meetup-5.html#convex-combinations",
    "title": "Meetup 5: Convex Sets",
    "section": "Convex Combinations",
    "text": "Convex Combinations\n\nIf we have two points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\), a convex combination is: \\[\n\\theta_1\\mathbf{x}_1 + (1-\\theta_1)\\mathbf{x}_2,\n\\]\n\nwhere \\(0\\leq\\theta_1\\leq 1\\)\n\nGenearlize to more points (weighted averages): \\[\n\\sum_i \\theta_i\\mathbf{x}_i, \\quad 0\\leq \\theta_i, \\quad\n\\sum_i \\theta_i = 1\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#math-definition-of-convex-sets",
    "href": "meetups/Meetup-5/meetup-5.html#math-definition-of-convex-sets",
    "title": "Meetup 5: Convex Sets",
    "section": "Math Definition of Convex Sets",
    "text": "Math Definition of Convex Sets\n\nA set is convex is for every pair of points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in the set, every convex combination of those two points is in the set."
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#definition-not-useful-in-practice",
    "href": "meetups/Meetup-5/meetup-5.html#definition-not-useful-in-practice",
    "title": "Meetup 5: Convex Sets",
    "section": "Definition Not Useful In Practice",
    "text": "Definition Not Useful In Practice\n\nUsing the definition is a last resort\nInstead, we will use a constructive approach\n\nHave example convex sets\nHave operations that preserve convexity\nWrite target set use operations on examples"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#affine-sets-lines-planes-hyperplanes",
    "href": "meetups/Meetup-5/meetup-5.html#affine-sets-lines-planes-hyperplanes",
    "title": "Meetup 5: Convex Sets",
    "section": "Affine Sets (Lines, Planes, Hyperplanes)",
    "text": "Affine Sets (Lines, Planes, Hyperplanes)\n\n\n\n\\(y = ax + b\\)\nSolutions of: \\[\nA\\mathbf{x} = \\mathbf{d}\n\\]\nUnderdetermined Case"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#convex-hull",
    "href": "meetups/Meetup-5/meetup-5.html#convex-hull",
    "title": "Meetup 5: Convex Sets",
    "section": "Convex Hull",
    "text": "Convex Hull\n\nSuppose you have a non-convex set\nDefine a new set to be the convex combination of all the points in the set"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#examples-polyhedra",
    "href": "meetups/Meetup-5/meetup-5.html#examples-polyhedra",
    "title": "Meetup 5: Convex Sets",
    "section": "Examples: Polyhedra",
    "text": "Examples: Polyhedra"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-1",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-1",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-2",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-2",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-3",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-3",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-4",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-4",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-5",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-5",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-6",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-6",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations\n\n\\[\\{x\\, |\\, C\\mathbf{x}\\preceq \\mathbf{d},\\, A\\mathbf{x} = \\mathbf{h} \\}\\]\n\n\\(\\preceq\\) stands for componentwise inequality\nTrue if every single entry satisfies inequality\nSometimes polyhedron stands for closed, polytope stands for open"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet",
    "href": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet",
    "title": "Meetup 5: Convex Sets",
    "section": "Example: Designing a Healthy Diet",
    "text": "Example: Designing a Healthy Diet\n\n\n\nYou are in charge of buying food\nLarge org\n\nArmy\nSchool System\nNGO/UN\n\nFood \\(i\\) has \\(V_{ij}\\) of nutrient \\(j\\)\n\n\n\n\n\nNutrient\nContent\n\n\n\n\nCalories (kCal)\n100\n\n\nFat (g)\n2\n\n\nCarbs (g)\n20\n\n\nProtein (g)\n2\n\n\nVitamin C (mg)\n90\n\n\nFiber (g)\n4\n\n\n\nOranges"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet-1",
    "href": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Example: Designing a Healthy Diet",
    "text": "Example: Designing a Healthy Diet\n\nEach nutrient has a minimum and/or maximum\n\n\n\n\nNutrient\nMinimum\nMaximum\n\n\n\n\nCalories\n2500\n\n\n\nFat\n50\n\n\n\nCarbs\n200\n\n\n\nProtein\n40\n200\n\n\nVitamin C\n100\n1000\n\n\nFiber\n20\n80"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#recommended-values-become-inequalities",
    "href": "meetups/Meetup-5/meetup-5.html#recommended-values-become-inequalities",
    "title": "Meetup 5: Convex Sets",
    "section": "Recommended Values Become Inequalities",
    "text": "Recommended Values Become Inequalities\n\n\\(\\mathbf{w}\\) is the diet vector, containing how much of each food we buy per person\nLet \\(\\mathbf{v}_j\\) be the \\(j\\)th row of \\(V\\) how much of nutrient \\(j\\) each food contains\nThen for each nutrient get an inequalities: \\[\nl_{min,j} \\leq \\mathbf{v}_j^T \\mathbf{w} \\leq l_{max,i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#rdv-constraints",
    "href": "meetups/Meetup-5/meetup-5.html#rdv-constraints",
    "title": "Meetup 5: Convex Sets",
    "section": "RDV Constraints",
    "text": "RDV Constraints\n\nSplit the inequalities up:\n\n\\[\n-\\mathbf{v}_j^T\\mathbf{w} \\leq -l_{min,j} \\\\\n\\mathbf{v}_j^T\\mathbf{w} \\leq l_{max,j}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#constraints-are-a-polyhedron",
    "href": "meetups/Meetup-5/meetup-5.html#constraints-are-a-polyhedron",
    "title": "Meetup 5: Convex Sets",
    "section": "Constraints are a polyhedron",
    "text": "Constraints are a polyhedron\n\nForm matrix \\(U\\) from rows \\(\\mathbf{v}\\)\nIf both positive and negative, include both \\(-\\mathbf{v}_j\\) and \\(\\mathbf{v}_J\\).\nIf only have a negative or positive, include just one\n\\(\\mathbf{l}\\) contains upper and lower bounds\nAdd constraint that \\(-\\mathbf{w}\\preceq 0\\) \\[ U\\mathbf{w} \\preceq \\mathbf{l}\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#each-food-has-a-cost",
    "href": "meetups/Meetup-5/meetup-5.html#each-food-has-a-cost",
    "title": "Meetup 5: Convex Sets",
    "section": "Each Food Has a Cost",
    "text": "Each Food Has a Cost\n\nEach food has a cost \\(c_i\\)\nDescribe with vector \\(\\mathbf{c}\\)\nObjective function \\(\\mathbf{c}^T\\mathbf{w}\\)\nOptimization Problem:\n\n\\[\n\\min_{\\mathbf{w}} \\mathbf{c}^T\\mathbf{w} \\\\\nU\\mathbf{w} \\preceq \\mathbf{l}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#simplex",
    "href": "meetups/Meetup-5/meetup-5.html#simplex",
    "title": "Meetup 5: Convex Sets",
    "section": "Simplex",
    "text": "Simplex\n\n\n\nSimplex is set of points satisfying: \\[\n\\{\\mathbf{p}\\, |\\, \\sum_{i}p_i=1,\\, p_i\\geq 0\\}\n\\]\nDiscrete probability distribution"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#half-planes",
    "href": "meetups/Meetup-5/meetup-5.html#half-planes",
    "title": "Meetup 5: Convex Sets",
    "section": "Half-Planes",
    "text": "Half-Planes\n\n\n\nSpecial case of polyhedron (and also cone)\n\\(\\mathbf{c}^T\\mathbf{x} \\leq d\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nBall has center and radius \\[\nB(\\mathbf{x}_c,r) = \\{\\mathbf{x}\\,|\\, \\|\\mathbf{x}-\\mathbf{x}_c\\|\\leq r\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-1",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nEllipsoid described by positive definite matrix \\(P\\) \\[\nE(\\mathbf{x}_c,P) = \\{\\mathbf{x}\\,|\\, (\\mathbf{x}-\\mathbf{x}_c)^TP^{-1}(\\mathbf{x}-\\mathbf{x}_x)\\leq 1\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-2",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nEllipsoid described by positive definite matrix \\(P\\)\nAlternately: \\[\nE(\\mathbf{x}_c,P) = \\{\\mathbf{x}\\,|\\, \\|A\\mathbf{x}-\\mathbf{x_c}  \\|\\leq 1  \\},\n\\]\nWhere \\(P = A^TA\\), \\(A\\) square, non-singular"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#examples",
    "href": "meetups/Meetup-5/meetup-5.html#examples",
    "title": "Meetup 5: Convex Sets",
    "section": "Examples:",
    "text": "Examples:\n\nLimit on expected return variance in a portfolio:\n\n\\(\\mathbf{w}^T\\Gamma\\mathbf{w} \\leq v_{max}\\)\n\nLimit on distance of an antenna from a transmitter:\n\n\\(\\|\\mathbf{x}-\\mathbf{x}_c\\|&lt; r\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#cones",
    "href": "meetups/Meetup-5/meetup-5.html#cones",
    "title": "Meetup 5: Convex Sets",
    "section": "Cones",
    "text": "Cones\n\nCone is a set which has the following property:\nIf \\(\\mathbf{x}\\) is in the cone, then \\(\\theta \\mathbf{x}\\) is also in the cone, for \\(\\theta\\geq 0\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#conic-combination",
    "href": "meetups/Meetup-5/meetup-5.html#conic-combination",
    "title": "Meetup 5: Convex Sets",
    "section": "Conic Combination",
    "text": "Conic Combination\n\n\n\nLike convex combination, but \\(\\theta\\) can be large: \\[\n\\sum_{i}\\mathbf{x}_i\\theta_i,\\, \\theta_i \\geq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#conic-combination-1",
    "href": "meetups/Meetup-5/meetup-5.html#conic-combination-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Conic Combination",
    "text": "Conic Combination\n\n\n\nLike convex combination, but \\(\\theta\\) can be large: \\[\n\\sum_{i}\\mathbf{x}_i\\theta_i,\\, \\theta_i \\geq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#norm-balls-and-norm-cone",
    "href": "meetups/Meetup-5/meetup-5.html#norm-balls-and-norm-cone",
    "title": "Meetup 5: Convex Sets",
    "section": "Norm Balls and Norm Cone",
    "text": "Norm Balls and Norm Cone\n\nFor any norm \\(N\\): \\[\nB_N(r,\\mathbf{x}_c) = \\{\\mathbf{x}\\,|\\, \\|\\mathbf{x}-\\mathbf{x}_c\\|_N &lt; r\\}\n\\]\n\\(B_N(r,\\mathbf{x}_c)\\) is Convex and called a “Norm Ball”\nNorm Cone: \\[\n\\{[\\mathbf{x}\\,, t]\\,|\\, \\|\\mathbf{x}\\|_N \\leq t\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#norm-cone",
    "href": "meetups/Meetup-5/meetup-5.html#norm-cone",
    "title": "Meetup 5: Convex Sets",
    "section": "Norm Cone",
    "text": "Norm Cone\n\nHere the \\(t\\) is restricting how wide \\(\\mathbf{x}\\) is allowed to be"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#optimization-under-uncertainty",
    "href": "meetups/Meetup-5/meetup-5.html#optimization-under-uncertainty",
    "title": "Meetup 5: Convex Sets",
    "section": "Optimization Under Uncertainty",
    "text": "Optimization Under Uncertainty\n\nConsider “uncertain” nutrition optimization problem \\[\n\\min_{\\mathbf{w}} \\mathbf{c}^T\\mathbf{w} \\\\\nU\\mathbf{w} \\preceq \\mathbf{l}\n\\]\nThe nutrient contents in \\(U\\) are random\nVariance \\(\\sigma_i\\), for each nutrient, mean \\(\\bar{U_i}\\).\nInsist that: \\[\n\\bar{U_i}\\mathbf{w} + k\\|\\sigma_i^T\\mathbf{w}  \\| \\leq l_i\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#thanks",
    "href": "meetups/Meetup-5/meetup-5.html#thanks",
    "title": "Meetup 5: Convex Sets",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "href": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb= np.random.randn(10)\n\n\na @ b\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nb.T @ a.T\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.linalg.inv(a) @ a\n\narray([[ 1.00000000e+00, -3.65595152e-16, -1.37241160e-16,\n        -1.67938840e-15,  9.33527437e-16,  1.44199349e-15,\n         1.15089786e-16, -1.55696214e-15,  1.11022302e-15,\n        -2.63677968e-16],\n       [ 1.26550255e-15,  1.00000000e+00, -2.68494358e-16,\n         1.42303383e-15, -9.79693103e-16, -4.51521472e-15,\n         2.63044668e-15,  1.74407151e-15, -2.66453526e-15,\n         4.44089210e-16],\n       [-1.64423122e-15, -1.66173972e-16,  1.00000000e+00,\n         2.75288470e-15,  1.74102465e-16,  1.54544175e-15,\n        -2.79739957e-15, -2.04740762e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [-3.51858931e-16,  6.23676926e-17,  2.66458886e-16,\n         1.00000000e+00,  9.38263196e-17,  2.61301662e-16,\n         2.45064178e-16, -1.53357911e-16,  4.44089210e-16,\n        -4.44089210e-16],\n       [ 7.84703105e-16, -1.04920005e-15,  4.13020606e-16,\n        -2.12483741e-15,  1.00000000e+00, -4.53483867e-15,\n         4.18017106e-15,  1.71820802e-15,  0.00000000e+00,\n         0.00000000e+00],\n       [ 9.37535921e-16,  3.43043467e-16,  9.65347805e-17,\n         6.68706576e-16, -4.20016708e-16,  1.00000000e+00,\n        -1.09267442e-15,  1.01169627e-16,  0.00000000e+00,\n         4.44089210e-16],\n       [-8.71025446e-16,  1.09123436e-15, -7.64517688e-16,\n         9.73897302e-16, -6.40358430e-16,  3.68586912e-15,\n         1.00000000e+00, -1.54120216e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [ 2.86959469e-15,  2.45431198e-16,  5.06260913e-16,\n        -5.81240373e-16, -5.42632247e-16, -2.38411883e-15,\n         1.09159912e-15,  1.00000000e+00, -8.88178420e-16,\n        -4.44089210e-16],\n       [-3.57782462e-17, -5.09336718e-16,  7.90198623e-16,\n        -2.53964484e-15, -7.71687051e-17, -1.71387168e-15,\n         1.28935306e-15,  1.41638585e-15,  1.00000000e+00,\n        -8.88178420e-16],\n       [-5.10166112e-16,  3.36418889e-16,  3.54329540e-16,\n        -1.34491999e-15, -3.29604245e-16, -7.44603616e-16,\n         1.18536447e-15,  5.30313254e-16,  4.44089210e-16,\n         1.00000000e+00]])\n\n\n\nnp.linalg.svd(a).Vh\n\narray([[ 0.28383114,  0.13031739,  0.08488282,  0.26915419, -0.05833712,\n        -0.55745972,  0.04622785,  0.52583554, -0.47933131, -0.01565541],\n       [-0.7299212 ,  0.04346431, -0.15481374,  0.1199476 ,  0.28639583,\n        -0.35620311,  0.24101023, -0.22640736, -0.21810744, -0.24728702],\n       [ 0.07621821,  0.43546203,  0.2088927 ,  0.28230896, -0.17741055,\n        -0.17934521,  0.42879952, -0.01233035,  0.62390845, -0.21049244],\n       [-0.39305518,  0.11448225,  0.09008748,  0.41564326, -0.41297868,\n         0.10835773, -0.0692628 , -0.00268073, -0.05982432,  0.67885942],\n       [ 0.06933558,  0.34860056, -0.39022658,  0.26430987, -0.41367073,\n         0.19057494, -0.35422431, -0.22534478, -0.22225331, -0.46737544],\n       [-0.05118823, -0.68217798, -0.03157757,  0.58681909,  0.00875722,\n         0.07532819, -0.10790011,  0.20278167,  0.24018863, -0.26543311],\n       [ 0.31944117, -0.2503526 , -0.62925149,  0.04300488, -0.15611409,\n        -0.20160406,  0.45924193, -0.30071194, -0.00703338,  0.26660451],\n       [ 0.25406731,  0.17492689,  0.15182522,  0.44334045,  0.50247181,\n         0.45816405,  0.28794845, -0.19810239, -0.31166484,  0.05968817],\n       [ 0.22725739, -0.10877031,  0.36247981,  0.15032739,  0.06499115,\n        -0.45411298, -0.38287566, -0.64660084,  0.0248702 ,  0.08237576],\n       [ 0.02620891,  0.29393688, -0.46195436,  0.15967492,  0.51017133,\n        -0.14029136, -0.41663193,  0.18003309,  0.35346832,  0.25124871]])\n\n\n\nnp.diagonal?\n\n\nSignature:       np.diagonal(a, offset=0, axis1=0, axis2=1)\nCall signature:  np.diagonal(*args, **kwargs)\nType:            _ArrayFunctionDispatcher\nString form:     &lt;function diagonal at 0x7b3729c832e0&gt;\nFile:            ~/miniforge3/envs/stan/lib/python3.11/site-packages/numpy/core/fromnumeric.py\nDocstring:      \nReturn specified diagonals.\nIf `a` is 2-D, returns the diagonal of `a` with the given offset,\ni.e., the collection of elements of the form ``a[i, i+offset]``.  If\n`a` has more than two dimensions, then the axes specified by `axis1`\nand `axis2` are used to determine the 2-D sub-array whose diagonal is\nreturned.  The shape of the resulting array can be determined by\nremoving `axis1` and `axis2` and appending an index to the right equal\nto the size of the resulting diagonals.\nIn versions of NumPy prior to 1.7, this function always returned a new,\nindependent array containing a copy of the values in the diagonal.\nIn NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\nbut depending on this fact is deprecated. Writing to the resulting\narray continues to work as it used to, but a FutureWarning is issued.\nStarting in NumPy 1.9 it returns a read-only view on the original array.\nAttempting to write to the resulting array will produce an error.\nIn some future release, it will return a read/write view and writing to\nthe returned array will alter your original array.  The returned array\nwill have the same type as the input array.\nIf you don't write to the array returned by this function, then you can\njust ignore all of the above.\nIf you depend on the current behavior, then we suggest copying the\nreturned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\nof just ``np.diagonal(a)``. This will work with both past and future\nversions of NumPy.\nParameters\n----------\na : array_like\n    Array from which the diagonals are taken.\noffset : int, optional\n    Offset of the diagonal from the main diagonal.  Can be positive or\n    negative.  Defaults to main diagonal (0).\naxis1 : int, optional\n    Axis to be used as the first axis of the 2-D sub-arrays from which\n    the diagonals should be taken.  Defaults to first axis (0).\naxis2 : int, optional\n    Axis to be used as the second axis of the 2-D sub-arrays from\n    which the diagonals should be taken. Defaults to second axis (1).\nReturns\n-------\narray_of_diagonals : ndarray\n    If `a` is 2-D, then a 1-D array containing the diagonal and of the\n    same type as `a` is returned unless `a` is a `matrix`, in which case\n    a 1-D array rather than a (2-D) `matrix` is returned in order to\n    maintain backward compatibility.\n    If ``a.ndim &gt; 2``, then the dimensions specified by `axis1` and `axis2`\n    are removed, and a new axis inserted at the end corresponding to the\n    diagonal.\nRaises\n------\nValueError\n    If the dimension of `a` is less than 2.\nSee Also\n--------\ndiag : MATLAB work-a-like for 1-D and 2-D arrays.\ndiagflat : Create diagonal arrays.\ntrace : Sum along diagonals.\nExamples\n--------\n&gt;&gt;&gt; a = np.arange(4).reshape(2,2)\n&gt;&gt;&gt; a\narray([[0, 1],\n       [2, 3]])\n&gt;&gt;&gt; a.diagonal()\narray([0, 3])\n&gt;&gt;&gt; a.diagonal(1)\narray([1])\nA 3-D example:\n&gt;&gt;&gt; a = np.arange(8).reshape(2,2,2); a\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n&gt;&gt;&gt; a.diagonal(0,  # Main diagonals of two arrays created by skipping\n...            0,  # across the outer(left)-most axis last and\n...            1)  # the \"middle\" (row) axis first.\narray([[0, 6],\n       [1, 7]])\nThe sub-arrays whose main diagonals we just obtained; note that each\ncorresponds to fixing the right-most (column) axis, and that the\ndiagonals are \"packed\" in rows.\n&gt;&gt;&gt; a[:,:,0]  # main diagonal is [0 6]\narray([[0, 2],\n       [4, 6]])\n&gt;&gt;&gt; a[:,:,1]  # main diagonal is [1 7]\narray([[1, 3],\n       [5, 7]])\nThe anti-diagonal can be obtained by reversing the order of elements\nusing either `numpy.flipud` or `numpy.fliplr`.\n&gt;&gt;&gt; a = np.arange(9).reshape(3, 3)\n&gt;&gt;&gt; a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n&gt;&gt;&gt; np.fliplr(a).diagonal()  # Horizontal flip\narray([2, 4, 6])\n&gt;&gt;&gt; np.flipud(a).diagonal()  # Vertical flip\narray([6, 4, 2])\nNote that the order in which the diagonal is retrieved varies depending\non the flip function.\nClass docstring:\nClass to wrap functions with checks for __array_function__ overrides.\nAll arguments are required, and can only be passed by position.\nParameters\n----------\ndispatcher : function or None\n    The dispatcher function that returns a single sequence-like object\n    of all arguments relevant.  It must have the same signature (except\n    the default values) as the actual implementation.\n    If ``None``, this is a ``like=`` dispatcher and the\n    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n    first (additional and positional) argument.\nimplementation : function\n    Function that implements the operation on NumPy arrays without\n    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n    will be forwarded to this (and the ``dispatcher``) as if using\n    ``*args, **kwargs``.\nAttributes\n----------\n_implementation : function\n    The original implementation passed in.\n\n\n\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nb = np.random.randn(100,2)\n\n\nnp.linalg.svd(b)\n\nSVDResult(U=array([[ 3.18048731e-05,  6.74648048e-02, -6.41579788e-02, ...,\n        -4.70143275e-02, -1.17012062e-02,  5.72707204e-02],\n       [-1.67854420e-01,  1.70642880e-01, -1.21850722e-01, ...,\n        -1.51615802e-01,  2.00743368e-02, -1.09802273e-01],\n       [-7.00863302e-02,  1.18377452e-01,  9.84604342e-01, ...,\n        -1.72681740e-02,  1.25486957e-03, -7.25125082e-03],\n       ...,\n       [-1.04494452e-01,  1.19440600e-01, -1.73568100e-02, ...,\n         9.79733632e-01,  1.95664282e-03, -1.09753255e-02],\n       [ 2.32789965e-02, -5.98960947e-04,  1.31502367e-03, ...,\n         2.01688869e-03,  9.99525031e-01,  2.51988849e-03],\n       [-1.23784387e-01,  8.99047891e-03, -7.56209658e-03, ...,\n        -1.12822312e-02,  2.51689274e-03,  9.86617378e-01]]), S=array([9.7586975 , 8.78873925]), Vh=array([[-0.46515128, -0.8852312 ],\n       [-0.8852312 ,  0.46515128]]))\n\n\n\\[ A = U\\Sigma V^T\\] \\[ (A^TA)^{-1} A = (V\\Sigma^T U^T U \\Sigma^T V^T)^{-1} U\\Sigma V^T    \\] \\[A^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V\\Sigma^T U^T \\] \\[ A^+ = V (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T \\] \\[ A^+ = V\\Sigma^+ U^T \\]\n\nU = np.linalg.svd(b)[0]\n\n\nU.T @ U\n\narray([[ 1.00000000e+00, -1.24938610e-16,  5.63615904e-17, ...,\n         2.77555756e-17, -5.20417043e-18,  0.00000000e+00],\n       [-1.24938610e-16,  1.00000000e+00, -3.15292394e-17, ...,\n        -3.46944695e-17,  6.50521303e-18, -9.54097912e-18],\n       [ 5.63615904e-17, -3.15292394e-17,  1.00000000e+00, ...,\n        -3.46944695e-18, -1.30104261e-18,  5.20417043e-18],\n       ...,\n       [ 2.77555756e-17, -3.46944695e-17, -3.46944695e-18, ...,\n         1.00000000e+00,  4.82754810e-19, -4.24892617e-18],\n       [-5.20417043e-18,  6.50521303e-18, -1.30104261e-18, ...,\n         4.82754810e-19,  1.00000000e+00,  3.18478586e-19],\n       [ 0.00000000e+00, -9.54097912e-18,  5.20417043e-18, ...,\n        -4.24892617e-18,  3.18478586e-19,  1.00000000e+00]])\n\n\n\n$$ V(\\Sigma^T\\Sigma)^{-1}\\Sigma^TU^T b = x $$\n$$ (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b  = V^T x$$\n$$ (\\Sigma^T U^T b) = (\\Sigma^T\\Sigma) (V^T x)$$\n\n`V @ np.linalg.solve(S^T @ S, U.T @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/LatexTutorial.html",
    "href": "meetups/Meetup-2/LatexTutorial.html",
    "title": "This is a markdown cell",
    "section": "",
    "text": "Here is a math expression \\[\nx+1 = 0\n\\]\nHere is a more complex\n\\[\nx^2_0 + y_1 = 0\n\\]\nHere is how you exert greater control over the sub and superscripts\n\\[\nx^{2y_1-2}_{opt}\n\\]\nHere is how we do a sum\n\\[\n\\sum_{i=1}^{100} x_i\n\\]\nHere is how to write an integral:\n\\[\n\\int_{0}^{\\infty} dx \\exp(-x^2)\n\\]\n\\[\n\\int_{0}^{\\infty} dx e^{-x^2}\n\\]\n\\[\nf(x,y) = \\frac{\\sin(xy) }{1 + \\exp(x+y) }\n\\]\n\\[\nf(x,y) = x + y^2\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = 1\n\\]\n\\[\n\\frac{\\partial f}{\\partial y} = 2y\n\\]\n\\[\n\\nabla f\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\n\\end{bmatrix} ^T\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n\\]\nThe Identity in 3D\n\\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\\n                    0 & 1 & 0 \\\\\n                    0 & 0 & 1\n    \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#richardson-iteration",
    "href": "meetups/Meetup-4/HW1Review.html#richardson-iteration",
    "title": "Lab 1 Review",
    "section": "Richardson Iteration",
    "text": "Richardson Iteration\n\nConsider Problem 3(a), showing that the Pseudoinverse is a fixed point of the Richardson Iteration algorithm:\n\n\\[\nx^{(k+1)} = x^{(k)} - \\mu A^{T}\\left(Ax^{(k)} - b \\right)\n\\]\n\nNext Step: Substitute \\(A^{+}b\\) for \\(x^{(x)}\\):\n\n\\[ x^{(k+1)} = A^{+}b - \\mu A^{(T)}\\left(AA^{+}b - b\\right)\\]"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#most-common-mistake",
    "href": "meetups/Meetup-4/HW1Review.html#most-common-mistake",
    "title": "Lab 1 Review",
    "section": "Most Common Mistake",
    "text": "Most Common Mistake\n\nMany of you then concluded:\n\n\\[ AA^{+}b - b = 0\\]\n\nSeveral justification, but:\n\n\nA = matrix(rnorm(40),nrow=10,ncol=4)\nb = rnorm(10)\n\nt(A  %*% solve(t(A) %*% A) %*% t(A) %*%  b)\n\n           [,1]       [,2]      [,3]       [,4]      [,5]       [,6]    [,7]\n[1,] -0.6893472 0.06370777 0.2121411 -0.2247358 0.2622433 -0.1384912 0.58586\n          [,8]       [,9]      [,10]\n[1,] 0.8049595 -0.2823374 -0.3482485"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#why-isnt-it-true",
    "href": "meetups/Meetup-4/HW1Review.html#why-isnt-it-true",
    "title": "Lab 1 Review",
    "section": "Why isn’t it true?",
    "text": "Why isn’t it true?\n\n\\(A^+\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#week-summary",
    "href": "meetups/Meetup7/meetup-7.html#week-summary",
    "title": "DATA 609 Meetup 7",
    "section": "Week Summary",
    "text": "Week Summary\n\nWorking on grading….\nThis week: definition and examples of convex functions\nNext week: How to construct convex functions\nReading: Section 3.1, CVX tutorials\nLab 4 available"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function",
    "href": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function",
    "title": "DATA 609 Meetup 7",
    "section": "What is a convex function?",
    "text": "What is a convex function?\n\nStart with function \\(f:\\, \\mathbb{R}^N\\to\\mathbb{R}\\)\nGraph of \\(f\\) is below line between any two points on graph:"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function-1",
    "href": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function-1",
    "title": "DATA 609 Meetup 7",
    "section": "What is a convex function?",
    "text": "What is a convex function?\n\\[\nf(\\theta \\mathbf{x}_1 + (1-\\theta)\\mathbf{x}_2) \\leq \\theta f(\\mathbf{x}_1) + (1-\\theta)f(\\mathbf{x}_2)\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#concave",
    "href": "meetups/Meetup7/meetup-7.html#concave",
    "title": "DATA 609 Meetup 7",
    "section": "Concave",
    "text": "Concave\n\n\\(f\\) is concave if \\(-f\\) is convex\n\\(f\\) curves down instead of up"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#strict-concavity",
    "href": "meetups/Meetup7/meetup-7.html#strict-concavity",
    "title": "DATA 609 Meetup 7",
    "section": "Strict Concavity",
    "text": "Strict Concavity\n\nEqualty only at endpoints, i.e. for \\(\\theta \\neq 0, 1\\): \\[\nf(\\theta \\mathbf{x}_1 + (1-\\theta)\\mathbf{x}_2) \\leq \\theta f(\\mathbf{x}_1) + (1-\\theta)f(\\mathbf{x}_2)\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#graph-in-higher-dimensions",
    "href": "meetups/Meetup7/meetup-7.html#graph-in-higher-dimensions",
    "title": "DATA 609 Meetup 7",
    "section": "Graph in Higher Dimensions",
    "text": "Graph in Higher Dimensions\n\ngraph of a function \\(f: \\mathbf{dom}(f)\\to\\mathbb{R}\\): \\[G(f) = \\{(\\mathbf{x},f(\\mathbf{x}))\\,|\\, \\mathbf{x}\\in\\mathbf{dom}(f)\\}\\]\n1D: \\((x,y=f(x))\\), familiar graph\n2D: \\((x,y,z=f(x,y))\\), 3D surface/landscape\n3D+: \\(\\left(x_1,x_2,x_3,...,x_n,z=f(x_1,x_2,x_3,...,x_n)\\right)\\)\n\nHypersurface"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#tangent-lines",
    "href": "meetups/Meetup7/meetup-7.html#tangent-lines",
    "title": "DATA 609 Meetup 7",
    "section": "Tangent Lines",
    "text": "Tangent Lines\n\nGraph lies above tangent lines/planes/hyperplanes"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-to-global-info",
    "href": "meetups/Meetup7/meetup-7.html#local-to-global-info",
    "title": "DATA 609 Meetup 7",
    "section": "Local to Global Info",
    "text": "Local to Global Info\n\nTangent line is a local property"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-to-global-info-1",
    "href": "meetups/Meetup7/meetup-7.html#local-to-global-info-1",
    "title": "DATA 609 Meetup 7",
    "section": "Local to Global Info",
    "text": "Local to Global Info\n\nBut for Convex gives global information!"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-min-is-global-min",
    "href": "meetups/Meetup7/meetup-7.html#local-min-is-global-min",
    "title": "DATA 609 Meetup 7",
    "section": "Local Min is Global Min",
    "text": "Local Min is Global Min\n\nLocal minimum implies flat tangent"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-if-there-is-no-single-tangent",
    "href": "meetups/Meetup7/meetup-7.html#what-if-there-is-no-single-tangent",
    "title": "DATA 609 Meetup 7",
    "section": "What if there is no single tangent?",
    "text": "What if there is no single tangent?\n\n\nSlightly different argument is needed"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-min-is-a-global-min",
    "href": "meetups/Meetup7/meetup-7.html#local-min-is-a-global-min",
    "title": "DATA 609 Meetup 7",
    "section": "Local Min is a Global Min",
    "text": "Local Min is a Global Min\n\nTake \\(f:\\,\\mathbb{R}^2\\to\\mathbb{R}\\)\nAssume there is a local min that isn’t the global min"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-affine",
    "href": "meetups/Meetup7/meetup-7.html#examples-affine",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Affine",
    "text": "Examples: Affine\n\nAffine \\(f(\\mathbf{x}) = \\mathbf{c}^T\\mathbf{x} + \\mathbf{d}\\)\nBoth convex and concave"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConvex for \\(\\alpha\\geq 1\\) on \\(\\mathbb{R}_+\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions-1",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions-1",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConcave for \\(0&lt;\\alpha\\leq 1\\) on \\(\\mathbb{R}_+\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions-2",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions-2",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConvex for \\(\\alpha\\leq 0\\) on \\(\\mathbb{R}_{++}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-xalpha",
    "href": "meetups/Meetup7/meetup-7.html#examples-xalpha",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: \\(|x|^{\\alpha}\\)",
    "text": "Examples: \\(|x|^{\\alpha}\\)\n\n\\(f(x) = |x|^{\\alpha}\\) is convex on \\(\\mathbb{R}\\) for \\(\\alpha\\geq 1\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-ealpha-x",
    "href": "meetups/Meetup7/meetup-7.html#examples-ealpha-x",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: \\(e^{\\alpha x}\\)",
    "text": "Examples: \\(e^{\\alpha x}\\)\n\n\\(e^{\\alpha x}\\) is convex for any \\(\\alpha\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-rectified-linear-unit",
    "href": "meetups/Meetup7/meetup-7.html#examples-rectified-linear-unit",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Rectified Linear Unit",
    "text": "Examples: Rectified Linear Unit\n\n\\(\\max(0,x)\\) is convex\n\\(\\min(0,x)\\) is concave"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-log-and-entropy",
    "href": "meetups/Meetup7/meetup-7.html#examples-log-and-entropy",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: log and entropy",
    "text": "Examples: log and entropy\n\n\\(f(x) = \\log(x)\\) is concave on \\(\\mathbb{R}_{++}\\)\n\\(f(x) = -x\\log(x)\\) is concave on \\(\\mathbb{R}_{+}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-norms",
    "href": "meetups/Meetup7/meetup-7.html#examples-norms",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Norms",
    "text": "Examples: Norms\n\nAll norms are convex on \\(\\mathbb{R}^N\\to\\mathbb{R}\\) \\[ \\|\\mathbf{x}\\| = \\left(\\sum_i x_i^2\\right)^{1/2} , \\|\\mathbf{x}\\|_p = \\left(\\sum_i x_i ^p\\right)^{1/p} \\\\\n\\|\\mathbf{x}\\|_{\\infty} = \\max(x_1,x_2,...,x_n)\n\\]\nSum of squares also \\(\\|\\mathbf{x}\\|^2 = \\mathbf{x}^T\\mathbf{x}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nConsider Least Squares Problem\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2\n\\]\n\nSuppose \\(\\mathbf{x}\\) has many entries\n\\(A\\) could even be wide\nYou only want to use a few features\nYour data may have outliers"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-1",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-1",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nApply the following penalty:\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 + \\lambda \\sum_i |x_i|\n\\]\n\nEquivalent to a norm:\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 + \\lambda\\|\\mathbf{x}\\|_1\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-2",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-2",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nWe will learn that adding convex functions preserves convexity\nLasso regularization of convex problem also convex\nHas a tendency to produce robust solutions with sparse coefficients \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#softmax-and-geometric-mean",
    "href": "meetups/Meetup7/meetup-7.html#softmax-and-geometric-mean",
    "title": "DATA 609 Meetup 7",
    "section": "Softmax and Geometric Mean",
    "text": "Softmax and Geometric Mean\n\n\\(\\log\\left(\\sum_{i=1}^n e^{x_i}\\right)\\) is convex\n\nThis is a smoothed maximum function\n\n\\(\\left(\\Pi_{i=1}^n x_i\\right)^{1/n}\\)\n\nGeometric mean"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#first-and-second-order-conditions",
    "href": "meetups/Meetup7/meetup-7.html#first-and-second-order-conditions",
    "title": "DATA 609 Meetup 7",
    "section": "First and Second Order Conditions",
    "text": "First and Second Order Conditions\n\nFirst Order Condition: \\(f\\) is convex if and only if graph lies above all tangent lines\nSecond Order Condition: \\(f\\) is convex if and only if the Hessian is positive semi-definite at all points in (convex) domain \\[\n\\left(\\nabla^2{f}\\right)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\succeq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#second-order-condition",
    "href": "meetups/Meetup7/meetup-7.html#second-order-condition",
    "title": "DATA 609 Meetup 7",
    "section": "Second Order Condition",
    "text": "Second Order Condition\n\nShow that \\(f(x) = e^{\\alpha x}\\) is convex\nTake first derivative:\n\n\\(f'(x) = \\alpha e^{\\alpha x}\\)\n\nTake second derivative:\n\n\\(f''(x) = \\alpha^2 e^{\\alpha x}\\)\n\n\\(f''(x) \\geq 0\\), so \\(f\\) is convex\nAlso implies sum of convex is convex"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#quadratic-over-linear",
    "href": "meetups/Meetup7/meetup-7.html#quadratic-over-linear",
    "title": "DATA 609 Meetup 7",
    "section": "Quadratic over Linear",
    "text": "Quadratic over Linear\n\n\\(f(x,y) = \\frac{x^2}{y}\\), \\(\\mathbf{dom}(f) = \\mathbb{R}\\times\\mathbb{R}_{++}\\)\nStart taking derivatives \\[\n\\nabla f = \\begin{bmatrix} \\frac{2x}{y} & -\\frac{x^2}{y^2} \\end{bmatrix}^T\n\\]\n\n\\[\\nabla^2 f = \\begin{bmatrix} \\frac{2}{y} & -\\frac{2x}{y^2} \\\\\n-\\frac{2x}{y^2} & \\frac{2x^2}{y^3} \\end{bmatrix}\n\\]\n\nPositive semidefinite because \\(\\mathrm{det}(\\nabla^2 f)\\geq 0\\) and \\(Tr(\\nabla^2 f) &gt; 0\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#restriction-to-line",
    "href": "meetups/Meetup7/meetup-7.html#restriction-to-line",
    "title": "DATA 609 Meetup 7",
    "section": "Restriction to Line",
    "text": "Restriction to Line\n\n\\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if and only if it is convex when restricted to every single line in \\(\\mathbb{R}^n\\)\n\\(g(t) = f(\\mathbf{b}+t\\mathbf{v})\\) is convex for all vectors \\(\\mathbf{v}\\), \\(\\mathbf{b}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#epigraph",
    "href": "meetups/Meetup7/meetup-7.html#epigraph",
    "title": "DATA 609 Meetup 7",
    "section": "Epigraph",
    "text": "Epigraph\n\n\\(\\mathbf{epi}(f)\\) is set of points above the graph of \\(f\\) \\[\n\\{(\\mathbf{x},z)\\,| \\mathbf{x}\\in\\mathbf{dom}(f),\\, z\\geq f(\\mathbf{x})\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#epigraph-1",
    "href": "meetups/Meetup7/meetup-7.html#epigraph-1",
    "title": "DATA 609 Meetup 7",
    "section": "Epigraph",
    "text": "Epigraph\n\n\\(f\\) is convex if and only if \\(\\mathbf{epi}(f)\\) is a convex set\nAllows for construction of more convex sets\n\\(f\\) is concave if and only if \\(\\mathbf{sub}(f)\\) is a convex set\nThis is convex: \\[\n\\{(x,y)| \\log(x) \\leq y \\ \\leq e^{\\alpha x}\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#subsuper-level-sets",
    "href": "meetups/Meetup7/meetup-7.html#subsuper-level-sets",
    "title": "DATA 609 Meetup 7",
    "section": "Sub/Super Level Sets",
    "text": "Sub/Super Level Sets\n\nSolution set of inequality defines sub: \\[\\mathbf{sub}_{\\alpha}(f) = \\{\\mathbf{x}| f(\\mathbf{x}\\leq \\alpha)\\}\\]\nSuper-level set has oppositve inequality\n\\(\\alpha\\)-sublevel sets of a convex function are convex\n\\(\\alpha\\)-superlevel sets of a concave function are convex"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensens-inequality-project",
    "href": "meetups/Meetup7/meetup-7.html#jensens-inequality-project",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen’s Inequality (Project?)",
    "text": "Jensen’s Inequality (Project?)\n\nUseful result in probability and statistics:\nIf \\(f\\) is a convex function and \\(x\\) a random variable, then: \\[\n\\mathrm{E}[f(x)]\\geq f(\\mathrm{E}[x])\n\\]\nBecause expectation is \\(\\sum_i p_i x_i\\), convex combination of values of \\(x\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensens-inequality",
    "href": "meetups/Meetup7/meetup-7.html#jensens-inequality",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen’s Inequality",
    "text": "Jensen’s Inequality\n ## Convexity/Concavity are very important in life"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensen-and-fragility",
    "href": "meetups/Meetup7/meetup-7.html#jensen-and-fragility",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen and Fragility",
    "text": "Jensen and Fragility\n\nSuppose something in your life is random\nConsider your payoff function \\(f(x)\\)\nIf your \\(f\\) is convex, Jensen says randomness helps you\nYour Outcome = \\(\\mathrm{E}[f(x)]\\) better than outcome of average \\(f(\\mathrm{E}[x])\\)\nIf your payoff is concave, Jensen says randomness hurts you"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensen-and-traffic",
    "href": "meetups/Meetup7/meetup-7.html#jensen-and-traffic",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen and Traffic",
    "text": "Jensen and Traffic\n\nYou are driving, and the amount of time it will take depends on the number of cars.\nWhen few cars on the road, trip length determined by speed limit\nWhen a lot of cars on the road, 30 minute trip could last 4 hours\nJensen says average trip time is longer than time under average traffic conditions"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensen-and-fragility-1",
    "href": "meetups/Meetup7/meetup-7.html#jensen-and-fragility-1",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen and Fragility",
    "text": "Jensen and Fragility\n\nConvex payouts are called anti-fragile\nConcave payouts are called fragile\nMany times in life we don’t can’t fully quantify the risks we face\nBut sometimes we can control the outcome for ourselves should something bad happen\nMoral- make your payouts convex if you can"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#thanks",
    "href": "meetups/Meetup7/meetup-7.html#thanks",
    "title": "DATA 609 Meetup 7",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "This website would not be possible without the quarto package. This course draws inspiration from several open educational resources on the internet, including two books and accompanying materials by Stephen Boyd and Lieven Vandenberghe: Introduction to Applied Linear Algebra and Convex Optimization",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "modules/module5.html",
    "href": "modules/module5.html",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\n\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning\nWe will use CVX for the first time in the next two weeks. If you have time go to https://www.cvxpy.org/tutorial/dcp/ or https://cvxr.rbind.io/cvxr_examples/cvxr_intro/ to begin learning the basics",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "modules/module5.html#homework",
    "href": "modules/module5.html#homework",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\n\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning\nWe will use CVX for the first time in the next two weeks. If you have time go to https://www.cvxpy.org/tutorial/dcp/ or https://cvxr.rbind.io/cvxr_examples/cvxr_intro/ to begin learning the basics",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "modules/module12.html",
    "href": "modules/module12.html",
    "title": "Module 12: Duality and Classifiers",
    "section": "",
    "text": "Learning Objectives\n\nDual Formulation of Optimization Problems\nLagrange Multipliers\nLinear and Nonlinear Classifiers, LDA\nSupport Vector Machines\n\n\n\nReadings\nSections 8.1, 8.2, 8.6, 8.7, 8.8 Convex Optimization",
    "crumbs": [
      "Topics",
      "12 - Duality and Classifiers"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9: Convex Optimization Problems and Disciplined Convex Programming",
    "section": "",
    "text": "Homework\nHomework 5 is due next week, Sunday at midnight.\n\n\nLearning Objectives\n\nDefinition of Convex Optimization Problem\nProblem Transformations\nImportant Subclasses of Convex Optimization Problems\nDiscilined Convex Programming and CVX\nVector Optimization and Scalarization\n\n\n\nReadings\nSections 4.1-4.4, 4.6-4.7 of Convex Optimization\nCVX users guide, read one of these: -python -R -julia",
    "crumbs": [
      "Topics",
      "9 - Convex Optimization and Disciplined Convex Programming"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, splines, and interpolation\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module10.html#homework",
    "href": "modules/module10.html#homework",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, splines, and interpolation\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Constructing Convex Sets",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nOperations that preserve convexity\n\nIntersection\nCartesian Product\nAffine Transformation\nPerspective and Linear Fractional Transformation\n\nIntroduce Disciplined Convex Programming\n\n\n\n\n\nSections 2.3 of Convex Optimization\nStart looking at cvx user guides:\n\ncvxpy\ncvxr",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module6.html#homework",
    "href": "modules/module6.html#homework",
    "title": "Module 6 - Constructing Convex Sets",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nOperations that preserve convexity\n\nIntersection\nCartesian Product\nAffine Transformation\nPerspective and Linear Fractional Transformation\n\nIntroduce Disciplined Convex Programming\n\n\n\n\n\nSections 2.3 of Convex Optimization\nStart looking at cvx user guides:\n\ncvxpy\ncvxr",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3: Statistical Applications of Least Squares",
    "section": "",
    "text": "This week we will learn more about the applications of least squares methods in statistics. We will learn about how weighted least squares methods allow us to apply linear regression in more cases, including those with heteroscedastic errors. Then we will show how to fit data using interpolating functions, and autoregressive models for time series. This discussion will lead into feature engineerging. Finally, we will explore validation and regularization, which is our first example of multi-objective least squares.\n\nDeliverables\nHomework 2 will be due Sunday, February 23rd at midnight.\n\n\nLearning Objectives\n\nLeast Squares and Linear Regression\nGeneralized Least Squares\nRecursive/Online Least Squares\nRegularization and Multi-Objective Least Squares\n\n\n\nReadings\nChapters 13 of Introduction to Applied Linear Algebra\nChapter 14 (on classification methods) is interesting but we won’t cover it.\nSections 4.2 and 4.5 of Introduction to Algorithms for Data Mining and Machine Learning",
    "crumbs": [
      "Topics",
      "3 - Least Squares and Statistics"
    ]
  },
  {
    "objectID": "modules/module13.html",
    "href": "modules/module13.html",
    "title": "Module 13: Local Optimization Algorithms and Neural Networks",
    "section": "",
    "text": "Homework\nHomework 7 is due in two weeks, Sunday at midnight\n\n\nLearning Objectives\n\nNeural Network Learning and Backpropagation\nGradient Descent\nNewton’s Method\nConvergence Rates and Condition Number\nAcceleration Using Momentum\n\n\n\nReadings\nChapter 9 of Convex Optimization\nChapter 8.1-8.4 of Introduction to Algorithms for Data Mining Machine Learning\nOr alternatively deeplearningbook Chapter 6",
    "crumbs": [
      "Topics",
      "13 - Local Optimization and Neural Networks"
    ]
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Constrained Least Squares and Applications",
    "section": "",
    "text": "Homework\nHomework 2 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving Least Squares problems with Linear Equality Constraints\nApplications to Portfolio Optimization\nApplications to B-Splines\n\n\n\nReadings\nChapters 16 and 17 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "4 - Constrained Least Squares and Applications"
    ]
  },
  {
    "objectID": "posts/2025-03-04-Meetup-6-Slides.html",
    "href": "posts/2025-03-04-Meetup-6-Slides.html",
    "title": "Meetup 6: Introduction to Convex Sets",
    "section": "",
    "text": "I have uploaded the slides and video for Meetup 6, which happened last night.\nWe learned about rules for constructing convex sets using intersections, cartesian products, and transformations by affine, linear fractional, and perspective functions.\n\nClick here for slides\nWatch Meetup 6 Here"
  },
  {
    "objectID": "posts/2025-02-03-Meetup-2-Slides.html",
    "href": "posts/2025-02-03-Meetup-2-Slides.html",
    "title": "Meetup 2: Least Squares Optimization",
    "section": "",
    "text": "Meetup 2 is tonight at 6:45PM. We will introduce least squares optimization, which is the most commonly used and possibly most important type of optimization problem. We will go through a few applications to illustrate least squares in its simplest formulation, and discuss some practical issues with solving least squares problems.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 2 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-16-Week-4-Constrained-Least-Squares.html",
    "href": "posts/2025-02-16-Week-4-Constrained-Least-Squares.html",
    "title": "Week 4- Constrained Least Squares",
    "section": "",
    "text": "This week we will learn how to deal with linear equality constraints in least squares optimization problems. There will be a discussion of Lagrange multipliers, and application examples will include B-splines and portfolio optimization. Constraints are a critical part of both optimization problems and modeling in general.\nYour second homework assignment will be due Sunday February 23rd.\nHere are more details on what you need to do this week:\n\nGo through module 4 and complete the week 4 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nContinue working on Lab 2"
  },
  {
    "objectID": "posts/2025-02-02-Week-2-Least-Squares.html",
    "href": "posts/2025-02-02-Week-2-Least-Squares.html",
    "title": "Week 2- Least Squares",
    "section": "",
    "text": "This week we will talk about the most basic and most important class of optimization problems, least squares optimization. Whenever you fit a linear regression, you are solving a least squares problems, and least squares methods have applications across all quantitative fields. Although least squares methods are simple, they should not be dismissed- the fact that we can solve them rapidly, reliably means that it is useful to know how to formulate them in applied situations and how to solve them.\nIn this week we will go over the basics and a few simple applications, while the two weeks after this will explore data fitting examples and how to handle more complicated cases.\nYour first homework assignment will be due next Sunday.\nHere are more details on what you need to do this week:\n\nGo through module 2 and complete the week 2 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nContinue working on Lab 1"
  },
  {
    "objectID": "posts/2025-02-10-Data-Fitting.html",
    "href": "posts/2025-02-10-Data-Fitting.html",
    "title": "Week 3- Data Fitting",
    "section": "",
    "text": "This week, we will discuss applications of least squares methods to statistics and data fitting. This will include weighted least squares methods for linear regression, function interpolation, auto-regressive models, and feature engineering, and generalization/regularization.\nYour second homework assignment will be due Sunday February 23rd.\nHere are more details on what you need to do this week:\n\nGo through module 3 and complete the week 3 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 2"
  },
  {
    "objectID": "posts/2025-02-10-Meetup-3-Slides.html",
    "href": "posts/2025-02-10-Meetup-3-Slides.html",
    "title": "Meetup 3: Least Squares Data Fitting",
    "section": "",
    "text": "Meetup 3 is tonight at 6:45PM. We will tall about applications of least squares to problems in statistics and data fitting, including the use of weighted least squares for problems with heterogeneous or dependent errors, function interpolation and autoregressive time-series models, validation and regularization, ridge regression, and feature engineering.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 3 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-17-Meetup-4-Slides.html",
    "href": "posts/2025-02-17-Meetup-4-Slides.html",
    "title": "Meetup 4: Least Squares Data Fitting",
    "section": "",
    "text": "I have uploaded the slides for Meetup 4, which happened tonight at 6:45PM\nWe discussed constrained least squares and went through a detailed example that showed how to fit B-splines to data using that technique.\n\nClick here for slides\nClick here"
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: George Hagstrom, Ph.D. Class Meetup: Monday 7:00-8:00 Eastern Office Hours: By appointment Email: george.hagstrom@cuny.edu\n\nDescription\nOptimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.\nIn this course you will learn optimization theory through its practical applications to statistics and machine learning. The first half of the course will cover convex optimization (including least squares and linear programming as special cases), and the second half will cover methods for non-convex problems, primarily stochastic gradient descent and Markov-Chain Monte Carlo Methods. Applications will include large-scale linear and logistic regression, regularization, maximum likelihood methods, support vector machines, neural networks, Bayesian statistics, and optimization problems arising in an operations research or other business context.\n\n\nCourse Learning Outcomes\nBy the end of the course, students should be able to:\n\nUnderstand how to formulate major statistical and machine learning algorithms as optimization problems\nLearn how to recognize and solve least squares, linear programming, and convex optimization problems.\nLearn how to represent convex optimization problems in the CVX package\nUnderstand the basics of algorithmic complexity theory and use it to understand how quickly different algorithms will converge to the solution of an optimization problem\nImplement stochastic gradient descent for neural networks and other non-convex problems, understand trade-offs in algorithm design\n\n\n\nProgram Learning Outcomes\n\nBusiness Understanding. Apply frameworks and processes to build out data analytics solutions from understanding of business goals.\nSolid foundational data programming skills, using industry standard tools, essential algorithms, and design patterns for working with structured data, unstructured data and big data.\nSolid foundational math and statistics skills, with emphasis on linear algebra, probability, Bayesian statistics, and numerical methods.\nData understanding. Collect, describe, model, explore and verify data.\nData preparation. Selecting, cleaning, constructing, integrating, and formatting data.\nOptimization Modeling. Selecting optimization modeling techniques, generating test designs, building and assessing models.\nModel implementation and deployment.\nCommunicating results.\n\n\n\nGrading\n\nLabs (80%)\nProject (20%)\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. A homework assignment will be due every other week, see the schedule for details). There will also be a final project required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. I highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Schedule page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course.\n\n\nTextbooks and Course Materials\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\n\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Brightspace, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students’ personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "title: “DATA609 - Math Modeling Techniques” tbl-colwidths: [10,10,20,5,5,5,5,5,5,5] editor_options: chunk_output_type: console",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#meetup-link",
    "href": "course/schedule.html#meetup-link",
    "title": "DATA 609 Spring 2025",
    "section": "Meetup Link:",
    "text": "Meetup Link:\nClick Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\nDate\nStart Time\nModule\nSlides\nVideo\n\n\n\n\nJan 27\n06:45PM\nIntroduction to Optimization Theory\nMeetup 1 Slides\nMeetup 1 Video\n\n\nFeb 3\n06:45PM\nLeast Squares Optimization\nMeetup 2 Slides\nMeetup 2 Video\n\n\nFeb 10\n06:45PM\nLeast Squares and Statistics\nMeetup 3 Slides\nMeetup 3 Video\n\n\nFeb 17\n06:45PM\nConstrained Least Squares and Applications\nMeetup 4 Slides\nMeetup 4 Video\n\n\nFeb 24\n06:45PM\nConvex Sets\nMeetup 5 Slides\nMeetup 5 Video\n\n\nMar 3\n06:45PM\nConvex Sets II\nMeetup 6 Slides\nMeetup 6 Video\n\n\nMar 10\n06:45PM\nConvex Functions\n\n\n\n\nMar 17\n06:45PM\nConvex Functions II\n\n\n\n\nMar 24\n07:0PM\nConvex Optimization Problems\n\n\n\n\nMar 31\n06:45PM\nData Fitting\n\n\n\n\nApr 7\n06:45PM\nApplications to Statistics and Machine Learning\n\n\n\n\nApr 14\n06:45PM\nDuality and Classifiers\n\n\n\n\nApr 21\n\nNo Meetup (Spring Break)\n\n\n\n\nApr 28\n06:45PM\nGradient Descent and Neural Networks\nLab 7\n\n\n\nMay 5\n06:45PM\nStochastic Gradient Descent\n\n\n\n\nMay 12\n06:45PM\nTraining Deep Neural Networks",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA 609 - Math Modeling Techniques",
    "section": "",
    "text": "Optimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]