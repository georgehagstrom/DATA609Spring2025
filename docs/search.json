[
  {
    "objectID": "course/instructors.html",
    "href": "course/instructors.html",
    "title": "Instructor",
    "section": "",
    "text": "George Hagstrom, Ph.D.\nEmail: george.hagstrom@cuny.edu\nI am currently a Doctoral Lecturer in Data Science and Information Systems at the City University of New York. Additionally, I am a consultant with Princeton University where I development mathematical models of marine phytoplankton and heterotrophic bacteria to improve our understanding of the Earth’s Biogeochemical Cycles. Prior to joining CUNY, I was a Research Scientist at Princeton University in the Levin lab at the Department of Ecology and Evolutionary Biology and a postdoctoral researcher in the Magneto Fluids Division at the Courant Institute for Mathematical Sciences. My research interests include the application of Bayesian statistics and Machine Learning to incorporate nontraditional datasets (such as from ’omics) into mechanistic models of marine ecosystems and biogeochemical cycling, trait-based modeling, and critical transitions in complex ecological, social, or economic systems. In my free time, I enjoy cycling and exploring New York City.\n\nContact\nOffice Hours (Zoom is preferred): By appointment. You’re encouraged to schedule an appointment and I have time nearly everyday. You are also encouraged to ask us questions on Slack. If you wish to ask a question in private, you can email me directly.\nFor the most part, you can expect me to respond to questions by email within 24 hours. If you do not hear back from me within 48 hours of sending an email, please resend your message. I will be checking in on the course regularly, just about every day and likely several times each day. Please do not hesitate to ask if you have questions or concerns.",
    "crumbs": [
      "Course information",
      "Instructors"
    ]
  },
  {
    "objectID": "course/textbooks.html",
    "href": "course/textbooks.html",
    "title": "Textbooks",
    "section": "",
    "text": "Required\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\n\nThis book provides a strong practical introduction to Linear Algebra. We are primarily using the later parts of the book to learn about applications of least squares. However, you are encouraged to use earlier parts of the book to review linear algebra if needed.\n\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\n\nThis is an excellent book on practical applications of convex optimization. We will use the first half of the book, which focuses on identifying convex optimization problems.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\nThis is a good introduction to neural networks and deep learning.\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\n\nThis textbook provides a concise introduction to optimization for machine learning.\n\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\nThis is free textbook covers a large number of topics that are of relevance to data science, often from a slightly different perspective than standard treatments. It provides an excellent introduction to Bayesian Inference and Neural Networks, but applies ideas from information theory too.",
    "crumbs": [
      "Course information",
      "Textbooks"
    ]
  },
  {
    "objectID": "course/software.html",
    "href": "course/software.html",
    "title": "Software",
    "section": "",
    "text": "This class will involve a number of computational homework assignments. These can be completed in a variety of different programming languages. I recommend that you use python or Julia to complete the assignments, but it is also possible to use R.\nGuide to Base Languages:\n\nPython\n\nscipy.linalg is a useful package for linear algebra in python\nCVXPY is the implementation of the CVX package in python\npytorch is a package for neural networks in python\n\nJulia\n\nCONVEX.jl is the implementation of the CVX package in Julia\ntorch.jl provides torch functionality for deep learning in julia.\nflux.jl is another good option for deep learning.\n\nR\n\nCVXR is the implementation of the CVX package in R.\ntorch for R Can be used to implement neural networks in R",
    "crumbs": [
      "Course information",
      "Software"
    ]
  },
  {
    "objectID": "posts/2025-03-03-Week-6-Constructing-Convex-Sets.html",
    "href": "posts/2025-03-03-Week-6-Constructing-Convex-Sets.html",
    "title": "Week 6- Constructing Convex Sets",
    "section": "",
    "text": "During this week, we will learn how to construct more complex convex sets from the simple examples that we learned about in Week 5. We will use operations that preserve convexity, including intersection, cartesian product, and transformations by affine, perspective, or linear fractional functions.\nWe will also introduce the cvx software and the concept of disciplined convex programming.\nYour third homework assignment is due this Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 6 and complete the week 6 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 3"
  },
  {
    "objectID": "posts/2025-03-24-Week-9-Convex-Optimization-Problems.html",
    "href": "posts/2025-03-24-Week-9-Convex-Optimization-Problems.html",
    "title": "Week 9- Convex Optimization Problems",
    "section": "",
    "text": "Welcome to Week 9. During this week we will cover Chapter 4 of CVX-book, which introduces convex optimization problems. We have already been solving some of these problems, so we won’t go in depth into all the different types of convex problems like is done in the textbook. Instead we will cover log-concave functions, which are very important for statistical applications of convex optimization and was a topic that we did not have time for in the past two weeks, and then we will move on to discuss some optimization terminology that might be new, problem transformations and when they are useful, geometric programming, and multi-objective optimization.\nYour fifth homework assignment available and due in two weeks, Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 9 and complete the week 9 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 5"
  },
  {
    "objectID": "posts/2025-02-10-Meetup-3-Slides.html",
    "href": "posts/2025-02-10-Meetup-3-Slides.html",
    "title": "Meetup 3: Least Squares Data Fitting",
    "section": "",
    "text": "Meetup 3 is tonight at 6:45PM. We will tall about applications of least squares to problems in statistics and data fitting, including the use of weighted least squares for problems with heterogeneous or dependent errors, function interpolation and autoregressive time-series models, validation and regularization, ridge regression, and feature engineering.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 3 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-24-Meetup-5-Slides.html",
    "href": "posts/2025-02-24-Meetup-5-Slides.html",
    "title": "Meetup 5: Introduction to Convex Sets",
    "section": "",
    "text": "I have uploaded the slides for Meetup 5, which happened tonight at 6:45PM\nWe will learn about convex optimization and examples of convex sets, which are used as constraints in convex optimization problems.\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-03-31-Week-10-Data-Fitting.html",
    "href": "posts/2025-03-31-Week-10-Data-Fitting.html",
    "title": "Week 10- Data Fitting",
    "section": "",
    "text": "Welcome to Week 10. During this week we will cover Chapter 6 of CVX-book, which focuses on data fitting applications of convex optimization. We will talk about the meaning of different norms and penalty functions which can be imposed when trying to fit data. Three primary topics will be robustness to outliers, regularization and sparsity, and stochastic and worst case approximation, amongst others.\nYour fifth homework assignment is due next Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 9 and complete the week 10 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 5"
  },
  {
    "objectID": "posts/2025-04-28-Week-13-Optimization-Algorithms.html",
    "href": "posts/2025-04-28-Week-13-Optimization-Algorithms.html",
    "title": "Week 13- Optimization Algorithms",
    "section": "",
    "text": "Welcome to Week 13. During this week, we will turn our attention to non-convex problems and methods for solving them. In general, we can only guarantee convergence to a local minimum, but there are several algorithms that can find a very advantageous local minimum in the context of machine learning problems like training neural networks. The focus will be on gradient descent, and improvement which handles large condition numbers better called gradient descent with momentum, and adaptive algorithms which are often the default choice for training neural networks such as ADAM.\nYour final homework assignment is due May 18th at the latest.\nHere are more details on what you need to do this week:\n\nGo through module 11 and complete the week 13 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 7 (available soon)"
  },
  {
    "objectID": "posts/2025-03-10-Week-7.html",
    "href": "posts/2025-03-10-Week-7.html",
    "title": "Week 7- Convex Functions",
    "section": "",
    "text": "During this week, we will begin learning about convex functions, which will be the objective functions for convex optimization problems. We will go over the definition of convexity and present a number of different examples of convex functions. We will discuss what convexity means and why it is such a crucial property for optimization theory. We will talk about Jensen’s inequality and discuss some applications to risk in uncertain situations.\nYour fourth homework assignment is available this Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 7 and complete the week 7 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 4"
  },
  {
    "objectID": "posts/2025-02-24-Week-5-Convex-Sets.html",
    "href": "posts/2025-02-24-Week-5-Convex-Sets.html",
    "title": "Week 5-Convex Sets",
    "section": "",
    "text": "During this week we will begin learning the tools we need to formulate and solve convex optimization problems. We will talk more about general optimization problems and look at convexity from the perspective of constraints. We will present the definition of a convex set and give several example, and discuss how these examples arise when formulating problems.\nYour third homework assignment has been posted and is going to be due two Sundays from now.\nHere are more details on what you need to do this week:\n\nGo through module 5 and complete the week 4 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 3"
  },
  {
    "objectID": "posts/2025-04-01-Meetup-10-Data-Fitting.html",
    "href": "posts/2025-04-01-Meetup-10-Data-Fitting.html",
    "title": "Meetup 10: Data Fitting",
    "section": "",
    "text": "I have uploaded the slides and video for Meetup 10.\nWe discussed the meaning of different norms and penalty functions used in data fitting, talked about the trade-offs between different regularization schemes, and robust optimization in both the stochastic and worst case frameworks.\n\nClick here for slides\nClick here to watch the meetup video"
  },
  {
    "objectID": "posts/2025-04-28-Meetup-13.html",
    "href": "posts/2025-04-28-Meetup-13.html",
    "title": "Meetup 13: Non-Convex Optimization",
    "section": "",
    "text": "I have uploaded the slides for Meetup 13\nWe are going to learn about methods for solving non-convex optimization problems that are useful specifically for training neural networks.\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-03-10-Meetup-7-Slides.html",
    "href": "posts/2025-03-10-Meetup-7-Slides.html",
    "title": "Meetup 7: Introduction to Convex Sets",
    "section": "",
    "text": "I have uploaded the slides for Meetup 7.\nWe will learn about convex functions, including the definition, their importance for optimization, and simple examples. We will also talk about Jensen’s inequality and its applications to risk.\n\nClick here for slides\nWatch Meetup 7 Here"
  },
  {
    "objectID": "posts/2025-03-18-Week-8.html",
    "href": "posts/2025-03-18-Week-8.html",
    "title": "Week 8- Convex Functions",
    "section": "",
    "text": "During this week, we will learn techniques for proving that a function is convex that are based on constructing the function from simpler building blocks that guarantee convexity. These tools also form the basis of disciplined convex programming, allowing you to implement convex optimization in a modeling framework with guarantees that your objective function is convex.\nYour fourth homework assignment is due this Sunday at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 8 and complete the week 8 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 4"
  },
  {
    "objectID": "posts/2025-04-20-Week-12-Duality.html",
    "href": "posts/2025-04-20-Week-12-Duality.html",
    "title": "Week 12- Duality",
    "section": "",
    "text": "Welcome to Week 12. During this week we will cover sections 5.1, 5.2, 5.4.3, 5.4.4, 5.5.2, and 5.6, which are about a topic called duality. Duality is a technique that transforms constrained optimization problems into new forms by introducing Lagrange multipliers for their constraints. These dual optimization problems can provide lower bounds on non-convex optimization problems and can also provide equivalent formulations for convex optimization problems. However, the most useful aspect of duality in practice is that it enables a sensitivity analysis of the optimal value of the objective to the value of the constraints. Each constraint has a shadow price that comes from the solution to the dual problem which says how much the objective could be lowered if that constraint was weakened. We talk about duality in general and show several examples of how to use it to gain deeper knowledge about optimization problems.\nYour sixth homework assignment is due Sunday April 27th at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 11 and complete the week 12 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 6"
  },
  {
    "objectID": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "href": "posts/2025-02-05-Linear-Algebra-In-Python.html",
    "title": "Coding Vignette- Linear Algebra in Numpy",
    "section": "",
    "text": "I’m not sure how many of you have had experience with using python for linear algebra, so I decided to make a short video that goes over the basics of numpy (you are free to use R or Julia as well). I also solve a problem where I re-express the formula for the pseudoinverse of a matrix using the singular value decomposition of that matrix. I did it sort of to show how you might write out a long math expression in markdown, and to talk about matrix algebra and the properties of the SVD. This calculation is quite a bit more invovled than what I am asking on the homework, so don’t be intimidated.\nClick here to watch the video on youtube\nYou may also find the numpy documentation helpful: Click Here\nI have also included the ipython notebook"
  },
  {
    "objectID": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "href": "posts/2025-02-05-Writing-Math-In-Markdown-Vid.html",
    "title": "Coding Vignette- Writing Math in Markdown",
    "section": "",
    "text": "When working as a data scientist, and in particular, communicating your results and work in a technical document, you will need to be able to write mathematical expressions. One powerful tool for doing this is called LaTeX. LaTex is a its own programming language designed for rendering technical publications. It is commonly used in math and scientific fields, but a small subset of the language has been adopted in other environments including Microsoft Word and markdown, allowing you to render math using LaTeX commands. Knowing how to do this will be helpful for your homeworks, so I’ve decided to make a short video showing you the very basics of how this works.\nClick here to watch the video on youtube\nThere are a number of documents on the internet which explain the finer details of latex, and most of the lecture slides I shared use it, so you can see examples of how it works. I have also included the ipython notebook"
  },
  {
    "objectID": "modules/module1.html",
    "href": "modules/module1.html",
    "title": "Module 1 - Introduction to Optimization Problems",
    "section": "",
    "text": "Overview and Deliverables\n\nLearning Objectives\n\nYou will learn about how optimization problems are formalized and the terminology used to describe them\nUbiquity of Optimization problems in data science\nWe will define several important classes of optimization problems:\n\nUnconstrained versus Constrained\nConvex versus Non-Convex\nGlobal versus Local\n\nWhy optimization is hard\n\n\n\nReadings\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf you want to brush up on your linear algebra skills:\nEssential Linear Algebra for Machine Learning\nAppendix A1, A3-A5 of Convex Optimization\nThe mathematical level of those appendices are above what I expect from you in the course, I think the essential linear algebra notes are less intimidating.",
    "crumbs": [
      "Topics",
      "1 - Introduction to Optimization Theory "
    ]
  },
  {
    "objectID": "modules/module7.html",
    "href": "modules/module7.html",
    "title": "Module 7: Convex Functions",
    "section": "",
    "text": "Learning Objectives\n\nDefinitions of Convex Functions\nImportant Examples of Convex Functions\nJensen’s Inequality\n\n\n\nReadings\nSection 3.1 of Convex Optimization",
    "crumbs": [
      "Topics",
      "7 - Convex Functions"
    ]
  },
  {
    "objectID": "modules/module2.html",
    "href": "modules/module2.html",
    "title": "Module 2 - Introduction to Least Squares Problems",
    "section": "",
    "text": "Overview and Deliverables\nHomework 1 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving unconstrained least squares problems\n\nProperties of the Gram matrix and the pseudoinverse\nBasic Numerical Linear Algebra Considerations\n\nApplications:\n\nOptimization\nLinear Regression\n\n\n\n\nReadings\nChapter 12 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "2 - Least Squares Optimization"
    ]
  },
  {
    "objectID": "modules/module15.html",
    "href": "modules/module15.html",
    "title": "Module 15 - Deep Learning Applications",
    "section": "",
    "text": "Learning Objectives\n\nTraining Deep Neural Networks\n\n\n\nReadings\nGoodfellow Chapter 11 of the deeplearningbook",
    "crumbs": [
      "Topics",
      "15 - Training Deep Neural Networks"
    ]
  },
  {
    "objectID": "modules/module14.html",
    "href": "modules/module14.html",
    "title": "Module 14: Stochastic Gradient Descent",
    "section": "",
    "text": "Homework\nHomework 7 is due this Sunday at midnight\n\n\nLearning Objectives\n\nStochastic Gradient Descent\nHyperparameter Tuning\nSaddle Points and Convergence\n\n\n\nReadings\nSection 8.5 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 8 of deeplearningbook",
    "crumbs": [
      "Topics",
      "14 - Stochastic Gradient Descent"
    ]
  },
  {
    "objectID": "modules/module8.html",
    "href": "modules/module8.html",
    "title": "Module 8: Convex Functions II",
    "section": "",
    "text": "Learning Objectives\n\nRules for constructing convex functions\n\n\n\nReadings\nSection 3.2 of Convex Optimization",
    "crumbs": [
      "Topics",
      "8 - Convex Functions II"
    ]
  },
  {
    "objectID": "modules/module11.html",
    "href": "modules/module11.html",
    "title": "Module 11 - Convex Optimization Problems in Statistics",
    "section": "",
    "text": "Homework\nHomework 6 is due in three weeks, the week after spring break, Sunday at midnight\n\n\nLearning Objectives\n\nMaximum Likelihood and Maximum Entropy Methods\nMaximum Posterior Probability and Bayesian Inference\nClassification and Support Vector Machines\n\n\n\nReadings\nSections 7.1, 7.2, and 8.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "11 - Convex Optimization in Statistics"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible."
  },
  {
    "objectID": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "license.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n__Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\n\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\n\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Extending Linear Regression",
    "text": "Case Study: Extending Linear Regression\nWhat are the validity conditions for linear regression?\n\nLinear Relationship\nIndependent Residuals\nNormal Residuals\n?????"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression-1",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-extending-linear-regression-1",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Extending Linear Regression",
    "text": "Case Study: Extending Linear Regression\nWhat are the validity conditions for linear regression?\n\nLinear Relationship\nIndependent Residuals\nNormal Residuals\nHomoscedasticity i.e. constant variance"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#case-study-homoscedasticity",
    "href": "meetups/Meetup-3/meetup-3.html#case-study-homoscedasticity",
    "title": "DATA 609 Meetup 3",
    "section": "Case Study: Homoscedasticity",
    "text": "Case Study: Homoscedasticity"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#unequal-variance-common",
    "href": "meetups/Meetup-3/meetup-3.html#unequal-variance-common",
    "title": "DATA 609 Meetup 3",
    "section": "Unequal Variance Common",
    "text": "Unequal Variance Common\n\nTypical for Variance to be proportional to magnitude in some way\n\n\nCould try a data transformation, but isn’t satisfying\n\n\nHeterogeneous detectors\n\n\nHave sensor network with mix of types"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#maximum-likelihood",
    "href": "meetups/Meetup-3/meetup-3.html#maximum-likelihood",
    "title": "DATA 609 Meetup 3",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nConsider following statistical model for \\(y_i\\) in terms of measurements \\(\\mathbf{x}_i\\): \\[\ny_i \\sim \\mathrm{Normal}\\left(\\mathbf{x}_i^T\\mathbf{\\theta},\\sigma_i\\right)\n\\]\n\\(\\sigma_i^2\\) is variance of measurement of \\(y_i\\)\nLikelihood: \\[\n\\log p(\\mathbf{y}|X,\\theta) = \\log\\left(\\Pi_{i=1}^n \\frac{1}{2\\pi\\sigma_i}\\exp\\left(-\\frac{\\left(y_i-\\mathbf{x}_i^T\\mathbf{\\theta}\\right)^2}{2\\sigma_i^2}  \\right) \\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#maximum-likelihood-1",
    "href": "meetups/Meetup-3/meetup-3.html#maximum-likelihood-1",
    "title": "DATA 609 Meetup 3",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nConsider following statistical model for \\(y_i\\) in terms of measurements \\(\\mathbf{x}_i\\): \\[\ny_i \\sim \\mathrm{Normal}\\left(\\mathbf{x}_i^T\\mathbf{\\theta},\\sigma_i\\right)\n\\]\n\\(\\sigma_i^2\\) is variance of measurement of \\(y_i\\)\nLikelihood: \\[\n\\log p(\\mathbf{y}|X,\\theta) = -\\frac{n}{2} \\log(2\\pi) - \\sum_{i=1}^n\\log(\\sigma_i) -\n\\left\\|\\mathrm{diag}\\left(1/\\sigma_i\\right)\\left( X \\mathbf{\\theta} - \\mathbf{y}\\right)\\right\\|^2\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#weighted-least-squares",
    "href": "meetups/Meetup-3/meetup-3.html#weighted-least-squares",
    "title": "DATA 609 Meetup 3",
    "section": "Weighted Least Squares",
    "text": "Weighted Least Squares\n\nMaximum Likelihood Estimate for the Unequal Variance Problem is a Weighted Least Squares optimization:\n\n\\[\n\\min_{\\mathbf{\\theta}} \\|W^{1/2}(X\\mathbf{\\theta} - \\mathbf{y})\\|^2\n\\]\n\nHere the matrix \\(W\\) is a diagonal matrix with \\(\\frac{1}{\\sigma_i^2}\\) on the diagonal entries\nLarge \\(\\sigma_i\\) means error of that term is less important \\[\nX^TWX \\mathbf{\\theta} = X^TW\\mathbf{y}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#gauss-markov-theorem",
    "href": "meetups/Meetup-3/meetup-3.html#gauss-markov-theorem",
    "title": "DATA 609 Meetup 3",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\nCan extend this result to scenario when measurement errors have a given covariance matrix \\(\\Gamma\\)\nThen \\(W=\\Gamma^{-1}\\) and Weighted Least Squares provides maximum likelihood estimate\nBest Linear Uniased Estimator"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#when-is-wls-useful",
    "href": "meetups/Meetup-3/meetup-3.html#when-is-wls-useful",
    "title": "DATA 609 Meetup 3",
    "section": "When is WLS useful?",
    "text": "When is WLS useful?\n\nSample more of one group than another\nErrors vary based on detector or value\nCare about some predictions on some data more than others"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#weekly-summary",
    "href": "meetups/Meetup-3/meetup-3.html#weekly-summary",
    "title": "DATA 609 Meetup 3",
    "section": "Weekly Summary",
    "text": "Weekly Summary\n\nReading: Chapter 13 of VMLS\nHW 2 Available, due in 2 weeks\n\nProblem 1 and 2 based on this week\nProblem 3 next week\n\nWill release another numpy coding video, lmk if you prefer a different language"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#data-fitting",
    "href": "meetups/Meetup-3/meetup-3.html#data-fitting",
    "title": "DATA 609 Meetup 3",
    "section": "Data Fitting",
    "text": "Data Fitting\n\nHave some observations \\(y_i\\) and \\(\\mathbf{x}_i\\) and want to fit a regression\n\\(m\\) observations, have \\(n\\) basis functions \\(f\\) \\[\ny_i \\sim \\mathrm{Normal}\\left(\\sum_{j=1}^{n} \\theta_j f_j(\\mathbf{x}_i),\\sigma\\right)\n\\]\nNormal errors gives justification for least squares"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#data-fitting-solution",
    "href": "meetups/Meetup-3/meetup-3.html#data-fitting-solution",
    "title": "DATA 609 Meetup 3",
    "section": "Data Fitting Solution",
    "text": "Data Fitting Solution\nCan form matrix \\(A\\): \\[\nA = \\begin{bmatrix}\nf_1(\\mathbf{x}_1) & f_2(\\mathbf{x}_1) & \\cdots & f_n(\\mathbf{x}_1) \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nf_1(\\mathbf{x}_m) & f_2(\\mathbf{x}_m) & \\cdots & f_n(\\mathbf{x}_m)\n\\end{bmatrix}\n\\]\n\nAnd find: \\[\n\\min_{\\theta} \\|A\\mathbf{\\theta} - \\mathbf{y}\\|^2\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series",
    "href": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Autoregressive Time Series",
    "text": "Example: Autoregressive Time Series\n\nSuppose we have observations \\(y_t\\) and want to predict new \\(y\\) based on previous observations \\[\ny_t = \\theta_1 y_{t-1} + \\theta_2 y_{t-2}+\\cdots+\\theta_n y_{t-n}\n\\]\nHow to write this in our framework?"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series-1",
    "href": "meetups/Meetup-3/meetup-3.html#example-autoregressive-time-series-1",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Autoregressive Time Series",
    "text": "Example: Autoregressive Time Series\n\nSuppose we have observations \\(y_t\\) and want to predict new \\(y\\) based on previous observations \\[\ny_t = \\theta_1 y_{t-1} + \\theta_2 y_{t-2}+\\cdots+\\theta_n y_{t-n}\n\\]\nHow to write this in our framework?\nDefine \\(\\mathbf{x}_i = \\begin{bmatrix} y_{t-1} & y_{t-2} & \\cdots & y_{t-n} \\end{bmatrix}\\)"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-central-park-temperature",
    "href": "meetups/Meetup-3/meetup-3.html#example-central-park-temperature",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Central Park Temperature",
    "text": "Example: Central Park Temperature\n\nHourly Central Park Temperature from Jan-March 2023"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#how-to-predict",
    "href": "meetups/Meetup-3/meetup-3.html#how-to-predict",
    "title": "DATA 609 Meetup 3",
    "section": "How to Predict",
    "text": "How to Predict\n\nBaseline Mean and Variance:\n\n\\({T}_{\\mathrm{mean}} = 42.5\\), \\(\\sigma_T = 8.57\\)\n\nGuess Temperature from 24 hours before\n\n\\(\\sigma_{lag} = 1.41\\)\n\nHow about using the past 8 hours?: \\[\nT_t = \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#setting-up-the-model",
    "href": "meetups/Meetup-3/meetup-3.html#setting-up-the-model",
    "title": "DATA 609 Meetup 3",
    "section": "Setting up the model",
    "text": "Setting up the model\n\nA = np.array([central_park[\"temp\"].values[i:i+8].T \nfor i in  range(len(central_park)-8)])\nx = central_park[\"temp\"].values[8:]\ntheta = np.linalg.lstsq(A,x)[0]\n\n\nSlight Improvement \\(\\sigma = 1.25\\)"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#predictions",
    "href": "meetups/Meetup-3/meetup-3.html#predictions",
    "title": "DATA 609 Meetup 3",
    "section": "Predictions",
    "text": "Predictions\n\n\nText(0, 0.5, 'Temperature')"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#more-time-series",
    "href": "meetups/Meetup-3/meetup-3.html#more-time-series",
    "title": "DATA 609 Meetup 3",
    "section": "More Time Series",
    "text": "More Time Series\n\nCan incorporate trends: \\[\nT_t = \\theta_0(i-i_0) + \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]\nOther variables or functions \\[\nT_t = \\theta_{m}\\mathrm{Month}(i) + \\sum_{i=1}^8 \\theta_i T_{t-i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#generalization-and-validation",
    "href": "meetups/Meetup-3/meetup-3.html#generalization-and-validation",
    "title": "DATA 609 Meetup 3",
    "section": "Generalization and Validation",
    "text": "Generalization and Validation\n\nGoal usually isn’t to find model with absolute lowest RMS error\nInstead want a model that predicts well on new data\nTwo goals often in conflict"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#example-polynomial-interpolation",
    "href": "meetups/Meetup-3/meetup-3.html#example-polynomial-interpolation",
    "title": "DATA 609 Meetup 3",
    "section": "Example: Polynomial Interpolation",
    "text": "Example: Polynomial Interpolation\n\nFitting high degree polynomials famously leads to overfitting \\[\ny_i = \\sum_{j=1}^n \\theta_j x^{j-1}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#fit-results",
    "href": "meetups/Meetup-3/meetup-3.html#fit-results",
    "title": "DATA 609 Meetup 3",
    "section": "Fit Results",
    "text": "Fit Results\n\nCurve develops unnecessary features at higher order"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#generalization-error",
    "href": "meetups/Meetup-3/meetup-3.html#generalization-error",
    "title": "DATA 609 Meetup 3",
    "section": "Generalization Error",
    "text": "Generalization Error\n\nAt high order, generalization fails"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#regularization",
    "href": "meetups/Meetup-3/meetup-3.html#regularization",
    "title": "DATA 609 Meetup 3",
    "section": "Regularization",
    "text": "Regularization\n\nAnother way to handle overfitting is with regularization\nAdd term proportional to \\(\\|\\mathbf{\\theta}\\|^2\\) \\[\n\\min_{\\mathbf{\\theta}} \\|A\\mathbf{\\theta}-\\mathbf{y}\\|^2 + \\lambda\\|\\mathbf{\\theta}\\|^2\n\\]\nUse if \\(A\\) is ill-conditioned or many parameters."
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nName comes from geometry\nConsider objective function with a “ridge”"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression-1",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression-1",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nName comes from geometry\nRegularizer turns ridge to peak"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#bayesian-linear-regression",
    "href": "meetups/Meetup-3/meetup-3.html#bayesian-linear-regression",
    "title": "DATA 609 Meetup 3",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\n\\[\n\\mathbf{y} \\sim \\mathrm{Normal}\\left(X\\theta,\\sigma^2 I\\right) \\\\\n\\mathbf{\\theta} \\sim \\mathrm{Normal}\\left(0, \\frac{1}{\\lambda} I\\right)\n\\]\n\n\\(\\frac{1}{\\lambda}\\) is variance of prior distribution on \\(\\theta\\)\n\\(p(\\theta|X)\\) is normal\n\\(E(\\theta|X)\\) is least squares ridge regression solution"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#ridge-regression-solution",
    "href": "meetups/Meetup-3/meetup-3.html#ridge-regression-solution",
    "title": "DATA 609 Meetup 3",
    "section": "Ridge Regression Solution",
    "text": "Ridge Regression Solution\n\nEquivalent to: \\[\n\\left(A^TA + \\lambda I\\right)\\mathbf{\\theta} = A^T\\mathbf{y}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#feature-engineering",
    "href": "meetups/Meetup-3/meetup-3.html#feature-engineering",
    "title": "DATA 609 Meetup 3",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nCan use domain knowledge to create custom features in your data\nNew feature vectors: \\(f_1(\\mathbf{x})\\), \\(f_2(\\mathbf{x}), \\cdots\\)\nStandard data transforms, log, z-score\nThresholding: \\(f(x) = \\max(0,x)\\)\nDomain Knowledge: \\[ \\mathrm{BMI} =\\frac{\\left(\\mathrm{Weight}\\right)}{\\left(\\mathrm{Height}\\right)^2}\\]"
  },
  {
    "objectID": "meetups/Meetup-3/meetup-3.html#thanks",
    "href": "meetups/Meetup-3/meetup-3.html#thanks",
    "title": "DATA 609 Meetup 3",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST.html",
    "href": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\n\"\"\"\nnetwork.py\n~~~~~~~~~~\n\nA module to implement the stochastic gradient descent learning\nalgorithm for a feedforward neural network.  Gradients are calculated\nusing backpropagation.  Note that I have focused on making the code\nsimple, easily readable, and easily modifiable.  It is not optimized,\nand omits many desirable features.\n\"\"\"\n\n#### Libraries\n# Standard library\nimport random\n\n# Third-party libraries\nimport numpy as np\n\nclass Network(object):\n\n    def __init__(self, sizes):\n        \"\"\"The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won't set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.\"\"\"\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y, x)\n                        for x, y in zip(sizes[:-1], sizes[1:])]\n\n    def feedforward(self, a):\n        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b)\n        return(a)\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            test_data=None):\n        \"\"\"Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.\"\"\"\n        if test_data: n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(\"Epoch {0}: {1} / {2}\".format(\n                    j, self.evaluate(test_data), n_test))\n            else:\n                print(\"Epoch {0} complete\".format(j))\n\n    def update_mini_batch(self, mini_batch, eta):\n        \"\"\"Update the network's weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.\"\"\"\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * \\\n            sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It's a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return((nabla_b, nabla_w))\n\n    def evaluate(self, test_data):\n        \"\"\"Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network's output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.\"\"\"\n        test_results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in test_data]\n        return(sum(int(x == y) for (x, y) in test_results))\n\n    def cost_derivative(self, output_activations, y):\n        \"\"\"Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.\"\"\"\n        return(output_activations-y)\n\n#### Miscellaneous functions\ndef sigmoid(z):\n    \"\"\"The sigmoid function.\"\"\"\n    return(1.0/(1.0+np.exp(-z)))\n\ndef sigmoid_prime(z):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return(sigmoid(z)*(1-sigmoid(z)))\n\n\nnet = Network([1,2,3])\n\n\nnet.biases\n\n[array([[ 2.38124015],\n        [-0.50238049]]),\n array([[ 0.69887341],\n        [-1.38724672],\n        [ 0.63378934]])]\n\n\n\n\n\n\nInit signature: zip(self, /, *args, **kwargs)\nDocstring:     \nzip(*iterables, strict=False) --&gt; Yield tuples until an input is exhausted.\n   &gt;&gt;&gt; list(zip('abcdefg', range(3), range(4)))\n   [('a', 0, 0), ('b', 1, 1), ('c', 2, 2)]\nThe zip object yields n-length tuples, where n is the number of iterables\npassed as positional arguments to zip().  The i-th element in every tuple\ncomes from the i-th iterable argument to zip().  This continues until the\nshortest argument is exhausted.\nIf strict is true and one of the arguments is exhausted before the others,\nraise a ValueError.\nType:           type\nSubclasses:     \n\n\n\n\n\"\"\"\nmnist_loader\n~~~~~~~~~~~~\n\nA library to load the MNIST image data.  For details of the data\nstructures that are returned, see the doc strings for ``load_data``\nand ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\nfunction usually called by our neural network code.\n\"\"\"\n\n#### Libraries\n# Standard library\nimport pickle\nimport gzip\n\n# Third-party libraries\nimport numpy as np\n\ndef load_data():\n    \"\"\"Return the MNIST data as a tuple containing the training data,\n    the validation data, and the test data.\n\n    The ``training_data`` is returned as a tuple with two entries.\n    The first entry contains the actual training images.  This is a\n    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n    numpy ndarray with 784 values, representing the 28 * 28 = 784\n    pixels in a single MNIST image.\n\n    The second entry in the ``training_data`` tuple is a numpy ndarray\n    containing 50,000 entries.  Those entries are just the digit\n    values (0...9) for the corresponding images contained in the first\n    entry of the tuple.\n\n    The ``validation_data`` and ``test_data`` are similar, except\n    each contains only 10,000 images.\n\n    This is a nice data format, but for use in neural networks it's\n    helpful to modify the format of the ``training_data`` a little.\n    That's done in the wrapper function ``load_data_wrapper()``, see\n    below.\n    \"\"\"\n \n    with open('data/mnist.pkl', 'rb') as f:  # Note the 'rb' (read binary)\n        training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n    f.close()\n    return (training_data, validation_data, test_data)\n\n\n\nnet = Network([784, 30, 10])\n\n\ntraining_data = list(zip(a[0][0].reshape((-1, 1)),a[0][1]))\ntest_data = list(zip(a[2][0].reshape((-1, 1),a[2][1]))\nvalidation_data = list(zip(a[1][0].reshape((-1, 1),a[1][1]))\n\n\n  Cell In[53], line 2\n    test_data = list(zip(a[2][0].reshape((-1, 1),a[2][1]))\n                     ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\n\n\n\n\n\n\n\n(50000,)\n\n\n\nnet.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[47], line 1\n----&gt; 1 net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n\nCell In[38], line 62, in Network.SGD(self, training_data, epochs, mini_batch_size, eta, test_data)\n     58 mini_batches = [\n     59     training_data[k:k+mini_batch_size]\n     60     for k in range(0, n, mini_batch_size)]\n     61 for mini_batch in mini_batches:\n---&gt; 62     self.update_mini_batch(mini_batch, eta)\n     63 if test_data:\n     64     print(\"Epoch {0}: {1} / {2}\".format(\n     65         j, self.evaluate(test_data), n_test))\n\nCell In[38], line 77, in Network.update_mini_batch(self, mini_batch, eta)\n     75 nabla_w = [np.zeros(w.shape) for w in self.weights]\n     76 for x, y in mini_batch:\n---&gt; 77     delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n     78     nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n     79     nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n\nCell In[38], line 117, in Network.backprop(self, x, y)\n    115     delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n    116     nabla_b[-l] = delta\n--&gt; 117     nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n    118 return((nabla_b, nabla_w))\n\nValueError: shapes (30,30) and (784,) not aligned: 30 (dim 1) != 784 (dim 0)\n\n\n\n\nf = gzip.open('data/mnist.pkl.gz', 'rb')\n\n\npickle.load(f)\n\n\n---------------------------------------------------------------------------\nUnicodeDecodeError                        Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 pickle.load(f)\n\nUnicodeDecodeError: 'ascii' codec can't decode byte 0x90 in position 614: ordinal not in range(128)\n\n\n\n\n\nwith open('data/mnist.pkl', 'rb') as f:  # Note the 'rb' (read binary)\n    mnist_data = pickle.load(f, encoding='latin1')\n\n\nmnist_data\n\n((array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([5, 0, 4, ..., 8, 4, 8])),\n (array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([3, 8, 6, ..., 5, 6, 8])),\n (array([[0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.],\n         [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n  array([7, 2, 1, ..., 4, 5, 6])))\n\n\n\ntraining_data[1]\n\narray([5, 0, 4, ..., 8, 4, 8])"
  },
  {
    "objectID": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Deep.html",
    "href": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Deep.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                       transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                       transform=transform)\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n\n\nif __name__ == '__main__':\n    main()\n\nusage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N]\n                             [--epochs N] [--lr LR] [--gamma M] [--no-cuda]\n                             [--no-mps] [--dry-run] [--seed S]\n                             [--log-interval N] [--save-model]\nipykernel_launcher.py: error: unrecognized arguments: -f /home/georgehagstrom/.local/share/jupyter/runtime/kernel-a713f3e2-434c-4a6e-82e9-3b680d4b29a4.json\n\n\n\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n\n\n\n\n/home/georgehagstrom/miniforge3/envs/stan/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n\n\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=10, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                        help='number of epochs to train (default: 2)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\n    parser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args(args=[])\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {'batch_size': args.batch_size}\n    test_kwargs = {'batch_size': args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                       transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                       transform=transform)\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n    if args.save_model:\n        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n        \n\n\n\nif __name__ == '__main__':\n    main()\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:00&lt;00:00, 52.0MB/s]\n\n\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00&lt;00:00, 2.25MB/s]\n\n\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n\n\n\n\n\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00&lt;00:00, 18.3MB/s]\n\n\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00&lt;00:00, 2.92MB/s]\n\n\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.305400\nTrain Epoch: 1 [640/60000 (1%)] Loss: 1.359781\nTrain Epoch: 1 [1280/60000 (2%)]    Loss: 0.830670\nTrain Epoch: 1 [1920/60000 (3%)]    Loss: 0.605967\nTrain Epoch: 1 [2560/60000 (4%)]    Loss: 0.346150\nTrain Epoch: 1 [3200/60000 (5%)]    Loss: 0.449750\nTrain Epoch: 1 [3840/60000 (6%)]    Loss: 0.298424\nTrain Epoch: 1 [4480/60000 (7%)]    Loss: 0.280805\nTrain Epoch: 1 [5120/60000 (9%)]    Loss: 0.564294\nTrain Epoch: 1 [5760/60000 (10%)]   Loss: 0.212350\nTrain Epoch: 1 [6400/60000 (11%)]   Loss: 0.268308\nTrain Epoch: 1 [7040/60000 (12%)]   Loss: 0.328092\nTrain Epoch: 1 [7680/60000 (13%)]   Loss: 0.177161\nTrain Epoch: 1 [8320/60000 (14%)]   Loss: 0.218695\nTrain Epoch: 1 [8960/60000 (15%)]   Loss: 0.295118\nTrain Epoch: 1 [9600/60000 (16%)]   Loss: 0.106185\nTrain Epoch: 1 [10240/60000 (17%)]  Loss: 0.280247\nTrain Epoch: 1 [10880/60000 (18%)]  Loss: 0.104401\nTrain Epoch: 1 [11520/60000 (19%)]  Loss: 0.483864\nTrain Epoch: 1 [12160/60000 (20%)]  Loss: 0.256220\nTrain Epoch: 1 [12800/60000 (21%)]  Loss: 0.254172\nTrain Epoch: 1 [13440/60000 (22%)]  Loss: 0.206407\nTrain Epoch: 1 [14080/60000 (23%)]  Loss: 0.171163\nTrain Epoch: 1 [14720/60000 (25%)]  Loss: 0.433929\nTrain Epoch: 1 [15360/60000 (26%)]  Loss: 0.172169\nTrain Epoch: 1 [16000/60000 (27%)]  Loss: 0.124138\nTrain Epoch: 1 [16640/60000 (28%)]  Loss: 0.168004\nTrain Epoch: 1 [17280/60000 (29%)]  Loss: 0.060927\nTrain Epoch: 1 [17920/60000 (30%)]  Loss: 0.185982\nTrain Epoch: 1 [18560/60000 (31%)]  Loss: 0.182676\nTrain Epoch: 1 [19200/60000 (32%)]  Loss: 0.277119\nTrain Epoch: 1 [19840/60000 (33%)]  Loss: 0.088108\nTrain Epoch: 1 [20480/60000 (34%)]  Loss: 0.045712\nTrain Epoch: 1 [21120/60000 (35%)]  Loss: 0.241692\nTrain Epoch: 1 [21760/60000 (36%)]  Loss: 0.008685\nTrain Epoch: 1 [22400/60000 (37%)]  Loss: 0.065719\nTrain Epoch: 1 [23040/60000 (38%)]  Loss: 0.222506\nTrain Epoch: 1 [23680/60000 (39%)]  Loss: 0.149206\nTrain Epoch: 1 [24320/60000 (41%)]  Loss: 0.017020\nTrain Epoch: 1 [24960/60000 (42%)]  Loss: 0.145989\nTrain Epoch: 1 [25600/60000 (43%)]  Loss: 0.093393\nTrain Epoch: 1 [26240/60000 (44%)]  Loss: 0.081233\nTrain Epoch: 1 [26880/60000 (45%)]  Loss: 0.339266\nTrain Epoch: 1 [27520/60000 (46%)]  Loss: 0.242246\nTrain Epoch: 1 [28160/60000 (47%)]  Loss: 0.143740\nTrain Epoch: 1 [28800/60000 (48%)]  Loss: 0.110527\nTrain Epoch: 1 [29440/60000 (49%)]  Loss: 0.041162\nTrain Epoch: 1 [30080/60000 (50%)]  Loss: 0.185229\nTrain Epoch: 1 [30720/60000 (51%)]  Loss: 0.050593\nTrain Epoch: 1 [31360/60000 (52%)]  Loss: 0.124320\nTrain Epoch: 1 [32000/60000 (53%)]  Loss: 0.185447\nTrain Epoch: 1 [32640/60000 (54%)]  Loss: 0.141373\nTrain Epoch: 1 [33280/60000 (55%)]  Loss: 0.072402\nTrain Epoch: 1 [33920/60000 (57%)]  Loss: 0.021166\nTrain Epoch: 1 [34560/60000 (58%)]  Loss: 0.018177\nTrain Epoch: 1 [35200/60000 (59%)]  Loss: 0.237060\nTrain Epoch: 1 [35840/60000 (60%)]  Loss: 0.182967\nTrain Epoch: 1 [36480/60000 (61%)]  Loss: 0.052063\nTrain Epoch: 1 [37120/60000 (62%)]  Loss: 0.114045\nTrain Epoch: 1 [37760/60000 (63%)]  Loss: 0.214742\nTrain Epoch: 1 [38400/60000 (64%)]  Loss: 0.122051\nTrain Epoch: 1 [39040/60000 (65%)]  Loss: 0.042043\nTrain Epoch: 1 [39680/60000 (66%)]  Loss: 0.024637\nTrain Epoch: 1 [40320/60000 (67%)]  Loss: 0.076457\nTrain Epoch: 1 [40960/60000 (68%)]  Loss: 0.100259\nTrain Epoch: 1 [41600/60000 (69%)]  Loss: 0.116372\nTrain Epoch: 1 [42240/60000 (70%)]  Loss: 0.069757\nTrain Epoch: 1 [42880/60000 (71%)]  Loss: 0.060663\nTrain Epoch: 1 [43520/60000 (72%)]  Loss: 0.209611\nTrain Epoch: 1 [44160/60000 (74%)]  Loss: 0.068317\nTrain Epoch: 1 [44800/60000 (75%)]  Loss: 0.128117\nTrain Epoch: 1 [45440/60000 (76%)]  Loss: 0.209076\nTrain Epoch: 1 [46080/60000 (77%)]  Loss: 0.103895\nTrain Epoch: 1 [46720/60000 (78%)]  Loss: 0.169364\nTrain Epoch: 1 [47360/60000 (79%)]  Loss: 0.129173\nTrain Epoch: 1 [48000/60000 (80%)]  Loss: 0.063871\nTrain Epoch: 1 [48640/60000 (81%)]  Loss: 0.017477\nTrain Epoch: 1 [49280/60000 (82%)]  Loss: 0.055380\nTrain Epoch: 1 [49920/60000 (83%)]  Loss: 0.079302\nTrain Epoch: 1 [50560/60000 (84%)]  Loss: 0.067277\nTrain Epoch: 1 [51200/60000 (85%)]  Loss: 0.282996\nTrain Epoch: 1 [51840/60000 (86%)]  Loss: 0.018431\nTrain Epoch: 1 [52480/60000 (87%)]  Loss: 0.022761\nTrain Epoch: 1 [53120/60000 (88%)]  Loss: 0.118848\nTrain Epoch: 1 [53760/60000 (90%)]  Loss: 0.065848\nTrain Epoch: 1 [54400/60000 (91%)]  Loss: 0.084057\nTrain Epoch: 1 [55040/60000 (92%)]  Loss: 0.043141\nTrain Epoch: 1 [55680/60000 (93%)]  Loss: 0.076629\nTrain Epoch: 1 [56320/60000 (94%)]  Loss: 0.114205\nTrain Epoch: 1 [56960/60000 (95%)]  Loss: 0.097859\nTrain Epoch: 1 [57600/60000 (96%)]  Loss: 0.123413\nTrain Epoch: 1 [58240/60000 (97%)]  Loss: 0.008551\nTrain Epoch: 1 [58880/60000 (98%)]  Loss: 0.024555\nTrain Epoch: 1 [59520/60000 (99%)]  Loss: 0.001746\n\nTest set: Average loss: 0.0467, Accuracy: 9851/10000 (99%)\n\nTrain Epoch: 2 [0/60000 (0%)]   Loss: 0.133165\nTrain Epoch: 2 [640/60000 (1%)] Loss: 0.035212\nTrain Epoch: 2 [1280/60000 (2%)]    Loss: 0.081603\nTrain Epoch: 2 [1920/60000 (3%)]    Loss: 0.174264\nTrain Epoch: 2 [2560/60000 (4%)]    Loss: 0.084172\nTrain Epoch: 2 [3200/60000 (5%)]    Loss: 0.045414\nTrain Epoch: 2 [3840/60000 (6%)]    Loss: 0.009123\nTrain Epoch: 2 [4480/60000 (7%)]    Loss: 0.090692\nTrain Epoch: 2 [5120/60000 (9%)]    Loss: 0.119748\nTrain Epoch: 2 [5760/60000 (10%)]   Loss: 0.096671\nTrain Epoch: 2 [6400/60000 (11%)]   Loss: 0.231434\nTrain Epoch: 2 [7040/60000 (12%)]   Loss: 0.151781\nTrain Epoch: 2 [7680/60000 (13%)]   Loss: 0.108274\nTrain Epoch: 2 [8320/60000 (14%)]   Loss: 0.033716\nTrain Epoch: 2 [8960/60000 (15%)]   Loss: 0.119623\nTrain Epoch: 2 [9600/60000 (16%)]   Loss: 0.039153\nTrain Epoch: 2 [10240/60000 (17%)]  Loss: 0.130411\nTrain Epoch: 2 [10880/60000 (18%)]  Loss: 0.060051\nTrain Epoch: 2 [11520/60000 (19%)]  Loss: 0.106906\nTrain Epoch: 2 [12160/60000 (20%)]  Loss: 0.060178\nTrain Epoch: 2 [12800/60000 (21%)]  Loss: 0.061571\nTrain Epoch: 2 [13440/60000 (22%)]  Loss: 0.007195\nTrain Epoch: 2 [14080/60000 (23%)]  Loss: 0.013582\nTrain Epoch: 2 [14720/60000 (25%)]  Loss: 0.106854\nTrain Epoch: 2 [15360/60000 (26%)]  Loss: 0.086269\nTrain Epoch: 2 [16000/60000 (27%)]  Loss: 0.131550\nTrain Epoch: 2 [16640/60000 (28%)]  Loss: 0.082255\nTrain Epoch: 2 [17280/60000 (29%)]  Loss: 0.007253\nTrain Epoch: 2 [17920/60000 (30%)]  Loss: 0.067956\nTrain Epoch: 2 [18560/60000 (31%)]  Loss: 0.127102\nTrain Epoch: 2 [19200/60000 (32%)]  Loss: 0.085078\nTrain Epoch: 2 [19840/60000 (33%)]  Loss: 0.138302\nTrain Epoch: 2 [20480/60000 (34%)]  Loss: 0.022628\nTrain Epoch: 2 [21120/60000 (35%)]  Loss: 0.093657\nTrain Epoch: 2 [21760/60000 (36%)]  Loss: 0.003313\nTrain Epoch: 2 [22400/60000 (37%)]  Loss: 0.005381\nTrain Epoch: 2 [23040/60000 (38%)]  Loss: 0.064144\nTrain Epoch: 2 [23680/60000 (39%)]  Loss: 0.057510\nTrain Epoch: 2 [24320/60000 (41%)]  Loss: 0.000885\nTrain Epoch: 2 [24960/60000 (42%)]  Loss: 0.009463\nTrain Epoch: 2 [25600/60000 (43%)]  Loss: 0.060560\nTrain Epoch: 2 [26240/60000 (44%)]  Loss: 0.026845\nTrain Epoch: 2 [26880/60000 (45%)]  Loss: 0.189767\nTrain Epoch: 2 [27520/60000 (46%)]  Loss: 0.028179\nTrain Epoch: 2 [28160/60000 (47%)]  Loss: 0.113922\nTrain Epoch: 2 [28800/60000 (48%)]  Loss: 0.005650\nTrain Epoch: 2 [29440/60000 (49%)]  Loss: 0.079232\nTrain Epoch: 2 [30080/60000 (50%)]  Loss: 0.037819\nTrain Epoch: 2 [30720/60000 (51%)]  Loss: 0.070176\nTrain Epoch: 2 [31360/60000 (52%)]  Loss: 0.104161\nTrain Epoch: 2 [32000/60000 (53%)]  Loss: 0.157919\nTrain Epoch: 2 [32640/60000 (54%)]  Loss: 0.089258\nTrain Epoch: 2 [33280/60000 (55%)]  Loss: 0.030640\nTrain Epoch: 2 [33920/60000 (57%)]  Loss: 0.007256\nTrain Epoch: 2 [34560/60000 (58%)]  Loss: 0.034558\nTrain Epoch: 2 [35200/60000 (59%)]  Loss: 0.070959\nTrain Epoch: 2 [35840/60000 (60%)]  Loss: 0.047352\nTrain Epoch: 2 [36480/60000 (61%)]  Loss: 0.023012\nTrain Epoch: 2 [37120/60000 (62%)]  Loss: 0.032101\nTrain Epoch: 2 [37760/60000 (63%)]  Loss: 0.056723\nTrain Epoch: 2 [38400/60000 (64%)]  Loss: 0.085559\nTrain Epoch: 2 [39040/60000 (65%)]  Loss: 0.001664\nTrain Epoch: 2 [39680/60000 (66%)]  Loss: 0.019043\nTrain Epoch: 2 [40320/60000 (67%)]  Loss: 0.068308\nTrain Epoch: 2 [40960/60000 (68%)]  Loss: 0.040082\nTrain Epoch: 2 [41600/60000 (69%)]  Loss: 0.054168\nTrain Epoch: 2 [42240/60000 (70%)]  Loss: 0.023815\nTrain Epoch: 2 [42880/60000 (71%)]  Loss: 0.077013\nTrain Epoch: 2 [43520/60000 (72%)]  Loss: 0.062710\nTrain Epoch: 2 [44160/60000 (74%)]  Loss: 0.002347\nTrain Epoch: 2 [44800/60000 (75%)]  Loss: 0.072447\nTrain Epoch: 2 [45440/60000 (76%)]  Loss: 0.095005\nTrain Epoch: 2 [46080/60000 (77%)]  Loss: 0.070857\nTrain Epoch: 2 [46720/60000 (78%)]  Loss: 0.066957\nTrain Epoch: 2 [47360/60000 (79%)]  Loss: 0.077697\nTrain Epoch: 2 [48000/60000 (80%)]  Loss: 0.082366\nTrain Epoch: 2 [48640/60000 (81%)]  Loss: 0.018147\nTrain Epoch: 2 [49280/60000 (82%)]  Loss: 0.010403\nTrain Epoch: 2 [49920/60000 (83%)]  Loss: 0.047980\nTrain Epoch: 2 [50560/60000 (84%)]  Loss: 0.058689\nTrain Epoch: 2 [51200/60000 (85%)]  Loss: 0.099832\nTrain Epoch: 2 [51840/60000 (86%)]  Loss: 0.047717\nTrain Epoch: 2 [52480/60000 (87%)]  Loss: 0.005454\nTrain Epoch: 2 [53120/60000 (88%)]  Loss: 0.050576\nTrain Epoch: 2 [53760/60000 (90%)]  Loss: 0.099193\nTrain Epoch: 2 [54400/60000 (91%)]  Loss: 0.025740\nTrain Epoch: 2 [55040/60000 (92%)]  Loss: 0.037621\nTrain Epoch: 2 [55680/60000 (93%)]  Loss: 0.064025\nTrain Epoch: 2 [56320/60000 (94%)]  Loss: 0.053236\nTrain Epoch: 2 [56960/60000 (95%)]  Loss: 0.016877\nTrain Epoch: 2 [57600/60000 (96%)]  Loss: 0.031278\nTrain Epoch: 2 [58240/60000 (97%)]  Loss: 0.014690\nTrain Epoch: 2 [58880/60000 (98%)]  Loss: 0.052964\nTrain Epoch: 2 [59520/60000 (99%)]  Loss: 0.004221\n\nTest set: Average loss: 0.0378, Accuracy: 9874/10000 (99%)\n\n\n\n\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=14, metavar='N',\n                        help='number of epochs to train (default: 14)')\nparser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\nparser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\nparser.add_argument('--no-cuda', action='store_true', default=True,\n                        help='disables CUDA training')\nparser.add_argument('--no-mps', action='store_true', default=False,\n                        help='disables macOS GPU training')\nparser.add_argument('--dry-run', action='store_true', default=False,\n                        help='quickly check a single pass')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\nparser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n\n\nargs = parser.parse_args()\n\n_StoreTrueAction(option_strings=['--save-model'], dest='save_model', nargs=0, const=True, default=False, type=None, choices=None, required=False, help='For Saving the current Model', metavar=None)\n\n\n\nargs = parser.parse_args()\n\nusage: ipykernel_launcher.py [-h] [--batch-size N] [--test-batch-size N]\n                             [--epochs N] [--lr LR] [--gamma M] [--no-cuda]\n                             [--no-mps] [--dry-run] [--seed S]\n                             [--log-interval N] [--save-model]\nipykernel_launcher.py: error: unrecognized arguments: -f /home/georgehagstrom/.local/share/jupyter/runtime/kernel-a713f3e2-434c-4a6e-82e9-3b680d4b29a4.json\n\n\n\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n\n\n\n\n\nimport pickle\nwith open('data/mnist.pkl', 'rb') as f:  # Note the 'rb' (read binary)\n    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\nf.close()\n\n\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\ndataset1 = datasets.MNIST('../data', train=True, download=True,\n                       transform=transform)\n\n\ndataset1\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: ../data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n           )"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#week-summary",
    "href": "meetups/Meetup-13/meetup-13.html#week-summary",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Week Summary",
    "text": "Week Summary\n\nFinal Lab is Almost Ready\n\nIt took lots of programming to prep it\n\nThis week: Algos\nNext Week: Neural Nets\nLast Week: Deep Nets"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#reading",
    "href": "meetups/Meetup-13/meetup-13.html#reading",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Reading",
    "text": "Reading\n\nChapter 8.1 to 8.4 of Intro to Optimization Algorithms\nWikipedia article on SGD Covers everything but good level\nPankaj Mehta’s Notebook I followed his notebook partially here\nDeep Learning Book 8.3 Parts of this chapter are very advanced\nGilbert Strang’s Linear Algebra and Learning from Data VI.4 and VI.5 Advanced and Scatterbrained, covers too much, but has deep insights\nAccompanying Lectures 22-26 on this playlist"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#meetup-may-7th",
    "href": "meetups/Meetup-13/meetup-13.html#meetup-may-7th",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Meetup May 7th!",
    "text": "Meetup May 7th!"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#non-convex-optimization",
    "href": "meetups/Meetup-13/meetup-13.html#non-convex-optimization",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Non-Convex Optimization",
    "text": "Non-Convex Optimization\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan’t guarantee global minimum\nCan find local minima\nHeuristics over theory"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#non-convex-optimization-1",
    "href": "meetups/Meetup-13/meetup-13.html#non-convex-optimization-1",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Non-Convex Optimization",
    "text": "Non-Convex Optimization\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan’t guarantee global minimum\nCan find local minima\nHeuristics over theory"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#revisiting-gradient-descent",
    "href": "meetups/Meetup-13/meetup-13.html#revisiting-gradient-descent",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Revisiting Gradient Descent",
    "text": "Revisiting Gradient Descent\n\nSimplest algorithm for finding a minimum\nStart with initial guess \\(\\mathbf{x}_0\\)\nCompute the gradient \\(\\nabla f(\\mathbf{x}_0)\\)\nGradient tells us slope of landscape\nTake steps down slope: \\[\n\\mathbf{x}_i = \\mathbf{x}_{i-1} -\\kappa\\nabla f(\\mathbf{x}_{i-1})\n\\]"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#gd-limited-by-condition-number",
    "href": "meetups/Meetup-13/meetup-13.html#gd-limited-by-condition-number",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "GD Limited by Condition Number",
    "text": "GD Limited by Condition Number\n\nLearning rate stability constraint: \\(\\kappa &lt; \\frac{\\lambda_{\\mathrm{max}}}{\\lambda_{\\mathrm{min}}} = c\\)\n\nPretending we are closing in on a minimum\nConvergence Rate limited by step size: \\[\n\\|\\mathbf{x}_{\\mathrm{opt}}-\\mathbf{x}_i\\| \\sim \\left(1-\\frac{\\lambda_{\\mathrm{min}}}{\\lambda_{\\mathrm{max}}}\\right)^{i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#valley-oscillation",
    "href": "meetups/Meetup-13/meetup-13.html#valley-oscillation",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Valley Oscillation",
    "text": "Valley Oscillation\n\nHigh Condition number indicates narrow, steep valley\nGD trajectories bounce up steep valley slopes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f(x,y) = x^2 + 10y^2\\)\n\\(\\kappa = 0.09\\)"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#gradient-descent-with-momentum",
    "href": "meetups/Meetup-13/meetup-13.html#gradient-descent-with-momentum",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\nImagine rolling a ball in a channel with steep sides\nFriction reduces movement up sides\nBuilds momentum rolling down channel"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#gradient-descent-with-momentum-1",
    "href": "meetups/Meetup-13/meetup-13.html#gradient-descent-with-momentum-1",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Gradient Descent with Momentum",
    "text": "Gradient Descent with Momentum\n\nIntroduce “velocity” variable \\[\n\\mathbf{x}_{i+1} = \\mathbf{x}_i - \\mathbf{v}_i\n\\]\nGradient drives velocity, momentum provides memory:\n\n\\[\n\\mathbf{v}_{i+1} = \\kappa\\nabla f(\\mathbf{x})_i + \\beta \\mathbf{v}_i\n\\]\n\nOscillations damp due to memory"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#gd-with-momentum",
    "href": "meetups/Meetup-13/meetup-13.html#gd-with-momentum",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "GD with Momentum",
    "text": "GD with Momentum\n\ndef gd_with_mom(grad, init, n_epochs=5000, eta=10**-4, beta=0.9,gamma=0.9):\n    params=np.array(init) # Start with initial condition\n    param_traj=np.zeros([n_epochs+1,2]) # Save the entire trajecotry\n    param_traj[0,]=init # Also save the initial condition to the trajectory\n    \n    v=0 # Starting with 0 momentum\n    \n    # Epochs is borrowing term from machine learning\n    # Here it means timestep\n    \n    for j in range(n_epochs): \n        v=gamma*v+(np.array(grad(params))) # Compute v\n        params=params-eta*v  # Update the location\n        param_traj[j+1,]=params # Save the trajectory\n    return param_traj"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#gd-with-momentum-1",
    "href": "meetups/Meetup-13/meetup-13.html#gd-with-momentum-1",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "GD with Momentum",
    "text": "GD with Momentum\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMomentum steps move horizontally more\nMomentum error dramatically better"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#hyperparameters",
    "href": "meetups/Meetup-13/meetup-13.html#hyperparameters",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Hyperparameters?",
    "text": "Hyperparameters?\n\nBoth cases I picked parameters close to optimal\nSee analysis in Strang\n\\(\\kappa = 0.05\\) for GD\n\\(\\kappa = \\left(\\frac{2}{\\sqrt{\\lambda_{\\mathrm{min}}}+\\sqrt{\\lambda_{\\mathrm{max}}}}\\right)^2\\), and \\(\\beta = \\left(\\frac{1-c^{-1/2}}{1+c^{-1/2}}\\right)^2\\) for GD with momentum\nError gap increases as \\(c\\) increases"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#hyperparameters-1",
    "href": "meetups/Meetup-13/meetup-13.html#hyperparameters-1",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nBut we don’t know \\(c\\) in advance, varies locally….\nWe handle this several ways:\n\nExperimentation\nHeuristics\nAdaptive Methods"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nCommon for optimization algorithms to “get stuck”"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-1",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-1",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nIn low-D local minima are problematic"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-2",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-2",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nTrajectories get Stuck"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-3",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-3",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nThe barriers in high dimensions likely more complex\nStochastic Gradient Descent modifies gradient descent style methods\n\nCompute the gradient of loss using only part of the data\nCalled a “mini-batch”\nSmaller the mini-batch, the more the noise"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-4",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-4",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-5",
    "href": "meetups/Meetup-13/meetup-13.html#stochastic-gradient-descent-5",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nIn 2D, can just add noise"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#annealing",
    "href": "meetups/Meetup-13/meetup-13.html#annealing",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Annealing",
    "text": "Annealing\n\nNoise can deflect you from the minimum when you are close\nCommon practice is to start with noise high, reduce it later\n\nSimulated annealing (from metallurgy)\n\nEven more common to start with high learning rate and gradually decrease it\n\nWhereas in some ML applications increasing batch-size could cause overfitting"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#adaptive-methods",
    "href": "meetups/Meetup-13/meetup-13.html#adaptive-methods",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Adaptive Methods",
    "text": "Adaptive Methods\n\nWe saw that hyper-parameters are hard to pick at outset\nIdeal learning rate depends on condition number and also magnitude of gradients\nConcept: Change Learning Rate as we go\n\nKeep memory of gradient in each variable\nStep in the direction of the average gradient\nDecrease learning rate if variance is high\nIncrease learning rate if variance is low"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#adam",
    "href": "meetups/Meetup-13/meetup-13.html#adam",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "ADAM",
    "text": "ADAM\n\nAdaptive Moment Estimation\nHave a local estimate of average gradient \\(\\hat{\\mathbf{v}}_i\\)\nHave a local estimate of squared gradient \\(\\hat{G}_{s,i}\\)\nAdjusted learning rate based on both: \\[\n\\mathbf{x}_{i+1} = \\mathbf{x}_i -\\kappa \\frac{\\hat{\\mathbf{v}}_i}{\\sqrt{\\hat{G}_{s,i}+\\epsilon}}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#estimating-the-moments",
    "href": "meetups/Meetup-13/meetup-13.html#estimating-the-moments",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Estimating the moments",
    "text": "Estimating the moments\n\nADAM uses an exponentially weighted moving average to update moments\nInitialize with \\(\\mathbf{v}_0=0\\) and \\(G_{s,0}=0\\)\nMemory parameters \\(\\beta\\) and \\(\\gamma\\) determine how fast ADAM forgets old values of gradient\nUpdate moments as follows:\n\n\\[\n\\mathbf{v}_{i+1} = (1-\\gamma)\\nabla f(\\mathbf{x}_i) + \\gamma\\mathbf{v}_i \\\\\nG_{s,i+1} = (1-\\beta)\\nabla f(\\mathbf{x}_i)^2 + \\beta G_{s,i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#bias-correction",
    "href": "meetups/Meetup-13/meetup-13.html#bias-correction",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Bias Correction",
    "text": "Bias Correction\n\nEarly on, very few points in time series\nDivide by \\((1-\\gamma^i)\\) and \\((1-\\beta^i)\\) to correct for bias: \\[\n\\hat{\\mathbf{v}}_i = \\frac{\\mathbf{v}_i}{1-\\gamma^i},\\quad \\hat{G}_{s,i} = \\frac{G_{s,i}}{1-\\beta^i}\n\\]\nFor large \\(i\\) bias correction vanishes"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#simple-adam-code",
    "href": "meetups/Meetup-13/meetup-13.html#simple-adam-code",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Simple ADAM Code",
    "text": "Simple ADAM Code\n\ndef adams(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9, beta=0.99,epsilon=10**-8):\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init\n    v=0;\n    grad_sq=0;\n    for j in range(n_epochs):\n        g=np.array(grad(params))\n        v=gamma*v+(1-gamma)*g\n        grad_sq=beta*grad_sq+(1-beta)*g*g\n        v_hat=v/(1-gamma**(j+1))\n        grad_sq_hat=grad_sq/(1-beta**(j+1))\n        params=params-eta*np.divide(v_hat,np.sqrt(grad_sq_hat+epsilon))\n        param_traj[j+1,]=params\n    return param_traj"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#when-and-why-is-adam-good",
    "href": "meetups/Meetup-13/meetup-13.html#when-and-why-is-adam-good",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "When and Why is ADAM good?",
    "text": "When and Why is ADAM good?\n\nADAM is much more robust to learning rate choices\nADAM is excellent when the gradient is sparse\nADAM is often the best in initial training stages"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#when-is-adam-bad",
    "href": "meetups/Meetup-13/meetup-13.html#when-is-adam-bad",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "When is ADAM bad?",
    "text": "When is ADAM bad?\n\nADAM is perhaps the most widely used optimizer, the default for deep learning\nHowever, it is not even guaranteed to converge on convex problems!\nCan generalize worse (need more regularization)\nLess memory efficient\nIt is very heuristic in nature, can be improved\nYou will explore benefits and drawbacks on your HW"
  },
  {
    "objectID": "meetups/Meetup-13/meetup-13.html#thanks",
    "href": "meetups/Meetup-13/meetup-13.html#thanks",
    "title": "DATA 609 Meetup 13: Optimization Algorithms",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#week-summary",
    "href": "meetups/Meetup-9/meetup-9.html#week-summary",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Week Summary",
    "text": "Week Summary\n\nTopic this week is convex optimization problems\nChapter 4.1, 4.2, 4.7 (we’ve sort of covered some of it)\nChapters 4.3-4.6 mostly useful for examples on first read (skim)\nRead about solvers\nLab 5 available\nKeep your eyes open for projects"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#before-we-start-log-concave",
    "href": "meetups/Meetup-9/meetup-9.html#before-we-start-log-concave",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Before we start, log-concave",
    "text": "Before we start, log-concave\n\nVariety of generalizations of convexity\n\nquasiconvex/quasiconcave\nconvexity w.r.t. a generalized inequality\n\nFor Stats and ML, log-concavity is very important\n\\(f(x)\\) is log-concave if \\(\\log(f(x))\\) is concave\n\nRequires \\(f(x) &gt; 0\\)\n\nThere is also log-convexity, but doesn’t arise"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#stats-examples",
    "href": "meetups/Meetup-9/meetup-9.html#stats-examples",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Stats Examples",
    "text": "Stats Examples\n\nMany probability density functions are log-concave:\n\nNormal distribution: \\(\\log(e^{-x^2}) = -x^2\\)\nExponential distribution \\(\\log(e^{-\\alpha x}) = -\\alpha x\\)\nAll from exponential family: \\[ p(x|\\theta) = \\exp\\left(\\sum_{i=1}^s\\nu_i(\\theta)T_i(x) - A(\\theta)\\right)h(x) \\]"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#stats-examples-1",
    "href": "meetups/Meetup-9/meetup-9.html#stats-examples-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Stats Examples",
    "text": "Stats Examples\n\nIncomplete list:\n\n\nwikipedia"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#marginalizationpartial-integration",
    "href": "meetups/Meetup-9/meetup-9.html#marginalizationpartial-integration",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Marginalization/Partial Integration",
    "text": "Marginalization/Partial Integration\n\nIf \\(f(\\mathbf{x},\\mathbf{y})\\) is log-concave, then so is: \\[\ng(\\mathbf{x}) = \\int \\mathbf{dy} f(\\mathbf{x},\\mathbf{y})\n\\]\n\\(p(\\mathbf{x},\\mathbf{y})\\) is log-concave then so is \\(p(\\mathbf{x}) = \\int\\mathbf{dy} p(\\mathbf{x},\\mathbf{y})\\)"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#convolution",
    "href": "meetups/Meetup-9/meetup-9.html#convolution",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Convolution",
    "text": "Convolution\n\nAddition of two random variables is done by convolution:\n\\(z = x+y\\) \\[\np_{z}(z) = \\int dx p_x(x)p_y(z-x) = p_x * p_y (z)\n\\]\nConvolution of two log-concave functions is log-concave"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#cumulative-distribution-functions",
    "href": "meetups/Meetup-9/meetup-9.html#cumulative-distribution-functions",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Cumulative Distribution Functions",
    "text": "Cumulative Distribution Functions\n\nCDF is probability random variable less than \\(x\\): \\[\nF(x) = \\int_{-\\infty}^x dx'p(x')\n\\]\nThis is log-concave if \\(p\\) is log-concave\nAlso true for multidimensional CDF"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#properties-of-log-concave",
    "href": "meetups/Meetup-9/meetup-9.html#properties-of-log-concave",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Properties of log-concave",
    "text": "Properties of log-concave\n\nProducts of log-concave are log-concave\n\n\\(f(\\mathbf{x})g(\\mathbf{x})\\)\n\nSums are not (but are for log-convex)\nAlternate definition: \\(f(x)\\) is log-convex if: \\[\nf(\\theta x + (1-\\theta)y) \\geq f(x)^{\\theta} f(y)^{1-\\theta}\n\\]\nMore technical: Moment and cumulant generating functions of log-concave density are log-concave"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#example-manufacturing-yield",
    "href": "meetups/Meetup-9/meetup-9.html#example-manufacturing-yield",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Example: Manufacturing Yield",
    "text": "Example: Manufacturing Yield\n\nYou have a factory manufacturing some product\n\nBicycles, Wheels, Aluminum Rods, Magnets for MRI machines, etc\n\nEach completed item is described by a set of parameters \\(\\theta\\)\n\nThe roundness of the wheels\nThickness of bicycle tubes\nStrength of rods/magnets\n\nThere is a set \\(S\\) which describes acceptable items, \\(\\theta\\in S\\) is item is well made"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#manufacturing-yield",
    "href": "meetups/Meetup-9/meetup-9.html#manufacturing-yield",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Manufacturing Yield",
    "text": "Manufacturing Yield\n\nSuppose that your factory tries to build items with a target set of paramters \\(x\\)\nRandomness/Imprecision causes manufactured items to deviate from target according to probability distribution: \\[\np(\\theta = x + w ) = p(w)\n\\]\nThen yield is defined as fraction of items within tolerances: \\[\nY(x) = \\int dw I(S,w+\\theta) p(w)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#manufacturing-yield-1",
    "href": "meetups/Meetup-9/meetup-9.html#manufacturing-yield-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Manufacturing Yield",
    "text": "Manufacturing Yield\n\nSuppose you want to achieve a certain yield\nIf \\(S\\) is convex, and \\(p\\) log-concave, then consider: \\[\nC = \\{x | Y(x) \\geq \\alpha \\} = \\{x | \\log(Y(x)) \\geq \\log(\\alpha) \\}\n\\]\nC is a convex set (super-level set of a convex function)"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#optimization-lingo",
    "href": "meetups/Meetup-9/meetup-9.html#optimization-lingo",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Optimization Lingo",
    "text": "Optimization Lingo\n\nCovered some already\n\nfeasible points satisfy constraints\noptimal value \\(p^{\\star}\\) is tightest lower bound objective\noptimal point is \\(\\mathbf{x}\\) where \\(f(\\mathbf{x}) = p^{\\star}\\)\n\nSome not:\n\ninfeasible: We say \\(p^{\\star}=\\infty\\) if we can’t find a single feasible point\nunbounded below: We say \\(p^{\\star} = -\\infty\\) if objective doesn’t have lower bound"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#implicit-constraints",
    "href": "meetups/Meetup-9/meetup-9.html#implicit-constraints",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Implicit Constraints",
    "text": "Implicit Constraints\n\nDomain of objective \\(f\\) and constraint functions is included implicitly in constraints\ni.e. \\(\\min_{x} f(x) = x\\log(x)\\) has implicit constraint \\(x &gt; 0\\)\nIf no explicit constraints problem is called unconstrained"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#slack-or-active-inequality-constraints",
    "href": "meetups/Meetup-9/meetup-9.html#slack-or-active-inequality-constraints",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Slack or Active Inequality Constraints",
    "text": "Slack or Active Inequality Constraints\n\nConsider: \\[\n\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\\ng_i(\\mathbf{x}) \\leq 0\n\\]\nLet \\(x_{\\mathrm{opt}}\\) be optimal point\nIf \\(g_i(x_{\\mathrm{opt}}) &lt; 0\\) inequality \\(i\\) is slack\nIf \\(g_i(x_{\\mathrm{opt}}) = 0\\) inequality \\(i\\) is active"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#transforming-optimization-problems",
    "href": "meetups/Meetup-9/meetup-9.html#transforming-optimization-problems",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Transforming Optimization Problems",
    "text": "Transforming Optimization Problems\n\nWe say that two (not necessarily convex) optimization problems are equivalent if we have a way to calculate the optimum of one problem from the solution of another problem\n\nMonotone Transformation of Objective\nInvertible Transformation of variables\nPartial Minimization\nElimination of Equality Constraints\nIntroduction of Slack Variables"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#transformation-by-monotone",
    "href": "meetups/Meetup-9/meetup-9.html#transformation-by-monotone",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Transformation by monotone",
    "text": "Transformation by monotone\n\nTransformation by monotone increasing function \\(h\\) \\[\n\\mathrm{argmin}_{x} h(f(x)) = \\mathrm{argmin}_{x} f(x)\n\\]\nlog is standard example\n\nML problems only convex once you take the log"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#transform-the-variables",
    "href": "meetups/Meetup-9/meetup-9.html#transform-the-variables",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Transform the variables",
    "text": "Transform the variables\n\nSuppose we have a one-to-one transformation \\(x_i = \\{\\phi_i(\\mathbf{z})\\}\\)\nThen can substitute directly into objective and constraints: \\[\n\\min_{\\mathbf{z}} f(\\phi(\\mathbf{z})) \\\\\ng_i(\\phi(\\mathbf{z})) \\leq 0 \\\\\nh_j(\\phi(\\mathbf{z})) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#transform-the-variables-1",
    "href": "meetups/Meetup-9/meetup-9.html#transform-the-variables-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Transform the variables",
    "text": "Transform the variables\n\nSuppose you have a bilinear objective or constraint \\[\nxy \\leq \\alpha\n\\]\nTake log (monotone):\n\\(\\log(x) + \\log(y)\\leq \\log(\\alpha)\\)\n\\(z_1 = \\log(x)\\),\\(z_2 = \\log(y)\\),\nLeads to \\(z_1+z_2\\leq\\log(\\alpha)\\)"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#example-geometric-programming",
    "href": "meetups/Meetup-9/meetup-9.html#example-geometric-programming",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Example: Geometric Programming",
    "text": "Example: Geometric Programming\n\nmonomial: \\(f(\\mathbf{x}) = cx_1^{a_1}x_2^{a_2}\\cdots x_n^{a_n}\\)\nposynomial: sum of monomials\nGeometric Program: \\[\n\\min_{\\mathbf{x}} f_0(\\mathbf{x}) \\\\\ng_i(\\mathbf{x}) \\leq 1,\\quad h_j(\\mathbf{x}) = 1\n\\]\n\\(f\\), \\(g_i\\) posynomials, \\(h_j\\) monomials"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#geometric-programming",
    "href": "meetups/Meetup-9/meetup-9.html#geometric-programming",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Geometric Programming",
    "text": "Geometric Programming\n\nCan transform to convex problem by: \\(y_i = \\log(x_i)\\), taking \\(\\log\\) of objectives and constraints: \\[\n\\min_{\\mathbf{y}} \\hat{f_0}(\\mathbf{y}) = \\log\\left(\\sum_k e^{\\mathbf{a}_{0k}^T\\mathbf{y}+b_{0k}} \\right) \\\\\n\\hat{g_i}(\\mathbf{y}) = \\log\\left(\\sum_k e^{\\mathbf{a}_{ik}^T\\mathbf{y}+b_{ik}} \\right) \\\\\n\\mathbf{h}_k^T\\mathbf{y} + d_i = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#partial-minimization",
    "href": "meetups/Meetup-9/meetup-9.html#partial-minimization",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Partial Minimization",
    "text": "Partial Minimization\n\nYou can minimize with respect to some of the variables \\[\n\\min_{x,y} f(x,y) = \\min_x \\hat{f}(x),\n\\]\nwhere \\(\\hat{f}(x) = \\min_y f(x,y)\\)\nConstraints at each step depend on \\(x\\)\nCan be life-saver, also preserves convexity"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#the-other-transformations",
    "href": "meetups/Meetup-9/meetup-9.html#the-other-transformations",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "The other transformations",
    "text": "The other transformations\n\nMostly useful for making fast algorithms\nEliminating linear equality constraints by solving matrix equation\nEliminating linear inequalities with slack variables\nSwitching to epigraph form"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#multicriterion-optimization",
    "href": "meetups/Meetup-9/meetup-9.html#multicriterion-optimization",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Multicriterion Optimization",
    "text": "Multicriterion Optimization\n\nOften you need to balance multiple competing objectives:\n\nFor example, if you are building a car, you might care about minimizing the cost and maximizing the crash test rating or the reliability\nIf you are investing, you want to maximize the return, but minimize the chance of a catastrophic loss\nIf you are taking a political position, you want to appeal to the needs of different groups with distinct opinions"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#multicriterion-optimization-1",
    "href": "meetups/Meetup-9/meetup-9.html#multicriterion-optimization-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Multicriterion Optimization",
    "text": "Multicriterion Optimization\n\\[\n\\min_{\\mathbf{x}} \\left(f_1(\\mathbf{x}),f_2(\\mathbf{x}),\\cdots,f_n(\\mathbf{x}\\right)) \\\\\ng_i(\\mathbf{x}) \\leq 0,\\quad h_j(\\mathbf{x}) = 0\n\\] - Feasible \\(x\\) that minimizes all objectives is an optimal point - Generally impossible - If possible objectives are called non-competing"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-optimal",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-optimal",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Optimal",
    "text": "Pareto Optimal\n\nConsider a feasible point \\(\\mathbf{x}_1\\)\n\\(\\mathbf{x}_1\\) dominates \\(\\mathbf{x}_2\\) if at least matches \\(\\mathbf{x}_2\\) on all objectives: \\[\nf_i(\\mathbf{x}_1)\\leq f_i(\\mathbf{x}_2)\n\\]\nand beats it on one \\(f_j(\\mathbf{x}_1) &lt; f_j(\\mathbf{x}_2)\\)\nTwo cars that are identical except one is cheaper\nA point \\(\\mathbf{x}\\) is Pareto optimal if it is dominated by no other points\nYou can make a case for a Pareto optimal points"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-front",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-front",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Front",
    "text": "Pareto Front\n\nGraph the competing objective functions"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-front-1",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-front-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Front",
    "text": "Pareto Front\n\nDominated points are above and to right of focal point"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-front-2",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-front-2",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Front",
    "text": "Pareto Front\n\nFocal point dominated by points down and left"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-front-3",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-front-3",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Front",
    "text": "Pareto Front\n\nPareto Front is Pareto Optimal points"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#pareto-front-4",
    "href": "meetups/Meetup-9/meetup-9.html#pareto-front-4",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Pareto Front",
    "text": "Pareto Front\n\nBoundary not necessarily a Pareto front"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#scalarization",
    "href": "meetups/Meetup-9/meetup-9.html#scalarization",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Scalarization",
    "text": "Scalarization\n\nCan add multiple objectives up to create a standard optimization problem \\[\n\\min_{\\mathbf{x}} \\sum_k \\lambda_k f_k(\\mathbf{x})\n\\]\n\\(\\lambda_k\\) convert between each objective function to the same units and define trade-offs\nAlways finds a Pareto Optimal point"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#ridge-regression-trade-off",
    "href": "meetups/Meetup-9/meetup-9.html#ridge-regression-trade-off",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Ridge Regression Trade Off",
    "text": "Ridge Regression Trade Off\n\n\nText(0, 0.5, 'Mean Square Error')"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#ridge-regression-trade-off-1",
    "href": "meetups/Meetup-9/meetup-9.html#ridge-regression-trade-off-1",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Ridge Regression Trade Off",
    "text": "Ridge Regression Trade Off\n\nfor v in lambd_values:\n    lambd.value = v\n    problem.solve()\n    train_errors.append(mse(X_train, Y_train, beta))\n    test_errors.append(mse(X_test, Y_test, beta))\n    beta_values.append(beta.value)\n    \nnorm_beta_vec = np.array([np.linalg.norm(vec)**2 for vec in beta_values])  \nplt.plot(norm_beta_vec,train_errors)\nplt.title(\"Pareto Surface for Ridge Regression\")\nplt.xlabel(\"Norm of Coefficients\")\nplt.ylabel(\"Mean Square Error\")"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#cvx-solvers",
    "href": "meetups/Meetup-9/meetup-9.html#cvx-solvers",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "CVX Solvers",
    "text": "CVX Solvers\n\nJust like other software packages, CVX has a number of different solvers\nThree free solvers: SCS, CLARABEL, OSQP\nSCS is the in-house solver (Splitting Conic Solver)\nOSQP from Oxford, does LP and QP\nCLARABEL from Oxford specializes in Second Order Cone Programs"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#choosing-a-solver",
    "href": "meetups/Meetup-9/meetup-9.html#choosing-a-solver",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Choosing a Solver",
    "text": "Choosing a Solver\n\nCVX can pick by default\nOr you can specify\n\n\nimport CVYPY as cvx\ncvx.solve(solver=\"CLARABEL\", \nverbose=False, \ngp=False, qcp=False,\nrequires_grad=False, \nenforce_dpp=False, \nignore_dpp=False, **kwargs)¶"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#not-all-solvers-can-solve-all-problems",
    "href": "meetups/Meetup-9/meetup-9.html#not-all-solvers-can-solve-all-problems",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Not all Solvers can Solve all Problems",
    "text": "Not all Solvers can Solve all Problems"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#warm-start",
    "href": "meetups/Meetup-9/meetup-9.html#warm-start",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Warm Start",
    "text": "Warm Start\n\nIf you are solving many versions of the same problem, CVX by default caches results of computation and uses them as starting point\nLeads to a big speed up"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#other-solvers-ive-used",
    "href": "meetups/Meetup-9/meetup-9.html#other-solvers-ive-used",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Other Solvers I’ve Used",
    "text": "Other Solvers I’ve Used\n\nMOSEK commercial nonlinear solver, some universities have a license/possible to get academic license\nGUROBI fastest for linear programming\nGLPK solver of last resort in some python packages when your MOSEK license isn’t working"
  },
  {
    "objectID": "meetups/Meetup-9/meetup-9.html#thanks",
    "href": "meetups/Meetup-9/meetup-9.html#thanks",
    "title": "DATA 609 Meetup 9: Convex Optimization Problems",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#richardson-iteration",
    "href": "meetups/Meetup-4/HW1Review.html#richardson-iteration",
    "title": "Lab 1 Review",
    "section": "Richardson Iteration",
    "text": "Richardson Iteration\n\nConsider Problem 3(a), showing that the Pseudoinverse is a fixed point of the Richardson Iteration algorithm:\n\n\\[\nx^{(k+1)} = x^{(k)} - \\mu A^{T}\\left(Ax^{(k)} - b \\right)\n\\]\n\nNext Step: Substitute \\(A^{+}b\\) for \\(x^{(x)}\\):\n\n\\[ x^{(k+1)} = A^{+}b - \\mu A^{(T)}\\left(AA^{+}b - b\\right)\\]"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#most-common-mistake",
    "href": "meetups/Meetup-4/HW1Review.html#most-common-mistake",
    "title": "Lab 1 Review",
    "section": "Most Common Mistake",
    "text": "Most Common Mistake\n\nMany of you then concluded:\n\n\\[ AA^{+}b - b = 0\\]\n\nSeveral justification, but:\n\n\nA = matrix(rnorm(40),nrow=10,ncol=4)\nb = rnorm(10)\n\nt(A  %*% solve(t(A) %*% A) %*% t(A) %*%  b)\n\n           [,1]      [,2]      [,3]       [,4]      [,5]      [,6]       [,7]\n[1,] -0.5249107 0.5515609 -1.100217 -0.8124702 0.5515855 0.2098126 -0.1389385\n          [,8]      [,9]    [,10]\n[1,] 0.4209922 0.6252625 -0.22508"
  },
  {
    "objectID": "meetups/Meetup-4/HW1Review.html#why-isnt-it-true",
    "href": "meetups/Meetup-4/HW1Review.html#why-isnt-it-true",
    "title": "Lab 1 Review",
    "section": "Why isn’t it true?",
    "text": "Why isn’t it true?\n\n\\(A^+\\)"
  },
  {
    "objectID": "meetups/Meetup-2/LatexTutorial.html",
    "href": "meetups/Meetup-2/LatexTutorial.html",
    "title": "This is a markdown cell",
    "section": "",
    "text": "Here is a math expression \\[\nx+1 = 0\n\\]\nHere is a more complex\n\\[\nx^2_0 + y_1 = 0\n\\]\nHere is how you exert greater control over the sub and superscripts\n\\[\nx^{2y_1-2}_{opt}\n\\]\nHere is how we do a sum\n\\[\n\\sum_{i=1}^{100} x_i\n\\]\nHere is how to write an integral:\n\\[\n\\int_{0}^{\\infty} dx \\exp(-x^2)\n\\]\n\\[\n\\int_{0}^{\\infty} dx e^{-x^2}\n\\]\n\\[\nf(x,y) = \\frac{\\sin(xy) }{1 + \\exp(x+y) }\n\\]\n\\[\nf(x,y) = x + y^2\n\\]\n\\[\n\\frac{\\partial f}{\\partial x} = 1\n\\]\n\\[\n\\frac{\\partial f}{\\partial y} = 2y\n\\]\n\\[\n\\nabla f\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y}\n\\end{bmatrix} ^T\n\\]\n\\[\n\\nabla f = \\begin{bmatrix}   \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y}\n\\end{bmatrix}\n\\]\nThe Identity in 3D\n\\[\nI = \\begin{bmatrix} 1 & 0 & 0 \\\\\n                    0 & 1 & 0 \\\\\n                    0 & 0 & 1\n    \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "href": "meetups/Meetup-2/LinearAlgebraTutorial.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb= np.random.randn(10)\n\n\na @ b\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nb.T @ a.T\n\narray([-0.63269674, -1.32769635, -0.09029985, -3.52182299, -6.03363023,\n        3.01976296, -2.75007156,  0.22389647, -1.55148195, -1.62650482])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.linalg.inv(a) @ a\n\narray([[ 1.00000000e+00, -3.65595152e-16, -1.37241160e-16,\n        -1.67938840e-15,  9.33527437e-16,  1.44199349e-15,\n         1.15089786e-16, -1.55696214e-15,  1.11022302e-15,\n        -2.63677968e-16],\n       [ 1.26550255e-15,  1.00000000e+00, -2.68494358e-16,\n         1.42303383e-15, -9.79693103e-16, -4.51521472e-15,\n         2.63044668e-15,  1.74407151e-15, -2.66453526e-15,\n         4.44089210e-16],\n       [-1.64423122e-15, -1.66173972e-16,  1.00000000e+00,\n         2.75288470e-15,  1.74102465e-16,  1.54544175e-15,\n        -2.79739957e-15, -2.04740762e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [-3.51858931e-16,  6.23676926e-17,  2.66458886e-16,\n         1.00000000e+00,  9.38263196e-17,  2.61301662e-16,\n         2.45064178e-16, -1.53357911e-16,  4.44089210e-16,\n        -4.44089210e-16],\n       [ 7.84703105e-16, -1.04920005e-15,  4.13020606e-16,\n        -2.12483741e-15,  1.00000000e+00, -4.53483867e-15,\n         4.18017106e-15,  1.71820802e-15,  0.00000000e+00,\n         0.00000000e+00],\n       [ 9.37535921e-16,  3.43043467e-16,  9.65347805e-17,\n         6.68706576e-16, -4.20016708e-16,  1.00000000e+00,\n        -1.09267442e-15,  1.01169627e-16,  0.00000000e+00,\n         4.44089210e-16],\n       [-8.71025446e-16,  1.09123436e-15, -7.64517688e-16,\n         9.73897302e-16, -6.40358430e-16,  3.68586912e-15,\n         1.00000000e+00, -1.54120216e-15,  0.00000000e+00,\n         8.88178420e-16],\n       [ 2.86959469e-15,  2.45431198e-16,  5.06260913e-16,\n        -5.81240373e-16, -5.42632247e-16, -2.38411883e-15,\n         1.09159912e-15,  1.00000000e+00, -8.88178420e-16,\n        -4.44089210e-16],\n       [-3.57782462e-17, -5.09336718e-16,  7.90198623e-16,\n        -2.53964484e-15, -7.71687051e-17, -1.71387168e-15,\n         1.28935306e-15,  1.41638585e-15,  1.00000000e+00,\n        -8.88178420e-16],\n       [-5.10166112e-16,  3.36418889e-16,  3.54329540e-16,\n        -1.34491999e-15, -3.29604245e-16, -7.44603616e-16,\n         1.18536447e-15,  5.30313254e-16,  4.44089210e-16,\n         1.00000000e+00]])\n\n\n\nnp.linalg.svd(a).Vh\n\narray([[ 0.28383114,  0.13031739,  0.08488282,  0.26915419, -0.05833712,\n        -0.55745972,  0.04622785,  0.52583554, -0.47933131, -0.01565541],\n       [-0.7299212 ,  0.04346431, -0.15481374,  0.1199476 ,  0.28639583,\n        -0.35620311,  0.24101023, -0.22640736, -0.21810744, -0.24728702],\n       [ 0.07621821,  0.43546203,  0.2088927 ,  0.28230896, -0.17741055,\n        -0.17934521,  0.42879952, -0.01233035,  0.62390845, -0.21049244],\n       [-0.39305518,  0.11448225,  0.09008748,  0.41564326, -0.41297868,\n         0.10835773, -0.0692628 , -0.00268073, -0.05982432,  0.67885942],\n       [ 0.06933558,  0.34860056, -0.39022658,  0.26430987, -0.41367073,\n         0.19057494, -0.35422431, -0.22534478, -0.22225331, -0.46737544],\n       [-0.05118823, -0.68217798, -0.03157757,  0.58681909,  0.00875722,\n         0.07532819, -0.10790011,  0.20278167,  0.24018863, -0.26543311],\n       [ 0.31944117, -0.2503526 , -0.62925149,  0.04300488, -0.15611409,\n        -0.20160406,  0.45924193, -0.30071194, -0.00703338,  0.26660451],\n       [ 0.25406731,  0.17492689,  0.15182522,  0.44334045,  0.50247181,\n         0.45816405,  0.28794845, -0.19810239, -0.31166484,  0.05968817],\n       [ 0.22725739, -0.10877031,  0.36247981,  0.15032739,  0.06499115,\n        -0.45411298, -0.38287566, -0.64660084,  0.0248702 ,  0.08237576],\n       [ 0.02620891,  0.29393688, -0.46195436,  0.15967492,  0.51017133,\n        -0.14029136, -0.41663193,  0.18003309,  0.35346832,  0.25124871]])\n\n\n\nnp.diagonal?\n\n\nSignature:       np.diagonal(a, offset=0, axis1=0, axis2=1)\nCall signature:  np.diagonal(*args, **kwargs)\nType:            _ArrayFunctionDispatcher\nString form:     &lt;function diagonal at 0x7b3729c832e0&gt;\nFile:            ~/miniforge3/envs/stan/lib/python3.11/site-packages/numpy/core/fromnumeric.py\nDocstring:      \nReturn specified diagonals.\nIf `a` is 2-D, returns the diagonal of `a` with the given offset,\ni.e., the collection of elements of the form ``a[i, i+offset]``.  If\n`a` has more than two dimensions, then the axes specified by `axis1`\nand `axis2` are used to determine the 2-D sub-array whose diagonal is\nreturned.  The shape of the resulting array can be determined by\nremoving `axis1` and `axis2` and appending an index to the right equal\nto the size of the resulting diagonals.\nIn versions of NumPy prior to 1.7, this function always returned a new,\nindependent array containing a copy of the values in the diagonal.\nIn NumPy 1.7 and 1.8, it continues to return a copy of the diagonal,\nbut depending on this fact is deprecated. Writing to the resulting\narray continues to work as it used to, but a FutureWarning is issued.\nStarting in NumPy 1.9 it returns a read-only view on the original array.\nAttempting to write to the resulting array will produce an error.\nIn some future release, it will return a read/write view and writing to\nthe returned array will alter your original array.  The returned array\nwill have the same type as the input array.\nIf you don't write to the array returned by this function, then you can\njust ignore all of the above.\nIf you depend on the current behavior, then we suggest copying the\nreturned array explicitly, i.e., use ``np.diagonal(a).copy()`` instead\nof just ``np.diagonal(a)``. This will work with both past and future\nversions of NumPy.\nParameters\n----------\na : array_like\n    Array from which the diagonals are taken.\noffset : int, optional\n    Offset of the diagonal from the main diagonal.  Can be positive or\n    negative.  Defaults to main diagonal (0).\naxis1 : int, optional\n    Axis to be used as the first axis of the 2-D sub-arrays from which\n    the diagonals should be taken.  Defaults to first axis (0).\naxis2 : int, optional\n    Axis to be used as the second axis of the 2-D sub-arrays from\n    which the diagonals should be taken. Defaults to second axis (1).\nReturns\n-------\narray_of_diagonals : ndarray\n    If `a` is 2-D, then a 1-D array containing the diagonal and of the\n    same type as `a` is returned unless `a` is a `matrix`, in which case\n    a 1-D array rather than a (2-D) `matrix` is returned in order to\n    maintain backward compatibility.\n    If ``a.ndim &gt; 2``, then the dimensions specified by `axis1` and `axis2`\n    are removed, and a new axis inserted at the end corresponding to the\n    diagonal.\nRaises\n------\nValueError\n    If the dimension of `a` is less than 2.\nSee Also\n--------\ndiag : MATLAB work-a-like for 1-D and 2-D arrays.\ndiagflat : Create diagonal arrays.\ntrace : Sum along diagonals.\nExamples\n--------\n&gt;&gt;&gt; a = np.arange(4).reshape(2,2)\n&gt;&gt;&gt; a\narray([[0, 1],\n       [2, 3]])\n&gt;&gt;&gt; a.diagonal()\narray([0, 3])\n&gt;&gt;&gt; a.diagonal(1)\narray([1])\nA 3-D example:\n&gt;&gt;&gt; a = np.arange(8).reshape(2,2,2); a\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n&gt;&gt;&gt; a.diagonal(0,  # Main diagonals of two arrays created by skipping\n...            0,  # across the outer(left)-most axis last and\n...            1)  # the \"middle\" (row) axis first.\narray([[0, 6],\n       [1, 7]])\nThe sub-arrays whose main diagonals we just obtained; note that each\ncorresponds to fixing the right-most (column) axis, and that the\ndiagonals are \"packed\" in rows.\n&gt;&gt;&gt; a[:,:,0]  # main diagonal is [0 6]\narray([[0, 2],\n       [4, 6]])\n&gt;&gt;&gt; a[:,:,1]  # main diagonal is [1 7]\narray([[1, 3],\n       [5, 7]])\nThe anti-diagonal can be obtained by reversing the order of elements\nusing either `numpy.flipud` or `numpy.fliplr`.\n&gt;&gt;&gt; a = np.arange(9).reshape(3, 3)\n&gt;&gt;&gt; a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n&gt;&gt;&gt; np.fliplr(a).diagonal()  # Horizontal flip\narray([2, 4, 6])\n&gt;&gt;&gt; np.flipud(a).diagonal()  # Vertical flip\narray([6, 4, 2])\nNote that the order in which the diagonal is retrieved varies depending\non the flip function.\nClass docstring:\nClass to wrap functions with checks for __array_function__ overrides.\nAll arguments are required, and can only be passed by position.\nParameters\n----------\ndispatcher : function or None\n    The dispatcher function that returns a single sequence-like object\n    of all arguments relevant.  It must have the same signature (except\n    the default values) as the actual implementation.\n    If ``None``, this is a ``like=`` dispatcher and the\n    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n    first (additional and positional) argument.\nimplementation : function\n    Function that implements the operation on NumPy arrays without\n    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n    will be forwarded to this (and the ``dispatcher``) as if using\n    ``*args, **kwargs``.\nAttributes\n----------\n_implementation : function\n    The original implementation passed in.\n\n\n\n\nnp.diag([1,2,3])\n\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])\n\n\n\nb = np.random.randn(100,2)\n\n\nnp.linalg.svd(b)\n\nSVDResult(U=array([[ 3.18048731e-05,  6.74648048e-02, -6.41579788e-02, ...,\n        -4.70143275e-02, -1.17012062e-02,  5.72707204e-02],\n       [-1.67854420e-01,  1.70642880e-01, -1.21850722e-01, ...,\n        -1.51615802e-01,  2.00743368e-02, -1.09802273e-01],\n       [-7.00863302e-02,  1.18377452e-01,  9.84604342e-01, ...,\n        -1.72681740e-02,  1.25486957e-03, -7.25125082e-03],\n       ...,\n       [-1.04494452e-01,  1.19440600e-01, -1.73568100e-02, ...,\n         9.79733632e-01,  1.95664282e-03, -1.09753255e-02],\n       [ 2.32789965e-02, -5.98960947e-04,  1.31502367e-03, ...,\n         2.01688869e-03,  9.99525031e-01,  2.51988849e-03],\n       [-1.23784387e-01,  8.99047891e-03, -7.56209658e-03, ...,\n        -1.12822312e-02,  2.51689274e-03,  9.86617378e-01]]), S=array([9.7586975 , 8.78873925]), Vh=array([[-0.46515128, -0.8852312 ],\n       [-0.8852312 ,  0.46515128]]))\n\n\n\\[ A = U\\Sigma V^T\\] \\[ (A^TA)^{-1} A = (V\\Sigma^T U^T U \\Sigma^T V^T)^{-1} U\\Sigma V^T    \\] \\[A^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V\\Sigma^T U^T \\] \\[ A^+ = V (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T \\] \\[ A^+ = V\\Sigma^+ U^T \\]\n\nU = np.linalg.svd(b)[0]\n\n\nU.T @ U\n\narray([[ 1.00000000e+00, -1.24938610e-16,  5.63615904e-17, ...,\n         2.77555756e-17, -5.20417043e-18,  0.00000000e+00],\n       [-1.24938610e-16,  1.00000000e+00, -3.15292394e-17, ...,\n        -3.46944695e-17,  6.50521303e-18, -9.54097912e-18],\n       [ 5.63615904e-17, -3.15292394e-17,  1.00000000e+00, ...,\n        -3.46944695e-18, -1.30104261e-18,  5.20417043e-18],\n       ...,\n       [ 2.77555756e-17, -3.46944695e-17, -3.46944695e-18, ...,\n         1.00000000e+00,  4.82754810e-19, -4.24892617e-18],\n       [-5.20417043e-18,  6.50521303e-18, -1.30104261e-18, ...,\n         4.82754810e-19,  1.00000000e+00,  3.18478586e-19],\n       [ 0.00000000e+00, -9.54097912e-18,  5.20417043e-18, ...,\n        -4.24892617e-18,  3.18478586e-19,  1.00000000e+00]])\n\n\n\n$$ V(\\Sigma^T\\Sigma)^{-1}\\Sigma^TU^T b = x $$\n$$ (\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b  = V^T x$$\n$$ (\\Sigma^T U^T b) = (\\Sigma^T\\Sigma) (V^T x)$$\n\n`V @ np.linalg.solve(S^T @ S, U.T @ b)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-of-optimization-problems",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-of-optimization-problems",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology of Optimization Problems",
    "text": "Zoology of Optimization Problems\n\\[ \\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\\ng_i(\\mathbf{x}) \\leq 0, i=1, \\cdots, p\n\\]\n\nDifferent “names” for optimization problems based on what \\(f\\) and \\(g\\) are"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-linear-programs-lp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-linear-programs-lp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Linear Programs (LP)",
    "text": "Zoology: Linear Programs (LP)\n\\[ \\min_{\\mathbf{x}} \\mathbf{c}^T\\mathbf{x} \\\\\nA\\mathbf{x} \\leq \\mathbf{d}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-qp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-qp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Quadratic Programming (QP)",
    "text": "Zoology: Quadratic Programming (QP)\n\\[ \\min_{\\mathbf{x}} \\frac{1}{2} \\mathbf{x}^T P \\mathbf{x} + \\mathbf{c}^T\\mathbf{x} \\\\\nA\\mathbf{x} \\leq \\mathbf{d}\n\\]\n\n\\(P\\) positive (semi)-definite"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-quadratic-constraints-qcqp",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-quadratic-programming-quadratic-constraints-qcqp",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Quadratic Programming Quadratic Constraints (QCQP)",
    "text": "Zoology: Quadratic Programming Quadratic Constraints (QCQP)\n\\[ \\min_{\\mathbf{x}} \\frac{1}{2} \\mathbf{x}^T P \\mathbf{x} + \\mathbf{c}^T\\mathbf{x} \\\\\n\\frac{1}{2} \\mathbf{x}^T P_i \\mathbf{x} + \\mathbf{c}_i^T\\mathbf{x} \\leq \\mathbf{d}_i \\\\\nA\\mathbf{x} = \\mathbf{b}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#zoology-others",
    "href": "meetups/Meetup-5/meetup-5.html#zoology-others",
    "title": "Meetup 5: Convex Sets",
    "section": "Zoology: Others…",
    "text": "Zoology: Others…\n\nSemidefinite Programming\nConic Optimization\nSecond Order Cone Programming"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development",
    "href": "meetups/Meetup-5/meetup-5.html#development",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nThese problems were studied sequentially starting in the 1800s\nDevelopment of Computer in the 1940s led to wide application\nChange in perspective:\n\nPre 1970s, Linear problems are easy, linear hard\nPost 1970s, Convex problems are easy, non-convex hard"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development-1",
    "href": "meetups/Meetup-5/meetup-5.html#development-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nRobert Vanderbei/NY Times 1984"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#development-2",
    "href": "meetups/Meetup-5/meetup-5.html#development-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Development",
    "text": "Development\n\nwikipedia"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#weeks-ahead",
    "href": "meetups/Meetup-5/meetup-5.html#weeks-ahead",
    "title": "Meetup 5: Convex Sets",
    "section": "Week(s) Ahead",
    "text": "Week(s) Ahead\n\nWe begin convex optimization today\nWe will learn how to recognize and formulate convex problems\nAt first it will seem weird, but stick with it\nNext two weeks: Understanding whether constraint sets are convex"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#how-will-we-solve-convex-problems",
    "href": "meetups/Meetup-5/meetup-5.html#how-will-we-solve-convex-problems",
    "title": "Meetup 5: Convex Sets",
    "section": "How will we solve Convex Problems?",
    "text": "How will we solve Convex Problems?\n\nFor us \\(f\\) will be convex and so will be the constraint set\nThe hard part is recognizing the convexity, algorithms are very well developed\n\n\nimport cvxpy as cp\nx = cp.Variable()\ny = cp.Variable()\n\n# DCP problems.\nprob1 = cp.Problem(cp.Minimize(cp.square(x - y)),\n                    [x + y &gt;= 0,\n                    x &gt;= 4,\n                    y &lt;= 1])\nprob1.solve()"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Convex Set",
    "text": "What is a Convex Set\n\nIf you draw a line between any two points in the set, the entire line will be in the set"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set-1",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-convex-set-1",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Convex Set",
    "text": "What is a Convex Set\n\nIf even one pair of points exists where this fails, the set is not convex"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\nConvex optimization: local minimum always coincides with global minimum\nLocal minimum is findable"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-1",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvex region\nSingle Local Min"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-2",
    "href": "meetups/Meetup-5/meetup-5.html#why-is-convexity-important-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Why is Convexity Important?",
    "text": "Why is Convexity Important?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonconvex\nMany local min"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#convex-combinations",
    "href": "meetups/Meetup-5/meetup-5.html#convex-combinations",
    "title": "Meetup 5: Convex Sets",
    "section": "Convex Combinations",
    "text": "Convex Combinations\n\nIf we have two points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\), a convex combination is: \\[\n\\theta_1\\mathbf{x}_1 + (1-\\theta_1)\\mathbf{x}_2,\n\\]\n\nwhere \\(0\\leq\\theta_1\\leq 1\\)\n\nGenearlize to more points (weighted averages): \\[\n\\sum_i \\theta_i\\mathbf{x}_i, \\quad 0\\leq \\theta_i, \\quad\n\\sum_i \\theta_i = 1\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#math-definition-of-convex-sets",
    "href": "meetups/Meetup-5/meetup-5.html#math-definition-of-convex-sets",
    "title": "Meetup 5: Convex Sets",
    "section": "Math Definition of Convex Sets",
    "text": "Math Definition of Convex Sets\n\nA set is convex is for every pair of points \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in the set, every convex combination of those two points is in the set."
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#definition-not-useful-in-practice",
    "href": "meetups/Meetup-5/meetup-5.html#definition-not-useful-in-practice",
    "title": "Meetup 5: Convex Sets",
    "section": "Definition Not Useful In Practice",
    "text": "Definition Not Useful In Practice\n\nUsing the definition is a last resort\nInstead, we will use a constructive approach\n\nHave example convex sets\nHave operations that preserve convexity\nWrite target set use operations on examples"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#affine-sets-lines-planes-hyperplanes",
    "href": "meetups/Meetup-5/meetup-5.html#affine-sets-lines-planes-hyperplanes",
    "title": "Meetup 5: Convex Sets",
    "section": "Affine Sets (Lines, Planes, Hyperplanes)",
    "text": "Affine Sets (Lines, Planes, Hyperplanes)\n\n\n\n\\(y = ax + b\\)\nSolutions of: \\[\nA\\mathbf{x} = \\mathbf{d}\n\\]\nUnderdetermined Case"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#convex-hull",
    "href": "meetups/Meetup-5/meetup-5.html#convex-hull",
    "title": "Meetup 5: Convex Sets",
    "section": "Convex Hull",
    "text": "Convex Hull\n\nSuppose you have a non-convex set\nDefine a new set to be the convex combination of all the points in the set"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#examples-polyhedra",
    "href": "meetups/Meetup-5/meetup-5.html#examples-polyhedra",
    "title": "Meetup 5: Convex Sets",
    "section": "Examples: Polyhedra",
    "text": "Examples: Polyhedra"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-1",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-1",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-2",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-2",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-3",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-3",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a polyhedron?",
    "text": "What is a polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-4",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-4",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-5",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-5",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-6",
    "href": "meetups/Meetup-5/meetup-5.html#what-is-a-polyhedron-6",
    "title": "Meetup 5: Convex Sets",
    "section": "What is a Polyhedron?",
    "text": "What is a Polyhedron?\n\nSolution set of linear inequalities and equations\n\n\\[\\{x\\, |\\, C\\mathbf{x}\\preceq \\mathbf{d},\\, A\\mathbf{x} = \\mathbf{h} \\}\\]\n\n\\(\\preceq\\) stands for componentwise inequality\nTrue if every single entry satisfies inequality\nSometimes polyhedron stands for closed, polytope stands for open"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet",
    "href": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet",
    "title": "Meetup 5: Convex Sets",
    "section": "Example: Designing a Healthy Diet",
    "text": "Example: Designing a Healthy Diet\n\n\n\nYou are in charge of buying food\nLarge org\n\nArmy\nSchool System\nNGO/UN\n\nFood \\(i\\) has \\(V_{ij}\\) of nutrient \\(j\\)\n\n\n\n\n\nNutrient\nContent\n\n\n\n\nCalories (kCal)\n100\n\n\nFat (g)\n2\n\n\nCarbs (g)\n20\n\n\nProtein (g)\n2\n\n\nVitamin C (mg)\n90\n\n\nFiber (g)\n4\n\n\n\nOranges"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet-1",
    "href": "meetups/Meetup-5/meetup-5.html#example-designing-a-healthy-diet-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Example: Designing a Healthy Diet",
    "text": "Example: Designing a Healthy Diet\n\nEach nutrient has a minimum and/or maximum\n\n\n\n\nNutrient\nMinimum\nMaximum\n\n\n\n\nCalories\n2500\n\n\n\nFat\n50\n\n\n\nCarbs\n200\n\n\n\nProtein\n40\n200\n\n\nVitamin C\n100\n1000\n\n\nFiber\n20\n80"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#recommended-values-become-inequalities",
    "href": "meetups/Meetup-5/meetup-5.html#recommended-values-become-inequalities",
    "title": "Meetup 5: Convex Sets",
    "section": "Recommended Values Become Inequalities",
    "text": "Recommended Values Become Inequalities\n\n\\(\\mathbf{w}\\) is the diet vector, containing how much of each food we buy per person\nLet \\(\\mathbf{v}_j\\) be the \\(j\\)th row of \\(V\\) how much of nutrient \\(j\\) each food contains\nThen for each nutrient get an inequalities: \\[\nl_{min,j} \\leq \\mathbf{v}_j^T \\mathbf{w} \\leq l_{max,i}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#rdv-constraints",
    "href": "meetups/Meetup-5/meetup-5.html#rdv-constraints",
    "title": "Meetup 5: Convex Sets",
    "section": "RDV Constraints",
    "text": "RDV Constraints\n\nSplit the inequalities up:\n\n\\[\n-\\mathbf{v}_j^T\\mathbf{w} \\leq -l_{min,j} \\\\\n\\mathbf{v}_j^T\\mathbf{w} \\leq l_{max,j}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#constraints-are-a-polyhedron",
    "href": "meetups/Meetup-5/meetup-5.html#constraints-are-a-polyhedron",
    "title": "Meetup 5: Convex Sets",
    "section": "Constraints are a polyhedron",
    "text": "Constraints are a polyhedron\n\nForm matrix \\(U\\) from rows \\(\\mathbf{v}\\)\nIf both positive and negative, include both \\(-\\mathbf{v}_j\\) and \\(\\mathbf{v}_J\\).\nIf only have a negative or positive, include just one\n\\(\\mathbf{l}\\) contains upper and lower bounds\nAdd constraint that \\(-\\mathbf{w}\\preceq 0\\) \\[ U\\mathbf{w} \\preceq \\mathbf{l}\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#each-food-has-a-cost",
    "href": "meetups/Meetup-5/meetup-5.html#each-food-has-a-cost",
    "title": "Meetup 5: Convex Sets",
    "section": "Each Food Has a Cost",
    "text": "Each Food Has a Cost\n\nEach food has a cost \\(c_i\\)\nDescribe with vector \\(\\mathbf{c}\\)\nObjective function \\(\\mathbf{c}^T\\mathbf{w}\\)\nOptimization Problem:\n\n\\[\n\\min_{\\mathbf{w}} \\mathbf{c}^T\\mathbf{w} \\\\\nU\\mathbf{w} \\preceq \\mathbf{l}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#simplex",
    "href": "meetups/Meetup-5/meetup-5.html#simplex",
    "title": "Meetup 5: Convex Sets",
    "section": "Simplex",
    "text": "Simplex\n\n\n\nSimplex is set of points satisfying: \\[\n\\{\\mathbf{p}\\, |\\, \\sum_{i}p_i=1,\\, p_i\\geq 0\\}\n\\]\nDiscrete probability distribution"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#half-planes",
    "href": "meetups/Meetup-5/meetup-5.html#half-planes",
    "title": "Meetup 5: Convex Sets",
    "section": "Half-Planes",
    "text": "Half-Planes\n\n\n\nSpecial case of polyhedron (and also cone)\n\\(\\mathbf{c}^T\\mathbf{x} \\leq d\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nBall has center and radius \\[\nB(\\mathbf{x}_c,r) = \\{\\mathbf{x}\\,|\\, \\|\\mathbf{x}-\\mathbf{x}_c\\|\\leq r\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-1",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nEllipsoid described by positive definite matrix \\(P\\) \\[\nE(\\mathbf{x}_c,P) = \\{\\mathbf{x}\\,|\\, (\\mathbf{x}-\\mathbf{x}_c)^TP^{-1}(\\mathbf{x}-\\mathbf{x}_x)\\leq 1\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-2",
    "href": "meetups/Meetup-5/meetup-5.html#balls-and-ellipsoids-2",
    "title": "Meetup 5: Convex Sets",
    "section": "Balls and Ellipsoids",
    "text": "Balls and Ellipsoids\n\nEllipsoid described by positive definite matrix \\(P\\)\nAlternately: \\[\nE(\\mathbf{x}_c,P) = \\{\\mathbf{x}\\,|\\, \\|A\\mathbf{x}-\\mathbf{x_c}  \\|\\leq 1  \\},\n\\]\nWhere \\(P = A^TA\\), \\(A\\) square, non-singular"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#examples",
    "href": "meetups/Meetup-5/meetup-5.html#examples",
    "title": "Meetup 5: Convex Sets",
    "section": "Examples:",
    "text": "Examples:\n\nLimit on expected return variance in a portfolio:\n\n\\(\\mathbf{w}^T\\Gamma\\mathbf{w} \\leq v_{max}\\)\n\nLimit on distance of an antenna from a transmitter:\n\n\\(\\|\\mathbf{x}-\\mathbf{x}_c\\|&lt; r\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#cones",
    "href": "meetups/Meetup-5/meetup-5.html#cones",
    "title": "Meetup 5: Convex Sets",
    "section": "Cones",
    "text": "Cones\n\nCone is a set which has the following property:\nIf \\(\\mathbf{x}\\) is in the cone, then \\(\\theta \\mathbf{x}\\) is also in the cone, for \\(\\theta\\geq 0\\)"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#conic-combination",
    "href": "meetups/Meetup-5/meetup-5.html#conic-combination",
    "title": "Meetup 5: Convex Sets",
    "section": "Conic Combination",
    "text": "Conic Combination\n\n\n\nLike convex combination, but \\(\\theta\\) can be large: \\[\n\\sum_{i}\\mathbf{x}_i\\theta_i,\\, \\theta_i \\geq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#conic-combination-1",
    "href": "meetups/Meetup-5/meetup-5.html#conic-combination-1",
    "title": "Meetup 5: Convex Sets",
    "section": "Conic Combination",
    "text": "Conic Combination\n\n\n\nLike convex combination, but \\(\\theta\\) can be large: \\[\n\\sum_{i}\\mathbf{x}_i\\theta_i,\\, \\theta_i \\geq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#norm-balls-and-norm-cone",
    "href": "meetups/Meetup-5/meetup-5.html#norm-balls-and-norm-cone",
    "title": "Meetup 5: Convex Sets",
    "section": "Norm Balls and Norm Cone",
    "text": "Norm Balls and Norm Cone\n\nFor any norm \\(N\\): \\[\nB_N(r,\\mathbf{x}_c) = \\{\\mathbf{x}\\,|\\, \\|\\mathbf{x}-\\mathbf{x}_c\\|_N &lt; r\\}\n\\]\n\\(B_N(r,\\mathbf{x}_c)\\) is Convex and called a “Norm Ball”\nNorm Cone: \\[\n\\{[\\mathbf{x}\\,, t]\\,|\\, \\|\\mathbf{x}\\|_N \\leq t\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#norm-cone",
    "href": "meetups/Meetup-5/meetup-5.html#norm-cone",
    "title": "Meetup 5: Convex Sets",
    "section": "Norm Cone",
    "text": "Norm Cone\n\nHere the \\(t\\) is restricting how wide \\(\\mathbf{x}\\) is allowed to be"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#optimization-under-uncertainty",
    "href": "meetups/Meetup-5/meetup-5.html#optimization-under-uncertainty",
    "title": "Meetup 5: Convex Sets",
    "section": "Optimization Under Uncertainty",
    "text": "Optimization Under Uncertainty\n\nConsider “uncertain” nutrition optimization problem \\[\n\\min_{\\mathbf{w}} \\mathbf{c}^T\\mathbf{w} \\\\\nU\\mathbf{w} \\preceq \\mathbf{l}\n\\]\nThe nutrient contents in \\(U\\) are random\nVariance \\(\\sigma_i\\), for each nutrient, mean \\(\\bar{U_i}\\).\nInsist that: \\[\n\\bar{U_i}\\mathbf{w} + k\\|\\sigma_i^T\\mathbf{w}  \\| \\leq l_i\n\\]"
  },
  {
    "objectID": "meetups/Meetup-5/meetup-5.html#thanks",
    "href": "meetups/Meetup-5/meetup-5.html#thanks",
    "title": "Meetup 5: Convex Sets",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#week-summary",
    "href": "meetups/Meetup-12/meetup-12.html#week-summary",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Week Summary",
    "text": "Week Summary\n\nThis week is about duality\n\nLet’s you perform a sensitivity analysis of your optimization problem\nEnables solution of more difficult problems\n\nLab 6 is Due this week at midnight\nReading is parts of Chapter 5: 5.1, 5.2, 5.4.3, 5.4.4, 5.5.2, 5.6\nIf you want, Section 3.3"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#may-9th-meetup",
    "href": "meetups/Meetup-12/meetup-12.html#may-9th-meetup",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "May 9th Meetup!",
    "text": "May 9th Meetup!"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#what-is-duality",
    "href": "meetups/Meetup-12/meetup-12.html#what-is-duality",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "What is Duality?",
    "text": "What is Duality?\n\nFor every optimization problem (not necessarily convex): \\[\n\\mathrm{min}_{\\mathbf{x}} f(\\mathbf{x}),\\quad g_i(\\mathbf{x}) \\leq 0,\\quad h_j(\\mathbf{x}) = 0\n\\]\nThere is a “dual” optimization problem: \\[\n\\min_{(\\lambda,\\nu)} f_d(\\lambda,\\nu),\\quad \\lambda_i \\geq 0\n\\]\nThe dual variables correspond to constraints and have interpretation as prices\n\\(f_d\\) depends on original objective and constraints"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#why-do-we-care-about-duality",
    "href": "meetups/Meetup-12/meetup-12.html#why-do-we-care-about-duality",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Why do we care about duality?",
    "text": "Why do we care about duality?\n\nThe dual problem is always convex!\nThe dual will provide a lower bound on the original objective\nFor convex problems, the dual problem is equivalent*\nThe dual tells us how the solution changes when we change our problem"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#economic-perspective-on-dual",
    "href": "meetups/Meetup-12/meetup-12.html#economic-perspective-on-dual",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Economic perspective on dual",
    "text": "Economic perspective on dual\n\nConsider original problem with inequality constraints: \\[\n\\mathrm{min}_{\\mathbf{x}} f(\\mathbf{x}),\\quad g_i(\\mathbf{x}) \\leq 0\n\\]\nSuppose \\(f(\\mathbf{x})\\) was the “profit” of our organization\n\\(g_i(\\mathbf{x})\\) constraint represents capacity constraint:\n\n\\(g\\) could be our available warehouse space\n\\(g\\) could be how many employees we have\n\\(g\\) could be our freight capacity"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#attach-a-cost-to-constraint",
    "href": "meetups/Meetup-12/meetup-12.html#attach-a-cost-to-constraint",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Attach a cost to constraint",
    "text": "Attach a cost to constraint\n\nSuppose we could pay a cost \\(\\lambda\\) to get more warehouse space/freight capacity/employees: \\[\n\\mathrm{min}_{\\mathbf{x}} f(\\mathbf{x}) + \\lambda g(\\mathbf{x})\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#attach-a-cost-to-constraint-1",
    "href": "meetups/Meetup-12/meetup-12.html#attach-a-cost-to-constraint-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Attach a cost to constraint",
    "text": "Attach a cost to constraint\n\nAlternative optimization problem\n\n\\[\n\\mathrm{min}_{\\mathbf{x}} f(\\mathbf{x}) + \\lambda g(\\mathbf{x})\n\\]\n\nSuppose that if we don’t use all capacity, we can get some money back\n\nrenting unused warehouse space,\nselling freight capacity\nreducing headcount"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#multiple-constraints",
    "href": "meetups/Meetup-12/meetup-12.html#multiple-constraints",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Multiple Constraints",
    "text": "Multiple Constraints\n\nIf we have multiple constraints, each one has a lambda: \\[\n\\min_{x}f(\\mathbf{x}) + \\sum_{i}\\lambda_i g_i(\\mathbf{x})\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#lagrange-dual",
    "href": "meetups/Meetup-12/meetup-12.html#lagrange-dual",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Lagrange Dual",
    "text": "Lagrange Dual\n\nLet \\(f_d(\\lambda) = \\min_{\\mathbf{x}}f(\\mathbf{x}) + \\lambda g(\\mathbf{x})\\)\n\\(f_d(\\lambda)\\) is the optimal profit for a given cost \\(\\lambda\\)\n\\(f_d(\\lambda)\\) is a concave function of \\(\\lambda\\)\n\\(f_d(\\lambda)\\) is called the Lagrange Dual function of the optimization problem\n\nSoon will generalize it to equality constraints"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#lower-bound-on-original-problem",
    "href": "meetups/Meetup-12/meetup-12.html#lower-bound-on-original-problem",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Lower bound on original problem",
    "text": "Lower bound on original problem\n\nLet \\(p^{\\star}\\) be the solution of original problem: \\[\np^{\\star} = \\mathrm{min}_{\\mathbf{x}} f(\\mathbf{x}),\\quad g(\\mathbf{x})\\leq 0\n\\]\nLet \\(\\mathbf{x}^{\\star}\\) be the value of the decision variables at the minimum \\[ p^{\\star} = f(\\mathbf{x}^{\\star}) \\geq \\cdots\\\\\nf(\\mathbf{x}^{\\star}) + \\lambda g(\\mathbf{x}^{\\star}) \\geq f_d(\\lambda)  \n\\]\nLagrange dual for any value of \\(\\lambda\\) is a lower bound on original (aka primal) problem"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#generalization-to-equality-constraints",
    "href": "meetups/Meetup-12/meetup-12.html#generalization-to-equality-constraints",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Generalization to equality constraints",
    "text": "Generalization to equality constraints\n\nFor inequality constraints, \\(\\lambda \\geq 0\\)\nFor equality constraints, multiply by \\(\\nu\\): \\[\nf_d(\\lambda,\\nu) = \\min_{\\mathbf{x}} f(\\mathbf{x}) +\\sum_{i}\\lambda_i g_i(\\mathbf{x}) + \\sum_j\n\\nu_j h_j(\\mathbf{x})\n\\]\nIntuition: equality constraint is like inequality constraint for \\(h\\) and \\(-h\\).\n\\(\\nu\\) can have either sign"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#lagrangian-function",
    "href": "meetups/Meetup-12/meetup-12.html#lagrangian-function",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Lagrangian Function",
    "text": "Lagrangian Function\n\nTheory of Lagrange Multipliers: \\[\nL(\\mathbf{x},\\lambda,\\nu) = f(\\mathbf{x}) +\\sum_{i}\\lambda_i g_i(\\mathbf{x}) + \\sum_j\n\\nu_j h_j(\\mathbf{x})\n\\]\nThis is called the Lagrangian function"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#dual-optimization-problem",
    "href": "meetups/Meetup-12/meetup-12.html#dual-optimization-problem",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Dual Optimization Problem",
    "text": "Dual Optimization Problem\n\nIf \\(f_d(\\lambda,\\nu)\\) provides a lower bound on the optimal value of the primal problem, makes sense to find the greatest lower bound\nDual optimization problem: \\[\np^{\\star}_d = \\max_{(\\lambda,\\nu)} f_d(\\lambda,\\nu) \\\\\n\\lambda_i \\geq 0\n\\]\n\\(f_d\\) concave, taking max is analogous to solving a convex optimization problem"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\nSuppose we have a two player game\nPlayer 1 will choose prices \\(\\lambda\\) and \\(\\nu\\)\nPlayer 2 then picks action \\(x\\)\nGoal of Player 2 is to maximize their own profit\nGoal of Player 1 is to minimize Player 2s profit"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-1",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\nConsider the following optimization problem \\[\n\\min_{(x,y)} (x+2)^2+(y-1)^2\\,\\\\\nx-y \\leq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-2",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-2",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\nPlayer 2 picks price \\(\\lambda\\)\nPlayer 1 picks action \\(x\\) to minimize \\[\n\\min_{(x,y)} ( (x+2)^2 + (y-1)^2 + \\lambda(x-y))\n\\]\nPlayer 1 wants this function as big as possible\nPlayer 2 wants this function as small as possible"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-3",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-3",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-4",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-4",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-5",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-5",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\lambda = 0\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-6",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-6",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\lambda = 0\\)\n\\(p=0\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-7",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-7",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\lambda = 5\\)\n\\(p=2.5\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-8",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-8",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\lambda =2\\)\n\\(p=4\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#game-interpretation-9",
    "href": "meetups/Meetup-12/meetup-12.html#game-interpretation-9",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Game Interpretation",
    "text": "Game Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\lambda = 3\\)\n\\(p=4.5\\)\nMaximum\nMin of original prob"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#strong-duality",
    "href": "meetups/Meetup-12/meetup-12.html#strong-duality",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Strong Duality",
    "text": "Strong Duality\n\nWhen original problem is convex, primal and dual problems have same minimum\n\nSome rare cases when not true\nSee section on Slater’s Condition\n\nCVX is a primal/dual solver\n\nSimultaneously solves both the primal and dual problem"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#how-to-find-the-dual",
    "href": "meetups/Meetup-12/meetup-12.html#how-to-find-the-dual",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "How to find the dual",
    "text": "How to find the dual\n\nConsider previous problem\nAfter solving, prob.solution.dual_vars\n\n\nx_var = cvx.Variable()\ny_var = cvx.Variable()\n\nobj = cvx.square(x_var+2) + cvx.square(y_var-1)\nconstr = -x_var+y_var\nprob = cvx.Problem(cvx.Minimize(obj),[constr&lt;=0])\nprint(prob.solve())\nprint(prob.solution.dual_vars)\n\n4.5\n{248: array(3.)}"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-dual-shadow-prices",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-dual-shadow-prices",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Dual: Shadow Prices",
    "text": "Meaning of Dual: Shadow Prices\n\nEconomic Interpretation:\n\nPrices \\(\\mu\\) and \\(\\lambda\\) where the firm doesn’t gain from violating constraints\nGain in objective from relaxing the constraints: \\[\n\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\\ng_i(\\mathbf{x}) - u_i \\leq 0 \\\\\nh_j(\\mathbf{x}) - v_j = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#shadow-prices",
    "href": "meetups/Meetup-12/meetup-12.html#shadow-prices",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Shadow Prices",
    "text": "Shadow Prices\n\nThen: \\[\n\\lambda_i = \\frac{\\partial p^{\\star}}{\\partial u_i} \\\\\n\\mu_j = \\frac{\\partial p^{\\star}}{\\partial v_j}\n\\]\nThe shadow prices tell us the sensitivity to constraints\nComplimentary Slackness: -\\(\\lambda_i = 0\\) means inequality is inactive"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#oil-refining-example",
    "href": "meetups/Meetup-12/meetup-12.html#oil-refining-example",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Oil Refining Example",
    "text": "Oil Refining Example"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#oil-refining-example-1",
    "href": "meetups/Meetup-12/meetup-12.html#oil-refining-example-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Oil Refining Example",
    "text": "Oil Refining Example\n\nTwo types of crude oil feedstock exist\nEach can be refined\n\nProcess yields 4 products\nProcess has limit\nEach product has a limit\n\nProducts and Crude have prices"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#oil-refining-example-2",
    "href": "meetups/Meetup-12/meetup-12.html#oil-refining-example-2",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Oil Refining Example",
    "text": "Oil Refining Example\n\\[\n\\max_{\\mathbf{c}} \\left(\\underbrace{\\mathbf{p}_{\\mathrm{prod}}^T Y\\mathbf{c}}_{\\mathrm{profit}}\n-\\underbrace{\\left(\\mathbf{p}_{\\mathrm{proc}}^T+\\mathbf{p}_{\\mathrm{crude}}^T\\right)\\mathbf{c}}_{\\mathrm{costs}}\n\\right) \\\\\n0\\preceq\\mathbf{c}\\preceq \\mathbf{c}_{\\mathrm{max}},\\quad \\mathrm{crude\\,\\, capacity} \\\\\n0 \\preceq Y\\mathbf{c} \\preceq \\mathbf{L}_{\\mathrm{prod}}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#oil-refining-products",
    "href": "meetups/Meetup-12/meetup-12.html#oil-refining-products",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Oil Refining Products",
    "text": "Oil Refining Products\n\n\n\n\n\n\n\n\n\ncapacity\nprice\n\n\n\n\ngasoline\n24000\n108\n\n\nkerosine\n2000\n72\n\n\nfuel oil\n6000\n63\n\n\nresidual\n2500\n30"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#oil-refining-examples",
    "href": "meetups/Meetup-12/meetup-12.html#oil-refining-examples",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Oil Refining Examples",
    "text": "Oil Refining Examples\n\nCrude prices and constraints\n\n\n\n\n\n\n\n\n\n\navailable\nprice\nprocess_cost\n\n\n\n\ncrude 1\n28000.0\n72.0\n1.5\n\n\ncrude 2\n15000.0\n45.0\n3.0\n\n\n\n\n\n\n\n\nProcess Yields and Costs\n\n\n\n\n\n\n\n\n\n\ngasoline\nkerosine\nfuel oil\nresidual\n\n\n\n\ncrude 1\n80\n5\n10\n5\n\n\ncrude 2\n44\n10\n36\n10"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#solving-the-problem",
    "href": "meetups/Meetup-12/meetup-12.html#solving-the-problem",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\n# decision variables, this is a vector of length 2\nx = cvx.Variable(len(crudes.index), pos=True, name=\"crudes\")\n\n# the objective will be the revenue minus costs\n\n# The production is the amount of each crude times the yield matrix\nprods = yields.to_numpy().T @ x / 100.0\n\n# To find revenue we sum up the production times the price of each crude\n\nrevenue = products[\"price\"].to_numpy().T @ prods\n\n# Feed costs are the crude prices times the decision variable, summed up\nfeed_cost = crudes[\"price\"].to_numpy().T @ x\n\n# Ditto for the process costs\n\nprocess_cost = crudes[\"process_cost\"].to_numpy().T @ x\n\n# profit \n\nprofit = revenue - feed_cost - process_cost\nobjective = cvx.Maximize(profit)\n\n# constraints: can't use more crude than you have\nfeeds = x &lt;= crudes[\"available\"].to_numpy()\n\n# Can't produce more of each product than the capacity\n\ncapacity = prods &lt;= products[\"capacity\"].to_numpy()\n\n# put them together to get all the constraints\n\nconstraints = [feeds, capacity]\n\n# solve using cvx\n\nproblem = cvx.Problem(objective, constraints)\nproblem.solve()\n\n860275.8618582088"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#results",
    "href": "meetups/Meetup-12/meetup-12.html#results",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Results",
    "text": "Results\n\nCrude Consumption\n\n\n\n\n\n\n\n\n\n\navailable\nprice\nprocess_cost\nconsumption\nshadow price\n\n\n\n\ncrude 1\n28000.0\n72.0\n1.5\n26206.9\n0.0\n\n\ncrude 2\n15000.0\n45.0\n3.0\n6896.6\n0.0\n\n\n\n\n\n\n\n\nProduct Capacity\n\n\n\n\n\n\n\n\n\n\ncapacity\nprice\nproduction\nunused capacity\nshadow price\n\n\n\n\ngasoline\n24000\n108\n24000.0\n0.0\n14.0\n\n\nkerosine\n2000\n72\n2000.0\n0.0\n262.6\n\n\nfuel oil\n6000\n63\n5103.4\n896.6\n0.0\n\n\nresidual\n2500\n30\n2000.0\n500.0\n0.0"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#kerosine-shadow-price",
    "href": "meetups/Meetup-12/meetup-12.html#kerosine-shadow-price",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Kerosine Shadow Price",
    "text": "Kerosine Shadow Price"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#improving-profits",
    "href": "meetups/Meetup-12/meetup-12.html#improving-profits",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Improving Profits",
    "text": "Improving Profits\n\nIn current environment, improve Kerosine capacity\n\nEvery unit increase in Kerosine adds $262.6 in profit\n\nWhat if we doubled Kerosine capacity?\n\nLinear estimate is a global upper bound on profit\nWould be a lower bound for convex\nGain is at most 525000"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#double-capacity",
    "href": "meetups/Meetup-12/meetup-12.html#double-capacity",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Double Capacity",
    "text": "Double Capacity\n\n\n\n\n\n\n\n\n\navailable\nprice\nprocess_cost\nconsumption\nshadow price\n\n\n\n\ncrude 1\n28000.0\n72.0\n1.5\n24590.2\n0.0\n\n\ncrude 2\n15000.0\n45.0\n3.0\n9836.1\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncapacity\nprice\nproduction\nunused capacity\nshadow price\n\n\n\n\ngasoline\n24000\n108\n24000.0\n-0.0\n22.6\n\n\nkerosine\n4000\n72\n2213.1\n1786.9\n0.0\n\n\nfuel oil\n6000\n63\n6000.0\n-0.0\n62.4\n\n\nresidual\n2500\n30\n2213.1\n286.9\n0.0\n\n\n\n\n\n\n\nTotal Profit:  916229.508223\n\n\n\nDid not reach expected profit target"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#double-capacity-1",
    "href": "meetups/Meetup-12/meetup-12.html#double-capacity-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Double Capacity",
    "text": "Double Capacity"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#robustness-to-shocks",
    "href": "meetups/Meetup-12/meetup-12.html#robustness-to-shocks",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Robustness to Shocks",
    "text": "Robustness to Shocks\n\nWhat if Crude 1 wasn’t available?\n\n\ncrudes = pd.DataFrame(\n    {\n        \"crude 1\": {\"available\": 0, \"price\": 72, \"process_cost\": 1.5},\n        \"crude 2\": {\"available\": 15000, \"price\": 45, \"process_cost\": 3},\n    }\n).T\n\n# constraints\nfeeds = x &lt;= crudes[\"available\"].to_numpy()\ncapacity = prods &lt;= products[\"capacity\"].to_numpy()\nconstraints = [feeds, capacity]\n\n# solution\nproblem = cvx.Problem(objective, constraints)\nproblem.solve()\n\n486000.0001304866"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#robustness-to-shocks-1",
    "href": "meetups/Meetup-12/meetup-12.html#robustness-to-shocks-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Robustness to Shocks",
    "text": "Robustness to Shocks\n\nWe are left with tons of spare capacity\n\n\n\n\n\n\n\n\n\n\navailable\nprice\nprocess_cost\nconsumption\nshadow price\n\n\n\n\ncrude 1\n0.0\n72.0\n1.5\n0.0\n26.6\n\n\ncrude 2\n15000.0\n45.0\n3.0\n15000.0\n32.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncapacity\nprice\nproduction\nunused capacity\nshadow price\n\n\n\n\ngasoline\n24000\n108\n6600.0\n17400.0\n0.0\n\n\nkerosine\n4000\n72\n1500.0\n2500.0\n0.0\n\n\nfuel oil\n6000\n63\n5400.0\n600.0\n0.0\n\n\nresidual\n2500\n30\n1500.0\n1000.0\n0.0"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#capacity-limits",
    "href": "meetups/Meetup-12/meetup-12.html#capacity-limits",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Capacity Limits?",
    "text": "Capacity Limits?\n\nIf any capacity is cut to 0, profit cut to 0 as well\n\n\n\n6.8994302405939854e-09\n\n\n\nInstead we will see what happens with 80% declines in capacity\nLoop over the four products and study the impact"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#resilience-to-capacity-shocks",
    "href": "meetups/Meetup-12/meetup-12.html#resilience-to-capacity-shocks",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Resilience to Capacity Shocks",
    "text": "Resilience to Capacity Shocks\n\n# Initialize products data frame with decline\nproducts[\"obj_loss\"] = 0.0\n\n# Start with a loop over each crude product\n\nfor product in products.index:   \n  # Make a deep copy of the data\n  # This lets us change elements without \n  # overwriting the original data\n  products_2 = products.copy()\n\n  # Now we are going to reduce the capacity by 80%\n  products_2[\"capacity\"].at[product] = 0.2*products[\"capacity\"].at[product]\n  \n  # We are ready to solve the optimization problem\n  feeds = x &lt;= crudes[\"available\"].to_numpy()\n  capacity = prods &lt;= products_2[\"capacity\"].to_numpy()\n  constraints = [feeds, capacity]\n  # solution\n  problem = cvx.Problem(objective, constraints)\n  obj = problem.solve()\n  # Now we compute the relative decrease and save it\n  products[\"obj_loss\"].at[product] = obj/original_obj"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#resilience-to-capacity-shocks-1",
    "href": "meetups/Meetup-12/meetup-12.html#resilience-to-capacity-shocks-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Resilience to Capacity Shocks",
    "text": "Resilience to Capacity Shocks\n\nLet’s look at the objective declines:\n\n\n\ngasoline    0.410862\nkerosine    0.225974\nfuel oil    0.338961\nresidual    0.282468\nName: obj_loss, dtype: float64\n\n\n\nLots of resilience to gasoline capacity loss\n\nInteresting since the inequality was active\n\nVery little resilience to kersoine capacity loss\n\nDecrease almost linear in capacity"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#can-duals-be-computed",
    "href": "meetups/Meetup-12/meetup-12.html#can-duals-be-computed",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Can duals be computed?",
    "text": "Can duals be computed?\n\nSo far the dual problem has been abstract\nInvolves a minimum function: \\[\n\\max_{(\\lambda,\\nu)} \\min_{\\mathbf{x}} L(\\mathbf{x},\\lambda,\\nu)\n\\]\n\nCan we ever write out the dual problem explicitly?\n\nSometimes"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#dual-with-linear-constraints",
    "href": "meetups/Meetup-12/meetup-12.html#dual-with-linear-constraints",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Dual with Linear Constraints",
    "text": "Dual with Linear Constraints\n\nConsider an optimization problem with linear constraints: \\[\n\\min_{\\mathbf{x}} f(\\mathbf{x}) \\\\\nA\\mathbf{x} \\preceq \\mathbf{b} \\\\\nC\\mathbf{x} = \\mathbf{d}\n\\]\nDual Objective: \\[\n\\max_{(\\lambda,\\nu)} \\min_{\\mathbf{x}}\\left( f(\\mathbf{x}) + \\lambda^T(A\\mathbf{x}-\\mathbf{b}) + \\nu^T\\left(C\\mathbf{x}-\\mathbf{d}\\right)\\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#dual-and-convex-conjugate",
    "href": "meetups/Meetup-12/meetup-12.html#dual-and-convex-conjugate",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Dual and Convex Conjugate",
    "text": "Dual and Convex Conjugate\n\nCan regroup the dual objective: \\[\n\\max_{(\\lambda,\\nu)}\\left( -\\lambda^T\\mathbf{b} -\\nu^T\\mathbf{d} +\\min_{\\mathbf{x}}\\left((\\lambda^TA+\\nu^TC)\\mathbf{x}+f(\\mathbf{x}) \\right) \\right)\n\\]\nFor some functions \\(f\\) we can calculate the term inside the minimum\nIt is called the convex conjugate of \\(f\\): \\[\nf^*(\\gamma) = \\max_{\\mathbf{x}} \\gamma^T\\mathbf{x} - f(\\mathbf{x})\n\\]"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#dual-and-convex-conjugate-1",
    "href": "meetups/Meetup-12/meetup-12.html#dual-and-convex-conjugate-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Dual and Convex Conjugate",
    "text": "Dual and Convex Conjugate\n\nIn terms of Convex Conjugate \\[\n\\max_{(\\lambda,\\nu)}-\\lambda^T\\mathbf{b} -\\nu^T\\mathbf{d} - f^*(-A^T\\lambda-C^T\\nu) \\\\\n\\lambda \\succeq 0 \\\\\n\\]\nSo if we can compute convex conjugate of objective, can actually write dual problem"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Convex Conjugate",
    "text": "Meaning of Convex Conjugate"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-1",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Convex Conjugate",
    "text": "Meaning of Convex Conjugate"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-2",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-2",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Convex Conjugate",
    "text": "Meaning of Convex Conjugate"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-3",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-3",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Convex Conjugate",
    "text": "Meaning of Convex Conjugate"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-4",
    "href": "meetups/Meetup-12/meetup-12.html#meaning-of-convex-conjugate-4",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Meaning of Convex Conjugate",
    "text": "Meaning of Convex Conjugate"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#properties-of-convex-conjugate",
    "href": "meetups/Meetup-12/meetup-12.html#properties-of-convex-conjugate",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Properties of Convex Conjugate",
    "text": "Properties of Convex Conjugate\n\nIt is a convex function\nIf \\(f\\) is convex, \\((f^*)^*=f\\)\nIf function is concave, it is undefined"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#convex-conjugate-of-some-functions",
    "href": "meetups/Meetup-12/meetup-12.html#convex-conjugate-of-some-functions",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Convex Conjugate of Some Functions",
    "text": "Convex Conjugate of Some Functions\n\nAffine Function:\n\\(f(x) = ax + b\\)\n\\(\\mathrm{sup}_x (xy - ax - b)\\) is infinity unless the \\(x\\) cancels.\n\\(f^*(y)=b\\), domain is \\(y\\in\\{a\\}\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#convex-conjugate-of-some-functions-1",
    "href": "meetups/Meetup-12/meetup-12.html#convex-conjugate-of-some-functions-1",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Convex Conjugate of Some Functions",
    "text": "Convex Conjugate of Some Functions\n\nExponential Function:\n\\(f(x) = e^x\\)\n\\(\\mathrm{sup}_x (xy - e^x)\\)\nif \\(y&lt;0\\) this goes to infinity for negative \\(x\\)\nif \\(y&gt;0\\) solve equation \\(y = e^x\\) so \\(x=\\log(y)\\)\n\\(f^*(y) = y\\log(y) - y\\)\n\\(f^*(0) = 0\\)"
  },
  {
    "objectID": "meetups/Meetup-12/meetup-12.html#thanks",
    "href": "meetups/Meetup-12/meetup-12.html#thanks",
    "title": "DATA 609 Meetup 12: Duality",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#week-summary",
    "href": "meetups/Meetup-10/meetup-10.html#week-summary",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Week Summary",
    "text": "Week Summary\n\nStarting a two week block on applications to stats/ML\n\nChapter 6, 7 of cvxbook\n\nFinish off convex optimization with duality\nLab 5 due this Sunday at midnight"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#norm-approximation",
    "href": "meetups/Meetup-10/meetup-10.html#norm-approximation",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Norm Approximation",
    "text": "Norm Approximation\n\nFind coefficients \\(\\mathbf{x}\\) that minimize: \\(\\|A\\mathbf{x}-\\mathbf{b}\\|\\)\nHere \\(\\|\\cdot\\|\\) is any norm\nCan have convex constraints on \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#a-norm-is-a-mathematical-measure-of-distance",
    "href": "meetups/Meetup-10/meetup-10.html#a-norm-is-a-mathematical-measure-of-distance",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "A norm is a mathematical measure of distance",
    "text": "A norm is a mathematical measure of distance\n\nSeveral ways to do it, share key properties\n\nShortest distance is straight line (triangle inequality): \\[\n\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|x\\| + \\|y\\|\n\\]"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#a-norm-is-a-mathematical-measure-of-distance-1",
    "href": "meetups/Meetup-10/meetup-10.html#a-norm-is-a-mathematical-measure-of-distance-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "A norm is a mathematical measure of distance",
    "text": "A norm is a mathematical measure of distance\n\nSeveral ways to do it, share key properties\n\nShortest distance is straight line (triangle inequality): \\[\n\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|x\\| + \\|y\\|\n\\]\nPositive \\(\\|\\mathbf{x}\\|\\geq 0\\)\nHomogeneous \\(\\|s\\mathbf{x}\\| = |s|\\|\\mathbf{x}\\|\\)\n\nNorms are convex"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#important-norms",
    "href": "meetups/Meetup-10/meetup-10.html#important-norms",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Important Norms",
    "text": "Important Norms\n\nEuclidean or 2-Norm: \\(\\|\\mathbf{x}\\|_2 = \\left(\\sum_i x_i^2\\right)^{1/2}\\)\n\\(L_1\\) Norm: $||_1 = _i |x_i|\n\\(L_{\\infty}\\) Norm: \\(\\|\\mathbf{x}\\|_{\\infty} = \\mathrm{\\max}_i \\{x_i\\}\\)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms",
    "href": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Level Sets of Norms",
    "text": "Level Sets of Norms\n\n\n\n\\(L_1\\) norm sometimes called taxicab distance\nDark Lines on Axes cause sparsity in applications"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms-1",
    "href": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Level Sets of Norms",
    "text": "Level Sets of Norms\n\n\n\nEuclidean Distance\nLeast Squares"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms-2",
    "href": "meetups/Meetup-10/meetup-10.html#level-sets-of-norms-2",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Level Sets of Norms",
    "text": "Level Sets of Norms\n\n\n\n\\(L_{\\infty}\\) measures maximum element"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#penalty-functions",
    "href": "meetups/Meetup-10/meetup-10.html#penalty-functions",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Penalty Functions",
    "text": "Penalty Functions\n\nCan generalize this further to sum of convex penalty functions \\[\n\\min_{x} \\sum_{i} \\phi\\left((A\\mathbf{x}-b)_i\\right)\n\\]\npenalty function \\(\\phi \\geq 0\\)\n\\(\\phi(0)=0\\)\n\\(\\phi\\) increasing for argument larger than \\(0\\), decreasing when it is smaller than \\(0\\)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#important-penalty-functions",
    "href": "meetups/Meetup-10/meetup-10.html#important-penalty-functions",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Important Penalty Functions",
    "text": "Important Penalty Functions\n\n\\(\\phi(x) = |x|\\), \\(\\phi(x) = x^2\\) lead to norm problems\n\\(\\phi(x) = \\mathrm{\\max}\\{0,|x|-a\\}\\) dead-zone linear\n\nIf you are close enough no penalty, otherwise linear\n\n\\(\\phi(x) = -\\log(1-x^2)\\), log-barrier penalty\n\nBlows up at \\(x=\\pm 1\\)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#visualizing-penalties",
    "href": "meetups/Meetup-10/meetup-10.html#visualizing-penalties",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Visualizing Penalties",
    "text": "Visualizing Penalties"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#robustness-to-outliers",
    "href": "meetups/Meetup-10/meetup-10.html#robustness-to-outliers",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Robustness to Outliers",
    "text": "Robustness to Outliers\n\nAn outlier is a data point that was generated by a different process than the other points in the dataset\n\nCould be due to an interesting phenomenon\nCould be due to a mistake, in which case it is called a spurious value\n\nProblem: Stats based on Gaussian Distribution (and thus quadratic penalty) are not robust to outliers"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#linear-regression-examples",
    "href": "meetups/Meetup-10/meetup-10.html#linear-regression-examples",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Linear Regression Examples",
    "text": "Linear Regression Examples"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#robust-regression",
    "href": "meetups/Meetup-10/meetup-10.html#robust-regression",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Robust Regression",
    "text": "Robust Regression\n\nQuadratic penalty squares residuals\nPenalizes large errors much more than small ones\nAbsolute value penalizes residuals equally, so less sensitive to outliers\nRobust Regression is \\(L_1\\) penalized linear regression"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#robust-regression-1",
    "href": "meetups/Meetup-10/meetup-10.html#robust-regression-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Robust Regression",
    "text": "Robust Regression"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#huber-penalty",
    "href": "meetups/Meetup-10/meetup-10.html#huber-penalty",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Huber Penalty",
    "text": "Huber Penalty\n\nQuadratic penalty is optimal when we have Gaussian errors\nHuber function is a compromise between quadratic and absolute value: \\[\n\\phi_h(x) = \\begin{cases}\nx^2 & \\mathrm{if}\\quad |x|&lt; M \\\\\nM\\left(2|x|-M\\right) & \\mathrm{if}\\quad |x|\\geq M\n\\end{cases}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#huber-penalty-1",
    "href": "meetups/Meetup-10/meetup-10.html#huber-penalty-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Huber Penalty",
    "text": "Huber Penalty"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#huber-regression",
    "href": "meetups/Meetup-10/meetup-10.html#huber-regression",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Huber Regression",
    "text": "Huber Regression\n\nHuber Penalty also leads to robust fits\nShould pick \\(M\\) based on a robust estimate of the standard deviation \\(\\sigma\\)\nWant the quadratic part to be in force when errors produced by Gauassian\nWant linear part for larger errors"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#huber-regression-1",
    "href": "meetups/Meetup-10/meetup-10.html#huber-regression-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Huber Regression",
    "text": "Huber Regression"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#least-norm-problems",
    "href": "meetups/Meetup-10/meetup-10.html#least-norm-problems",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Least Norm Problems",
    "text": "Least Norm Problems\n\\[\n\\mathrm{min} \\|\\mathbf{x}\\| \\\\\nA\\mathbf{x} - \\mathbf{b} = 0\n\\]\n\nEquivalent to norm approximation\n\nSystem is underdetermined\n\\(\\mathbf{x} = \\mathbf{x}_0 + Z\\mathbf{u}\\)\n\\(\\mathrm{min}_{\\mathbf{u}}\\|\\mathbf{x}_0 + Z\\mathbf{u}\\|\\)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#regularization-and-sparsity",
    "href": "meetups/Meetup-10/meetup-10.html#regularization-and-sparsity",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Regularization and Sparsity",
    "text": "Regularization and Sparsity\n\nLeast Norm lets you “solve” underdetermined linear systems\nWe have also discussed regularization: \\[\n\\mathrm{min}_{\\mathbf{x}} \\|f(\\mathbf{x})\\| + \\lambda\\|\\mathbf{x}\\|\n\\]\nWhat are the trade-offs between different penalties?"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#l_1-induces-sparsity-and-bias",
    "href": "meetups/Meetup-10/meetup-10.html#l_1-induces-sparsity-and-bias",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "\\(L_1\\) induces sparsity and bias",
    "text": "\\(L_1\\) induces sparsity and bias\n\nBig difference between quadratic and abs penalty when coefficients are small:\n\\(|x| \\gg x^2\\)\nLasso induces sparsity- actually zeros out coefficients with small effect\nSparsity can give big computational efficiency advantage and improve interpretability\nCan lead to bias (inaccuracy)"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#comparison-on-diabetes-dataset",
    "href": "meetups/Meetup-10/meetup-10.html#comparison-on-diabetes-dataset",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Comparison On Diabetes Dataset",
    "text": "Comparison On Diabetes Dataset\n\nPredictor variables: age, sex, bmi, bp, and six blood test values taken at baseline\nTarget variable: Measure of diabetes progression one year later"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#comparisons-on-diabetes-dataset",
    "href": "meetups/Meetup-10/meetup-10.html#comparisons-on-diabetes-dataset",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Comparisons on Diabetes Dataset",
    "text": "Comparisons on Diabetes Dataset"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#ridge-regression",
    "href": "meetups/Meetup-10/meetup-10.html#ridge-regression",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Ridge Regression",
    "text": "Ridge Regression"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#ridge-regression-1",
    "href": "meetups/Meetup-10/meetup-10.html#ridge-regression-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nIn ridge regression the coefficients shrink together\n\n\n\nText(0.5, 0, 'log(lambda)')"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#lasso-regression",
    "href": "meetups/Meetup-10/meetup-10.html#lasso-regression",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Lasso Regression",
    "text": "Lasso Regression"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#lasso-regression-1",
    "href": "meetups/Meetup-10/meetup-10.html#lasso-regression-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Lasso Regression",
    "text": "Lasso Regression\n\nCoefficients forced to 0 one at a time"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#stochastic-robust-approximation",
    "href": "meetups/Meetup-10/meetup-10.html#stochastic-robust-approximation",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Stochastic Robust Approximation",
    "text": "Stochastic Robust Approximation\n\nStart off with a problem where we are minimizing a convex objective\nCould be least squares type like the one we write below, but doesn’t have to be: \\[\n\\mathrm{min}_x\\|A\\mathbf{x}-\\mathbf{b}\\|\n\\]\nSuppose that \\(A\\) and \\(\\mathbf{b}\\) are actually uncertain\nCould correspond to things like measurement error"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#stochastic-robust-approximation-1",
    "href": "meetups/Meetup-10/meetup-10.html#stochastic-robust-approximation-1",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Stochastic Robust Approximation",
    "text": "Stochastic Robust Approximation\n\nReformulate problem, minimize expected value of objective: \\[\n\\min_{x}\\int dA d\\mathbf{b}\\left( p(A,\\mathbf{b})\\|A\\mathbf{x}-\\mathbf{b}\\| \\right)\n\\]\nThis is convex, but viciously hard\nGenerally will have to approximate the integral\nEquivalent to discretizing distribution"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#discrete-version",
    "href": "meetups/Meetup-10/meetup-10.html#discrete-version",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Discrete Version",
    "text": "Discrete Version\n\nConsider Discrete Version \\[\n\\mathrm{min}_{\\mathbf{x}} \\sum_{i}\\sum_j p_{ij}\\|A_i\\mathbf{x}-\\mathbf{b}_j\\|\n\\]\nCould be a lot of terms but is convex"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#worst-case-robust-optimization",
    "href": "meetups/Meetup-10/meetup-10.html#worst-case-robust-optimization",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Worst Case Robust Optimization",
    "text": "Worst Case Robust Optimization\n\nIdea: Instead of minimizing expected value, minimize worst case error \\[\n\\mathrm{min}_{\\mathbf{x}} \\mathrm{sup}_{A} \\|A\\mathbf{x}-\\mathbf{b}\\|\n\\]\nConvex, but still can be viciously hard"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#problem-3-on-homework",
    "href": "meetups/Meetup-10/meetup-10.html#problem-3-on-homework",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Problem 3 on Homework",
    "text": "Problem 3 on Homework\n\nStart with logistic regression: \\[\nl(\\theta) = \\sum_{i=1}^n \\log\\left(1 + \\exp\\left(-y_i\\theta^T\\mathbf{x}_i\\right)\\right)\n\\]\n\\(y_i = \\pm 1\\)\n\\(\\theta\\) coefficients\n\\(\\mathbf{x}_i\\) is \\(i\\)th data point"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#problem-3-tips",
    "href": "meetups/Meetup-10/meetup-10.html#problem-3-tips",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Problem 3 Tips",
    "text": "Problem 3 Tips\n\nNow we assume that each \\(\\mathbf{x}_i\\) has an error \\(\\delta_i\\) which has maximal element of \\(\\epsilon\\)\nNew Convex Optimization Problem:\n\n\\[\nl_{wc}(\\theta) = \\sum_{i=1}^n \\sup_{\\|\\delta_i\\|_{\\infty}\\leq\\epsilon}\\log\\left(1+\\exp\\left(-y_i\\theta^T\\left(\\mathbf{x}_i+\\delta_i\\right)\\right)\\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#how-to-deal-with-this",
    "href": "meetups/Meetup-10/meetup-10.html#how-to-deal-with-this",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "How to deal with this?",
    "text": "How to deal with this?\n\\[\nl_{wc}(\\theta) = \\sum_{i=1}^n \\sup_{\\|\\delta_i\\|_{\\infty}\\leq\\epsilon}\\log\\left(1+\\exp\\left(-y_i\\theta^T\\left(\\mathbf{x}_i+\\delta_i\\right)\\right)\\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-10/meetup-10.html#thanks",
    "href": "meetups/Meetup-10/meetup-10.html#thanks",
    "title": "DATA 609 Meetup 10: Applications to Data Fitting",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "images/IntroSlides.html#about-me",
    "href": "images/IntroSlides.html#about-me",
    "title": "Welcome to DATA 609",
    "section": "About Me",
    "text": "About Me\n\n\n\nDoctoral Lecturer at CUNY SPS\nPast research experience:\n\nPlasma physics and Applied Mathematics (NYU)\nEcology and Evolutionary Biology (Princeton)\n\nResearch using Bayesian methods and genomic data to improve global scale ocean and climate models\nTaught math at NYU"
  },
  {
    "objectID": "images/IntroSlides.html#why-optimization",
    "href": "images/IntroSlides.html#why-optimization",
    "title": "Welcome to DATA 609",
    "section": "Why Optimization?",
    "text": "Why Optimization?\n\n\n\nOptimization theory underlies most methods in data science\nThree foci:\n\nLeast Squares Optimization\nConvex Optimization\nLocal Optimization and Stochastic Gradient Descent"
  },
  {
    "objectID": "images/IntroSlides.html#course-websites",
    "href": "images/IntroSlides.html#course-websites",
    "title": "Welcome to DATA 609",
    "section": "Course Website(s)",
    "text": "Course Website(s)\n\n\n\nBrightspace to submit homework\nCourse Website"
  },
  {
    "objectID": "images/IntroSlides.html#course-website-and-meetup",
    "href": "images/IntroSlides.html#course-website-and-meetup",
    "title": "Welcome to DATA 609",
    "section": "Course Website and Meetup",
    "text": "Course Website and Meetup\n\n\n\nBrightspace to submit homework\nCourse Website\nCourse Meetup: Monday 6:45-7:45PM\nZoom Link"
  },
  {
    "objectID": "images/IntroSlides.html#slack-channel",
    "href": "images/IntroSlides.html#slack-channel",
    "title": "Welcome to DATA 609",
    "section": "Slack Channel",
    "text": "Slack Channel\n\n\n\nCourse slack channel for messaging and rapid communications\nhttps://data609-spring-2025.slack.com\nInvite Link"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks",
    "href": "images/IntroSlides.html#textbooks",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#textbooks-1",
    "href": "images/IntroSlides.html#textbooks-1",
    "title": "Welcome to DATA 609",
    "section": "Textbooks",
    "text": "Textbooks\n\n\nTwo most important textbooks:\n\nIntroduction to Applied Linear Algebra\n\n\nAvailable free online VMLS\n\n\nConvex Optimization\n\n\nAvailable free online cvxbook"
  },
  {
    "objectID": "images/IntroSlides.html#assignments",
    "href": "images/IntroSlides.html#assignments",
    "title": "Welcome to DATA 609",
    "section": "Assignments",
    "text": "Assignments\n\n\n\n8 Lab Assignments (80%)\nFinal Project (20%)"
  },
  {
    "objectID": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "href": "images/IntroSlides.html#looking-forward-to-a-great-semester",
    "title": "Welcome to DATA 609",
    "section": "Looking forward to a great semester!",
    "text": "Looking forward to a great semester!\nThanks for watching"
  },
  {
    "objectID": "assignments/labs/Lab1.html",
    "href": "assignments/labs/Lab1.html",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#instructions",
    "href": "assignments/labs/Lab1.html#instructions",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "href": "assignments/labs/Lab1.html#problem-1-gradient-descent",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 1: Gradient Descent",
    "text": "Problem 1: Gradient Descent\n\nConsider the mathematical function defined on \\(f: \\mathbb{R}^2\\,\\to\\, \\mathbb{R}\\):\n\n\\[\nf(x,y) = (x-1)^2 + (y+2)^2,\n\\]\nFind the single critical point of this function and show that it is a local minimum (in this case, this will also be a global minimum).\n\nNow consider a new objective function that depends on a parameter \\(b\\): \\[\nf(x,y) = x^2 + by^2\n\\] Here we will look at two different values of \\(b\\), \\(b=3\\) and \\(b=10\\). The global minimum of this function occurs at the point \\(x^* = 0\\), \\(y^*=0\\) no matter what the value of \\(b\\). Suppose that we didn’t know this and wanted to find the minimum of this function using gradient descent instead of direct calculation.\n\n\nFirst write code to perform the gradient descent algorithm, that is perform the iteration: \\[\n\\mathbf{v}_{n+1} = \\mathbf{v}_n - k \\nabla f(\\mathbf{v}_n),\n\\]\n\nwhere the vector \\(\\mathbf{v} = \\begin{bmatrix} x & y\\end{bmatrix}^T\\) and \\(k\\) is the learning rate.\n\nThen test the performance of your algorithm as a function of the learning rates \\(k\\) by performing 100 iterations of the algorithm for 100 values of \\(k\\) equally spaced between \\(k=0.01\\) and \\(k=0.3\\). Start with an initial guess of \\(\\mathbf{v}_0 = \\begin{bmatrix} b & 1\\end{bmatrix}^T\\). Do this for \\(b=3\\) and \\(b=10\\). Make separate plots for \\(b=3\\) and \\(b=10\\) of the log base 10 of the error (in this case it is \\(\\sqrt{x_{100}^2+y_{100}^2}\\)) for the final value of the iteration versus the value of \\(k\\). How does learning rate relate to the final value of the error? For which value of \\(b\\) does the algorithm have the ability to converge fastest (have the lowest value of the error at the end)?\n\nNote: For some combinations of \\(k\\) and \\(b\\), the algorithm won’t converge to the right answer, i.e. the error will grow with time. To make your plot easier to read, don’t plot the error for iterations that didn’t converge.\n\nAs \\(k\\) increases, for one or both values of \\(b\\), you will observe a point where the trend of final error versus learning rate reverses direction. Pick a value of \\(k\\) very close to the point where this occurs, and make a contour plot of the function \\(f\\) and the trajectory of the iterations for the gradient descent algorithm for that value of \\(k\\) superimposed over the contour plot. What do you observe?\n\nNote: The differences that you observe here are a special case of a more general phenomenon: the speed of convergence of gradient descent depends on something called the condition number of the Hessian matrix (the matrix of the 2nd order partial derivatives) of the target function. The condition number for a symmetric matrix is just the ratio of the largest to smallest eigenvalues, in this case the condition number is \\(b\\) (or 1/\\(b\\)). Gradient descent performs worse and worse the larger the condition number (and large condition numbers are problematic for a wide variety of other numerical methods)."
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "href": "assignments/labs/Lab1.html#problem-2-solving-least-squares-problems",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 2: Solving Least Squares Problems",
    "text": "Problem 2: Solving Least Squares Problems\nGenerate a random \\(20\\times 10\\) matrix \\(A\\) and a random 20-vector \\(b\\) (use a Gaussian distribution). Then, solve the least squares problem: \\[\n\\min_{\\mathbf{x}\\in \\mathbb{R}^{10}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\n\\] in the following ways:\n\nMultiply \\(\\mathbf{b}\\) by the Morse-Penrose Pseudoinverse \\(A^+\\).\nUse built in functions to solve the least squares problem (i.e. in python numpy.lstsq, in R lm, and in Julia the backslash operator).\nUsing the \\(QR\\) factorization of \\(A\\). This factorization rewrites \\(A\\) as: \\[\nA = \\begin{bmatrix} Q & 0\\end{bmatrix} \\begin{bmatrix} R & 0 \\end{bmatrix}^T,\n\\] where \\(Q\\) is an orthonormal matrix and \\(R\\) is upper triangular. The least squares solution equals: \\[\n\\mathbf{x} = R^{-1}Q^T\\mathbf{b}\n\\]\nVerify that each of these solutions are nearly equal and that the residuals \\(A\\mathbf{x}-\\mathbf{b}\\) are orthogonal to the vector \\(A\\mathbf{x}\\)"
  },
  {
    "objectID": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "href": "assignments/labs/Lab1.html#problem-3-iterative-solutions-to-least-squares",
    "title": "Homework 1: Introduction to Optimization and Least Squares",
    "section": "Problem 3: Iterative Solutions to Least Squares",
    "text": "Problem 3: Iterative Solutions to Least Squares\nAlthough the pseudoinverse provides an exact formula for the least squares solutions, there are some situations in which using the exact solution is computationally difficult, particularly when the matrix \\(A\\) and vector \\(\\mathbf{b}\\) have a large number of entries. In isn’t out of the ordinary for \\(A^TA\\) to be more than a terabyte, for example . In these cases it may be better to use an approximate solution instead of the exact formula. There are many different approximate methods for solving least squares problems, here we will use an iterative method developed by Richardson.\nThis method begins with an initial guess \\(\\mathbf{x}^{(0)} = 0\\) and calculates successive approximations as follows:\n\\[\n    \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mu A^T\\left(A\\mathbf{x}^{(k)}-\\mathbf{b}\\right)\n\\]\nHere \\(\\mu\\) is a positive paramter that has a similar interpretation to the learning rate for gradient descent. A choice that guarantees convergence is \\(\\mu \\leq \\frac{1}{\\|A\\|}\\). The iteration is terminated when the change in the residual \\(\\|A^T(Ax^{(k)} − b)\\|\\) after successive steps is below a user determined threshold, which indicates that the least squares optimality conditions are nearly satisfied.\n\nSuppose that \\(\\mathbf{x}\\) is a solution to the least squares problem: \\[\n\\mathbf{x} = A^+\\mathbf{b}\n\\]\n\nShow by substitution of the formula for the pseudoinverse that \\(\\mathbf{x}\\) is a fixed-point of the iteration scheme, i.e. that: \\[\n\\mathbf{x} = \\mathbf{x} - \\mu A^T\\left(A\\mathbf{x}-\\mathbf{b}\\right)\n\\]\n\nGenerate a random 20 × 10 matrix \\(A\\) and 20-vector \\(\\mathbf{b}\\), and compute the least squares solution \\(\\mathbf{x} = A^+\\mathbf{b}\\). Then run the Richardson algorithm with \\(\\mu = \\frac{1}{\\|A\\|^2}\\) for 500 iterations, and plot \\(\\|\\mathbf{x}^{(k)}-\\mathbf{x}\\|\\) to verify that \\(\\mathbf{x}^{(k)}\\) is converging to \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "assignments/labs/Lab7.html",
    "href": "assignments/labs/Lab7.html",
    "title": "Homework 7:",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab7.html#instructions",
    "href": "assignments/labs/Lab7.html#instructions",
    "title": "Homework 7:",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab7.html#problem-1-comparing-optimization-algorithms",
    "href": "assignments/labs/Lab7.html#problem-1-comparing-optimization-algorithms",
    "title": "Homework 7:",
    "section": "Problem 1: Comparing Optimization Algorithms",
    "text": "Problem 1: Comparing Optimization Algorithms\nConsider the function \\(f(x,y) = (1-x^2) + 100(y - x^2)^2\\), which has a global minimum at \\(x=1\\), \\(y=1\\). For this problem, you are going to explore using different optimization algorithms to find the global minimum, to gain an intuitive understanding of their different strengths and weaknesses.\n\nMake a contour plot of this function. You should observe a that the contour lines are “banana-shaped” around the global minimum point, which lies in a deep valley. In technical terms, we would say that the gradient of this function is strongly anisotropic, a fact that can cause slow or no convergence for optimization algorithms.\nIn the code chunk below I have python code for three different optimization algorithms,(1) stochastic gradient descent; (2) stochastic gradient descent with momentum, and (3) ADAM (ADAptive Moment Estimation). Starting at the initial point \\(x=-4\\), \\(y=-2\\), use each algorithm to find the minimum of the function \\(f\\) given above. Start with a learning rate of \\(\\kappa = 10^{-4}\\) for all three algorithms, and run the algorithm for \\(10^5\\) timesteps. For this problem, maintain the noise level at 0. Plot the trajectories of each algorithm and the \\(\\log\\) base 10 of the error rate as a function of the time step. What do you notice about the performance of the difference algorithms, both in terms of convergence speed and ultimate accuracy?\nPerform the same experiment for the learning rate \\(\\kappa = 10^{-3}\\), only comparing ADAM and gradient descent with momentum. You will likely observe that one of the methods does not converge, keep the same range of values for your trajectory/contour plot as you did in (b). Which method worked better with \\(\\kappa = 10^{3}\\)?\nNow perform a comparison between ADAM with \\(\\kappa=10^{-2}\\) against gradient descent with momentum using \\(\\kappa=10^{-4}\\). What are the trade-offs between the two methods for these values of the learning rate?"
  },
  {
    "objectID": "assignments/labs/Lab7.html#problem-2-escaping-a-saddle",
    "href": "assignments/labs/Lab7.html#problem-2-escaping-a-saddle",
    "title": "Homework 7:",
    "section": "Problem 2: Escaping a Saddle",
    "text": "Problem 2: Escaping a Saddle\nOne of the challenges of optimization of functions of many variables (which we sometimes call “high-dimensional” even if they output a single number/are scalar-valued) is that the estimate of the optimum can become trapped in a local minimum or spend a large amount of time near a saddle point. It is generally not possible to design a method that can always escape a local minimum, but fortunately many machine learning researchers believe that escaping from saddle points is a more relevant issue in real world problems because saddle points are much more common for high-dimensional functions. One of the advantages of stochastic gradient descent over regular gradient descent relates to avoiding saddle points. When training neural networks, the stochasticity of stochastic gradient descent originates from the fact that the algorithm takes steps without based on an estimation of the gradient of the loss function from a mini-batch of data, which induces noise in the gradient. To demonstrate this for a function of a few variables, we have included an explicit noise term in the the solver code that we specified above. For this problem we are going to the gradient descent algorithm and gradient descent with momentum, and explore how they behave when the trajectory is near a saddle-point.\nWe will use the function \\(f(x,y)=-x^2+y^2\\). This function has a saddle point at \\(x=0\\) and \\(y=0\\). This point is also unbounded below, so no algorithm we use will converge to a finite global minimum. You should think of a saddle point as point on a surface which is locally flat, which isn’t the top of a hill or the bottom of a valley. This means that there are some directions from the saddle point where the function will eventually increase, and some directions where it will eventually decrease, just like a saddle which curves up both fore and aft and curves down to the left and right (an alternative would be to imagine a Pringles chip instead).\n\nEither write code to use the gradient descent solvers on the saddle function or adapt the code for the gradient of the function in part (a). Set the noise equal to 0 and the initial point of the trajectory to \\(x=-2\\), \\(y=0\\). Use gradient descent to minimize the function, using \\(N=10^5\\) time steps. Plot the \\(\\log\\) base 10 of the norm of \\((x,y)\\) over time and show that it converges to the saddle point.\n\nIn the real world it would be very unlikely to have the exact starting value \\(y=0\\), which is what causes the the solver to get stuck, and had \\(y\\) been slightly perturbed from \\(0\\) the trajectory would have escaped the saddle point. However in real problems the time to escape can still be quite large and the number of saddle points encountered during learning may also be quite large, so this can impact the overall learning rate.\n\nPerform additional experiments, with values of the noise parameter of \\(0.1\\), \\(0.5\\), and \\(1.5\\). Again plot the trajectories and the \\(\\log\\) base 10 of the norm of \\((x,y)\\) over time. For these problems exit the minimization routine when the absolute value of \\(|y|\\geq 1\\), so that you do not have trajectories blowing up to infinity."
  },
  {
    "objectID": "assignments/labs/Lab7.html#problem-3-shallow-nets-and-mnist",
    "href": "assignments/labs/Lab7.html#problem-3-shallow-nets-and-mnist",
    "title": "Homework 7:",
    "section": "Problem 3: Shallow Nets and MNIST",
    "text": "Problem 3: Shallow Nets and MNIST\nFor this exercise, we will work on one of the standard model problems in Machine Learning, classifying handwritten digits. We will use an adaptation of the neural network code from your reading assignment to pytorch, which is one of the leading frameworks for training neural networks. pytorch is fairly flexible, you can use it with the CPU on your personal computer, with GPUs, and even on computing clusters. If you have trouble getting pytorch to work on your own computer I recommend trying in on google colab, or alternatively I can provide you with similar code written in pure numpy (or you are welcome to develop your own implementation).\nFirst, you should acquire the MNIST dataset. This can be downloaded automatically using pytorch via the following code chunk:\n\n# Here is some code that automatically downlaods the MNIST data. Technically it will also read the\n# data in if you have already downloaded and the path points to the folder where you have the files\n# There will be 4 binary files which together contain the testing and training examples and the labels\n# for the testing and training examples. \n\nfrom torchvision import datasets, transforms\n\n\n\n# Load MNIST\n\n\n# transform defines a function which takes an image file, converts the analog bits into floating point \n# numbers (it's a literal image file in the data), and then flattens the file. Each image is 28x28\n# so at the end we get a 784x1 vector\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n\n# The first line downloads the entire MNIST dataset to the data directory (or whereever you want it)\n# If the data is already there, this won't download it. THis downloads both the training and testing data.\n# the transform keyword applies the transform defined above, the train dataset has 60,000 examples, and\n# the test dataset has 10,000 examples. The train and test data is loaded in the variables.\n\ntrain_dataset = datasets.MNIST('data/', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('data/', train=False, transform=transform)\n\nWe are going to train a simple neural network to classify the MNIST images. The neural network has an input layer of 784 neurons (one for each pixel), a 30 neuron hidden layer, and a 10 neuron output layer. The outputs of the neural network will add to 1 and represent a probabilistic prediction of which digit the input is. We will initially use sigmoidal neurons to process the inputs. I’ve provided a file called neural_network.py which contains code to train a neural network using pytorch. Throughout the rest of the assignment you will use and improve the code in this file to study the performance of different combinations of network structure, optimization algorithm choice, hyperparameters, and activation function.\n\n# This brings in the code in neural_network.py\nimport neural_network\n\n\n# Initialize network- this creates a neural network with 784 input neurons, one hidden layer with 30 neurons, and 10 output neurons (which are processed by a softmax layer)\n# In the initial file these neurons are sigmoidal\n\nnet = Network([784, 30, 10])\n\n# This will train the neural network\n\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.01, test_data=test_loader)\n\n\nThe initial configuration of the neural network uses stochastic gradient descent without momentum. The hyperparameters chosen in the line above are a good initial choice that will lead to a network that can learn. Execute the above code to train this neural network. Keep track of the learning progress over time, which is stored in the sol variable.\n\nNext test how two alternative optimization algorithms perform on this problem with the same network structure by modifying the train function in neural_network.py by replacing the line defining the optimizer with the following two different options:\n\n# Put this in for SGD with momentum using nesterov acceleration:\n\noptimizer = optim.SGD(network.parameters(),momentum=0.8,nesterov=True, lr=eta,weight_decay=0)\n\n# And try this option for the ADAM optimizer:\n\noptimizer = optim.Adam(network.parameters(),betas = (0.9,0.999(, lr=eta,weight_decay=0)\n\n\n\nHow good can you make your 3-layer network?"
  },
  {
    "objectID": "assignments/labs/Lab7.html#problem-4-deep-nets-overcoming-gradients",
    "href": "assignments/labs/Lab7.html#problem-4-deep-nets-overcoming-gradients",
    "title": "Homework 7:",
    "section": "Problem 4: Deep Nets: Overcoming Gradients",
    "text": "Problem 4: Deep Nets: Overcoming Gradients\n\nIn deep networks, the gradients for the neural network weights\n\nExperiment more with hyper-parameters (including the number of neurons in each layer). Can you improve upon the best shallow network that you constructed for problem 3? I had the best results using a structure that steadily reduced the number of neurons from 784 in the input layer to 10 in the output layer."
  },
  {
    "objectID": "assignments/labs.html",
    "href": "assignments/labs.html",
    "title": "Labs",
    "section": "",
    "text": "The homework assignments in this course are lab assignments where you will solve the problem using a combination of mathematics and computer code.\nPlease submit a and either a quarto file or jupyter notebook or other code that generates your homework. Labs should be submitted on Blackboard.\n\n\nGradient Descent and Least Squares (Download)\n\n\nLeast Squares Applications (Download)\n\n\nConvex Sets (PDF) (Quarto Download)\n\n\nConvex Functions (PDF) (Quarto Download)\n\n\nConvex Optimizations Problems (Download)\n\n\nApplications to Statistics and Machine Learning (Download)\n\n\nNonconvex Optimization and Stochastic Gradient Descent (Download)",
    "crumbs": [
      "Assignments",
      "Labs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Announcements",
    "section": "",
    "text": "Be sure to check this page regularly for updates. In addition to periodic updates about the course, slides and recordings of the weekly meetups will be posted here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 13: Non-Convex Optimization\n\n\n\n\n\nClick here to read more about Meetup 13 and view/download the lecture slides.\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13- Optimization Algorithms\n\n\n\n\n\nClick here to read about Week 13.\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 12: Duality\n\n\n\n\n\nClick here to read more about Meetup 12 and view/download the lecture slides.\n\n\n\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12- Duality\n\n\n\n\n\nClick here to read about Week 12.\n\n\n\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11- Stats and Machine Learning\n\n\n\n\n\nClick here to read about Week 11.\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 11: Statistics\n\n\n\n\n\nClick here to read more about Meetup 11 and view/download the lecture slides.\n\n\n\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 10: Data Fitting\n\n\n\n\n\nClick here to read more about Meetup 10 and view/download the lecture slides.\n\n\n\n\n\nApr 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10- Data Fitting\n\n\n\n\n\nClick here to read about Week 10.\n\n\n\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 9: Convex Optimization Problems\n\n\n\n\n\nClick here to read more about Meetup 9 and view/download the lecture slides.\n\n\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9- Convex Optimization Problems\n\n\n\n\n\nClick here to read about Week 9.\n\n\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8- Convex Functions\n\n\n\n\n\nClick here to read about Week 8.\n\n\n\n\n\nMar 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 8: Constructing Convex Functions\n\n\n\n\n\nClick here to read more about Meetup 8 and view/download the lecture slides.\n\n\n\n\n\nMar 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 7: Introduction to Convex Sets\n\n\n\n\n\nClick here to read more about Meetup 7 and view/download the lecture slides.\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7- Convex Functions\n\n\n\n\n\nClick here to read about Week 7.\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 6: Introduction to Convex Sets\n\n\n\n\n\nClick here to read more about Meetup 6 and view/download the lecture slides.\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6- Constructing Convex Sets\n\n\n\n\n\nClick here to read about Week 6.\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5-Convex Sets\n\n\n\n\n\nClick here to read about Week 5.\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 5: Introduction to Convex Sets\n\n\n\n\n\nClick here to read more about Meetup 5 and view/download the lecture slides.\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 4: Least Squares Data Fitting\n\n\n\n\n\nClick here to read more about Meetup 4 and view/download the lecture slides.\n\n\n\n\n\nFeb 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4- Constrained Least Squares\n\n\n\n\n\nClick here to read about Week 3.\n\n\n\n\n\nFeb 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Linear Algebra in R\n\n\n\n\n\nClick here for a video on basic linear algebra in R.\n\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 3: Least Squares Data Fitting\n\n\n\n\n\nClick here to read more about Meetup 3 and view/download the lecture slides.\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3- Data Fitting\n\n\n\n\n\nClick here to read about Week 3.\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Writing Math in Markdown\n\n\n\n\n\nClick here for a 20 minute video on writing math in markdown using Latex.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Vignette- Linear Algebra in Numpy\n\n\n\n\n\nClick here for a video on basic linear algebra in Numpy.\n\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 2: Least Squares Optimization\n\n\n\n\n\nClick here to read more about Meetup 2 and view/download the lecture slides.\n\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2- Least Squares\n\n\n\n\n\nClick here to read about Week 2.\n\n\n\n\n\nFeb 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMeetup 1: Introduction to Optimization\n\n\n\n\n\nClick here to read more about Meetup 1 and view/download the lecture slides.\n\n\n\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DATA 609\n\n\n\n\n\nImportant information on how to get started with this course. Click this post now for instructions on getting started.\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Announcements"
    ]
  },
  {
    "objectID": "assignments/project.html",
    "href": "assignments/project.html",
    "title": "Project",
    "section": "",
    "text": "The idea of this project is for you to find data and do something interesting with it that relates to the skills you have developed in this course.\nThe project is intentionally open ended- this is an opportunity for you to explore your interests, through using techniques or technologies that branch off those that were introduced in this class, datasets related to problems you face in your career or that you are curious about, or both.\nMy expectation for your project is that it has the length and sophistication of a complete homework assignment, but will all the work focused on the topic. It should include an introduction to contextualize the topic, but otherwise you can structure it how you like.\nThe output of your project should software that allows me to reproduce your work and a research report which describes what you did with the data and your findings, which could be contained within a github repository.\n\n\nThe first step of your project is to submit an initial proposal, which will help to ensure that your project is both worthwhile and feasible in the time frame of the last half of the semester. Your proposal should describe the area that you are interested in, the datasets you are going to use/mathematical problems you want to solve, and the link to the topics of this course. The proposal length should be around one page (excluding figures and references),\nThe proposal is preliminary, and you can include additional data or a different analysis as needs require.\n\n\n\nWhen writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/project.html#heilmeiers-questions",
    "href": "assignments/project.html#heilmeiers-questions",
    "title": "Project",
    "section": "",
    "text": "When writing proposals, I’ve found the following set of considerations, called “Heilmeier's Questions”, to be helpful to consider. Although this is just a very short project, you might find them helpful as well, and useful for formulating future projects and proposals that you might have to complete. These questions were developed by Dr. George Heilmeier, who was the director of DARPA from 1975-1977. Heilmeier said that every proposal to DARPA needed to answer these questions clearly and completely in order to receive funding:\n\nWhat are you trying to do? Articulate your objectives using absolutely no jargon. What is the problem? Why is it hard?\nHow is it done today, and what are the limits of current practice?\nWhat is new in your approach, and why do you think it will be successful?\nWho cares?\nIf you’re successful, what difference will it make? What applications are enabled as a result?\nWhat are the risks?\nHow long will it take?\nWhat are the midterm and final “exams” to check for success? How will progress be measured?",
    "crumbs": [
      "Assignments",
      "Project"
    ]
  },
  {
    "objectID": "assignments/labs/Lab2.html",
    "href": "assignments/labs/Lab2.html",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#instructions",
    "href": "assignments/labs/Lab2.html#instructions",
    "title": "Homework 2: Applications of Least Squares",
    "section": "",
    "text": "Please submit a .qmd file along with a rendered pdf to the Brightspace page for this assignment. You may use whatever language you like within your qmd file, I recommend python, julia, or R."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "href": "assignments/labs/Lab2.html#problem-1-online-updating-for-least-squares-and-autoregressive-time-series-models",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models",
    "text": "Problem 1: Online Updating for Least Squares and Autoregressive Time Series Models\nMany applications of least squares (and other statistical methods) involve , in which data is collected over a time period and the statistical model is updated as new data arrives. If the quantity of data arriving is very large, it may be inefficient or even impossible to refit the entire model on the entire dataset. Instead, we use techniques (often referred to as which take the current model as a starting point and update them to incorporate the new data.\nThe structure of least squares problems makes them amenable to online updating (sometimes this is called “recursive” least squares). The structure of the problem is as follows, at time \\(t\\) we receive a vector of observations \\(\\mathbf{x}_t\\) and an observation of our target variable \\(y_t\\).\nThe full set of all observations and target variable data that we have received up to time \\(t\\) is contained in the following matrix and vector:\n\\[\nX_{(t)} = \\begin{bmatrix} \\cdots\\, \\mathbf{x}_1^T\\, \\cdots \\\\\n\\cdots\\, \\mathbf{x}_2^T\\, \\cdots \\\\ \\vdots \\\\ \\cdots\\, \\mathbf{x}_t^T\\, \\cdots \\end{bmatrix},\\quad\n\\mathbf{y}_{(t)} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_t \\end{bmatrix}\n\\]\nwhich says that row \\(j\\) of the matrix \\(X_{(t)}\\) is the \\(j\\)th observation \\(\\mathbf{x}_j^T\\), and the \\(j\\)th entry of \\(\\mathbf{y}_{(t)}\\) is \\(b_j\\).\nHere we assume that the vectors \\(\\mathbf{x}\\) each contain \\(n\\) observations, so that \\(X_{(t)}\\) is a \\(t\\times n\\) matrix and the vector \\(\\mathbf{y}_{(t)}\\) is a \\(t\\)-vector.\nAs long as \\(t&gt;n\\), i.e. the number of time observations is greater than the number of features/data points in each observation, we can fit a linear model predicting the target as a function of the features \\(\\mathbf{x}\\) by solving the following system of equations:\n\\[\n(X^T_{(t)}X_{(t)})\\mathbf{x}_{(t)} = X_{(t)}^T\\mathbf{y}_{(t)}\n\\]\nAs the length of the time series increases, the computational difficulty of solving this problem also increases. However, it is possible to re-use work done on the previous time step to avoid solving the full system at each step.\nThis algorithm is based on the fact that the Gram Matrix \\(X^T_{(t+1)}X_{(t+1)}\\) can be calculated from the Gram matrix \\(X^T_{(t)}X_{(t)}\\) from the previous time step. \\[\nX^T_{(t+1)}X_{(t+1)} = X^T_{(t)}X_{(t)} + \\mathbf{x}_{t+1}\\mathbf{x}_{t+1}^T\n\\] Similarly, the product \\(X^T_{(t+1)} \\mathbf{y}_{(t+1)}\\) can also be updated from the value on the previous time step: \\[\nX^T_{(t+1)}\\mathbf{y}_{(t+1)} = X^T_{(t)}\\mathbf{y}_{(t)} + y_{t+1}\\mathbf{x}_{t+1}\n\\]\nWe can write an efficient algorithm to compute the updated least squares solution as follows:\n\nStep 1: Pick an initial time \\(t\\) such that \\(X_{t}\\) is square or tall so that the least squares problem can be solved (i.e. wait for enough data to have built up before your start) and then calculate the Gram matrix and the product \\(X^T_{t} \\mathbf{y}_t\\) \\[\nG_{(t)} = X^T_{(t)}X_{(t)}, \\quad \\mathbf{h}_{(t)} = X^T_{t}\\mathbf{y}_t\n\\]\nStep 2: Find the least squares solution at time \\(t\\) by solving the linear system: \\[\nG_{(t)}\\mathbf{\\theta}_{(t)} = \\mathbf{h}_{(t+1)}\n\\]\nStep 3: When the next data points \\(\\mathbf{x}_{t+1}\\) and \\(y_{t+1}\\), update \\(G_{(t+1)}\\) and \\(\\mathbf{h}_{(t+1)}\\):\n\n\\[ G_{(t+1)} = G_{(t)} + \\mathbf{x}_{t+1}\\mathbf{x}^T_{t+1},\\quad\n\\mathbf{h}_{(t+1)} = \\mathbf{h}_{(t)} + y_{t+1}\\mathbf{x}_{t+1}\n\\] Then you can repeat Step 2 to find \\(\\mathbf{theta}_{t+1}\\). This algorithm can be improved upon slightly using the Matrix Inversion Lemma/Woodbury Formula, which could be a topic for a project (see note at the end which mentions Kalman filters).\n\nYou are going to use this algorithm to make a linear, autoregressive model that predicts total day-ahead citibike trips from the daily high temperature and the number of daily citibike trips taken each of the past 7 days. The data is contained in the file daily_citibike_trips.csv.\n\nSpecifically, for each time point \\(t&gt;7\\) , fit the following model as a least squares estimation problem: \\[\nN_{trips,\\tau} = \\sum_{i=1}^7 \\theta_i N_{trips,\\tau-i} + \\theta_{i+1} T,\n\\] Here, each \\(N_{trips,\\tau}\\) stands for the number of citibike trips on the \\(t\\)th day of the time series, \\(T\\) stands for the forecast high temperature in New York City that day, and the coefficients \\(\\theta_i\\) are the decision variables.\nFind the coefficients \\(\\theta_{i,t}\\) that minimize the mean square errors on all the observed citibike trips prior to time \\(t\\). Use the recursive least squares optimization outlined in the preamble to this problem to calculate the coefficients for each time point, and plot how they and the \\(R^2\\) of the model change over time.\nWhat patterns do you notice in how the regression coefficients and \\(R^2\\) change over time?\nTip: Be very cautious when coding about the dimensionality of matrices and arrays. In python, x @ x.T will be an 8x8 matrix if a.shape = (8,1). However, by default x.shape = (8,), indicating that a is not being treated as either a row or column vector. For this problem it is important that the vectors are either row or column vectors, and not arrays without such an orientation. In python, you can use numpy.reshape to adjust.\n\nWe have included temperature as a variable because it probably influences the decision to ride a citibike. However, the relationship could be nonlinear, as both extreme high and low temperatures make bike riding less comfortable. With this idea in mind, create a new feature by choosing a nonlinear function of the temperature \\(T\\) that represents the potential for both high and low values of \\(T\\) to the same impact on predicted ridership. Use least squares optimization to fit an autoregressive time series model, replacing \\(T\\) with the value of your new feature \\(f(T)\\) in the time series. How does the temperature dependence coefficient differ between this model and the one you fit in (a)? Does the accuracy of the model improve or get worse using the new feature?"
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "href": "assignments/labs/Lab2.html#problem-2-weighted-least-squares",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 2: Weighted Least Squares",
    "text": "Problem 2: Weighted Least Squares\nThe file social-mobility.csv contains data on the fraction of individuals born in the years 1980-1982 to parents in the bottom 20% of the income distribution who reach the top 20% of the income distribution by the time they turn 30 in a large number of municipalities throughout the United States. The dataset also contains additional variables that describe other socio-economic differences between the cities in the dataset.\n\nMake a scatter-plot of mobility versus population (use a log-scale for population). What do you notice about the variance of social mobility as a function of population? This is a common feature of nearly every dataset containing geographic regions with widely different populations.\nAssume that the number of children born in families making below the 20th percentile of the income distribution in each city is linearly proportional to the city population. Write down a formula for how the variance of each measurement of the social mobility should depend on the measured social mobility and the population. Hint: start with either the formula for the variance of binomial counts or look up the variance of a proportion derived from a binomial distribution. Don’t worry about constant factors when deriving this formula.\nUse weighted least squares to calculate an estimate of how social mobility depends on commute time and student-teacher ratio, using weights calculated based on the variance estimate derived in (b). Compare the coefficients to those derived from ordinary least squares with no weights."
  },
  {
    "objectID": "assignments/labs/Lab2.html#problem-3-markowitz-portfolio-optimization",
    "href": "assignments/labs/Lab2.html#problem-3-markowitz-portfolio-optimization",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Problem 3: Markowitz Portfolio Optimization",
    "text": "Problem 3: Markowitz Portfolio Optimization\nIn this problem you will use Markowitz Portfolio Optimization to construct a set of portfolios that aim to achieve target expected rates of return while minimizing risk. The file stock_returns.csv contains information on daily asset returns from 2020-2024 for a group of assets, consisting mostly of large-cap stocks but also a handful of exchange traded funds that correspond to US Treasury Bonds and Notes with varying maturities.\nYou will divide the data into two time periods, a training period (2020-2022) and a testing period (2022-2024). The data in the stock_returns.csv is stored in a long format, with the following variables:\n\nCompany - The ticker symbol that identifies the stock\ndate - The date to which the data corresponds\nadjusted- the closing price of the stock, adjusted for special events like dividends and stock splits\nreturn - the ratio of the current adjusted close to the adjusted close on the previous trading day\nlog_return- the natural logarithm of the return\n\n\nConstruct a vector (we call it \\(\\mu\\)) containing the annualized rate of return over the training period (for the \\(i\\)th stock, you can use the formula: \\(\\mu_i = \\exp\\left(0.5\\sum_{t} \\mathrm{log\\_return}_i(t)\\right)\\)), or the square root of the total return over the first two years of data, and daily return covariance \\(\\Gamma\\).\n\nHint: Possible workflow if working in python: convert your data to a wide format using pandas, extract the values into a numpy ndarray, and then use the function np.cov.\nThen solve the following constrained least squares problem to calculate optimal portfolios achieving a fixed rate of return with minimum variance:\n\\[\n\\begin{aligned}\n    \\min_{w} \\mathbf{x}^T\\Gamma \\mathbf{x}, \\\\\n    \\mathbf{w}^T\\mathbf{\\mu} = r, \\\\\n    \\sum_{i} w_i = 1\n\\end{aligned}\n\\] Here \\(\\mathbf{w}\\) is a vector containing the investment allocations into different assets, and \\(r\\) is the target rate of return.\nCalculate optimal portfolios based on the 2020-2022 data for \\(r=1.05\\), \\(r=1.10\\), and \\(r=1.20\\).\n\nPlot the cumulative value of each portfolio over time, assuming that an initial investment is made at the start of the period and that there is no rebalancing of the portfolio, i.e. \\(r_{T} = \\sum_{i=1}^n w_i\\Pi_{t=1}^T r_{it}\\), where \\(r_{it}\\) is the return of asset \\(i\\) on trading day \\(t\\), (hint: np.cumprod allows you to efficiently calculate this quantity). Make the plot for both the training and test sets of returns.\n\nFor each of the 3 portfolios also report:\n\nThe annualized return on the training and test sets;\nThe risk on the training and test sets, defined as the realized variance of the daily return;\nThe asset with the maximum allocation weight, and its weight;\nThe initial leverage, defined as \\(\\sum_{i=1}^n |w_i|\\). This number is always at least one, and it is exactly one only if the portfolio has no short positions.\n\nComment briefly on your observations about the different portfolios and the difference between their training and testing performance.\n\nIt is well known that optimal portfolios constructed using the Markowitz procedure perform much more poorly out of sample compared to in sample. This is due to a variety of reasons, one of which is that the procedure assumes that future returns are equal to past returns, another that the correlation structure of the market might change over time, and finally, when there are many assets there is the potential for overfitting. Repeat the previous problem but introduce a ridge regression/\\(l_2\\) norm penalty term to the objective function, with a hyperparameter \\(\\lambda\\) governing the size of the penalty term.\n\nSpecifically, you will select 10 positive values of \\(\\lambda\\) on a log scale between \\(1e-1\\) and \\(10\\) and for each value of \\(\\lambda\\) solve the following penalized regression problem:\n\\[\n\\begin{aligned}\n    \\min_{w} w^T(\\Gamma+\\lambda I) w ,\\\\\n    w^T\\mu = r,\\\\\n    \\sum_{i=1}^n w_i = 1\n\\end{aligned}\n\\]\nfor just the single value of \\(r=20\\%\\). Then calculate the performance of each of these regularized Markowitz strategies on both the training and test datasets and plot the the return of the portfolios over both the training and testing period.\nFor each portfolio, also report:\n\nThe annualized return on the training and test sets;\nThe risk on the training and test sets;\nThe maximum allocation weight and the asset with maximum allocation;\nThe initial leverage\n\nComment on how the different values of \\(\\lambda\\) changed the optimal portfolios and the difference between in-sample and out-of-sample return and variance."
  },
  {
    "objectID": "assignments/labs/Lab2.html#potential-projects-based-on-this-homework",
    "href": "assignments/labs/Lab2.html#potential-projects-based-on-this-homework",
    "title": "Homework 2: Applications of Least Squares",
    "section": "Potential Projects Based on this Homework:",
    "text": "Potential Projects Based on this Homework:\nRecursive least squares is a gateway to several important techniques that would lead to good projects. The Kalman Filter or Bayes Filter is an algorithm that recursively estimates a statistical model while allowing the underlying coefficients (or state) of that model to undergo dynamical evolution (as a simple example, think of correcting noisy measurements of the GPS location of a drone using Newtownian physics). These are some of the most useful prediction algorithms and can be applied to many areas, including econometrics, web traffic prediction, and vehicle location. For a more math related project, studying the accuracy of the Woodbury formula could be interesting."
  },
  {
    "objectID": "assignments/labs/tbd.html",
    "href": "assignments/labs/tbd.html",
    "title": "Lab TBD",
    "section": "",
    "text": "Check back later\nThis assignment is not available to view yet. Please check back closer to the assignment due date/week of the module this assignment is based on."
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-optimization",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is Optimization?",
    "text": "What is Optimization?\n\nWhat is the decision that leads to the best outcome?\nWhat is the value of our decision variable \\(\\mathbf{x}\\) that minimizes our objective function f()?\n\n\\[\n\\textrm{minimize}\\quad f(\\mathbf{x}), \\\\\n\\textrm{subject to}\\quad g_i(\\mathbf{x}) \\leq 0, \\\\\n\\textrm{and}\\quad h_j(\\mathbf{x}) = 0,\\\\\n\\mathbf{x}\\in \\mathbb{R}^n, \\quad f,\\, g_i,\\, h_j: \\mathbb{R}^n\\mapsto \\mathbb{R}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#constraint-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Constraint Terminology",
    "text": "Constraint Terminology\n\nA feasible point \\(\\mathbf{x}\\) for the optimization problem is a point that satisfies all the constraints: \\[\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]\nTechnically we don’t need the \\(h_j\\) terms\nCan get greater than constraints with \\(-g_i\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "href": "meetups/Meetup-1/meetup-1.html#optimum-terminology",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimum Terminology",
    "text": "Optimum Terminology\n\n\\(\\mathbf{x}\\) is called a local minimum if there is some region \\(\\Gamma\\) surrounding \\(\\mathbf{x}\\) where \\(f(\\mathbf{x})\\leq f(\\mathbf{x'})\\) for all \\(\\mathbf{x}'\\) in \\(\\Gamma\\).\n\\(\\mathbf{x}\\) is a global minimum if there is no feasible point \\(x'\\) with \\(f(\\mathbf{x'})&lt;f(\\mathbf{x})\\)\nCan turn a maximization problem into a minimization problem by replacing the objective function \\(f\\) with \\(-f\\)"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(\\mathbf{x}\\) represents something we control:\n\nCoefficients of a linear regression\nWeights of a neural network\nAllocation of $ to advertising channels\nInvestment allocations in a portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(f\\) models how the decision \\(\\mathbf{x}\\) impacts us\n\nRMS error of the regression\ncross-entropy plus a penalty for the weights\nAd impressions on target demographic\nExpected variance of your portfolio"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "href": "meetups/Meetup-1/meetup-1.html#meaning-of-the-math-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Meaning of the math",
    "text": "Meaning of the math\n\n\\(g_i\\) and \\(h_j\\) constrain your decisions \\(\\mathbf{x}\\) so they are realistic\n\nCan’t put negative $ in an ad channel\nRegulatory limits on leverage, short positions\nValid probability distribution\nPrior constraints on parameters"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-underlies-nearly-all-data-science-algorithms",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization underlies nearly all data science algorithms",
    "text": "Optimization underlies nearly all data science algorithms\nGoing in depth brings some benefits:\n\nRecognize what problem you are facing, can enable you to solve it 1000 times easier in some cases\nUnderstand how the tools you use work and what can go wrong\nAbsolutely indispensable for some cutting edge methods (i.e. deep learning)\nUseful non-stats applications"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\n\n\\(i\\)th observation is vector \\(\\mathbf{a}_i\\)\nObs of target function \\(b_i\\)\nLinear model \\(b \\sim a^t x\\)\n\n\\[\n\\textrm{minimize RMS error:}\\quad \\min_{x\\in\\mathbb{R}^n} \\sum_{i} \\left(\\mathbf{a}_i^t \\mathbf{x}-b_i\\right)^2 \\] or let \\(A\\) be the matrix whose rows are the vectors \\(\\mathbf{a}_i^t\\): \\[\\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 \\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "href": "meetups/Meetup-1/meetup-1.html#examples-least-squares-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Least Squares",
    "text": "Examples: Least Squares\nAdd penalty terms for regularization or constraints\n\\[ \\min_{\\mathbf{x}\\in\\mathbb{R}^n} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 +k\\|x\\|^2 \\\\\ng_i(\\mathbf{x}) \\leq 0 \\\\\nh_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "href": "meetups/Meetup-1/meetup-1.html#examples-maximum-likelihood",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Examples: Maximum Likelihood",
    "text": "Examples: Maximum Likelihood\n\n\\(p(\\mathbf{y}|\\mathbf{x})\\) is likelihood of data given parameters\nmaximum likelihood is the optimization problem\n\n\\[\n\\textrm{find:}\\quad \\min_{\\mathbf{x}\\in\\mathbb{R}^n} -\\log\\left(p(\\mathbf{y}|\\mathbf{x})\\right) \\\\\n\\textrm{subject to:}\\quad g_i(\\mathbf{x}) \\leq 0 \\\\\n\\textrm{and:}\\quad h_j(\\mathbf{x}) = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "href": "meetups/Meetup-1/meetup-1.html#example-deep-learning",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Example: Deep Learning",
    "text": "Example: Deep Learning\n\nTraining examples \\(\\mathbf{x}_i\\) and labels \\(y_i\\)\nNeural network defined by a function \\(\\phi\\) that depends on weights \\(\\mathbf{w}\\)\nLearning problem:\n\n\\[ \\min_w \\sum_i C(\\phi(\\mathbf{x}_i,\\mathbf{w}),y_i), \\]\n\n\\(C\\) is a cost function"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-outside-data-sciece",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization outside data sciece",
    "text": "Optimization outside data sciece\n\nOrgs and Companies face constant optimization problems\nLearning how to handle them can be a superpower"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nLeast Squares\n\n\nOldest, easiest, most mature technologies, first four weeks\nPrimary book is VMLS"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nConvex Optimization\n\n\nNon-negative curvature\nIncredibly useful, difficult, weeks 5-12\nSubsumes many special cases (least squares, linear programs, etc)\nGrowing field, especially with advent of AI"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "href": "meetups/Meetup-1/meetup-1.html#course-structure-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Structure",
    "text": "Course Structure\n\n\n\nNonconvex Optimization\n\n\nCan’t solve these but can come close\nUseful for things like deep learning\nLast 3 weeks\nMore based on lectures but also https://www.deeplearningbook.org/"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "href": "meetups/Meetup-1/meetup-1.html#other-textbook",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Other Textbook",
    "text": "Other Textbook\n\n\n\nWe have one more good book, unforunately not free\nAlso doesn’t cover anything in depth\nGreat for overall picture"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "href": "meetups/Meetup-1/meetup-1.html#homework-assignments",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nDue every two weeks\nMostly solving problems with data but some math\nPreferred format: quarto file accompanied with rendered pdf\nChoice of language, Recommended python, Julia, R\n80% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#final-project",
    "href": "meetups/Meetup-1/meetup-1.html#final-project",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Final Project",
    "text": "Final Project\n\nFinal Project will be to explore an aspect of the course in a little more depth.\nIntentionally open ended, but could be as simple as picking an application area of interest to you and designing and solving a problem in that area\n20% of your grade"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "href": "meetups/Meetup-1/meetup-1.html#course-website-and-slack-channel",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Course Website and Slack Channel",
    "text": "Course Website and Slack Channel\n\nKey annoucements, homework, readings, etc will be posted to: https://georgehagstrom.github.io/DATA609Spring2025/\nCourse will have a slack channel to enable rapid communication: invite link\nTurn in assignments on Brightspace"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "href": "meetups/Meetup-1/meetup-1.html#generative-ai-policy",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\n\nAI is useful, you can ask it questions, have it generate code, etc\nIt is great at easy problems, but that is a trap\nIt makes lots of mistakes\nResearch shows it helps people who know the material well much more than those who don’t\nPolicy: You should understand everything that you turn in completely. I’ll will be generous with grades for genuine effort but will penalize AI-driven mistakes harshly"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "href": "meetups/Meetup-1/meetup-1.html#what-to-do-this-week",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What to do this week?",
    "text": "What to do this week?\n\nChapter 1 Introduction to Algorithms for Data Mining and Machine Learning\nChapter 1 of Convex Optimization\nIf needed review linear algebra:\n\nEssential Linear Algebra for Machine Learning -Appendix A1, A3-A5 of Convex Optimization\n\nStart Lab 1\n\nProblem 1 this week, 2 and 3 next week"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "href": "meetups/Meetup-1/meetup-1.html#optimization-needles-in-a-haystack",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Optimization: Needles in a haystack?",
    "text": "Optimization: Needles in a haystack?\n\nGeneral optimization is a hard problem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError proportional to # boxes sampled: \\[\\mathrm{Num Boxes} \\sim \\frac{C}{\\epsilon^N}\\]\nN is num. parameters\nModest N, problem takes longer than age of universe"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "href": "meetups/Meetup-1/meetup-1.html#only-local-optimization-is-possible",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Only Local Optimization is Possible",
    "text": "Only Local Optimization is Possible\n\n\n\nCan’t find lowest valley, but can find a valley\nOften, this is still very useful in practice\nNo guarantees, sensitive to initial guess\nFor convex problems, only one valley!"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nIn calculus need two things: \\[\n\\frac{df}{dx} = 0,\n\\frac{d^2f}{dx^2} &gt; 0\n\\]\nFunction is “flat”\nCurvature is “up”"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "href": "meetups/Meetup-1/meetup-1.html#what-is-a-local-minimum-2",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "What is a local minimum?",
    "text": "What is a local minimum?\n\nvalue \\(\\mathbf{x}_0\\) where \\(f(\\mathbf{x})&gt;f(\\mathbf{x}_0)\\) close\nHigher dimensions \\[\n\\nabla f(\\mathbf{x}) = 0 \\\\\n(\\nabla^2 f)_{ij} = \\frac{\\partial^2 f(\\mathbf{x})}{\\partial x_i \\partial x_j} \\succ 0\n\\]\nVanishing gradient\nPositive Definite “Hessian”"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\nIterative methods:\n\nGradient Descent, Newton’s Method, ….\n\nStart with initial guess \\(\\mathbf{x}_0\\)\nCalculate gradient \\(\\nabla f\\)\nTake a “step” in direction of \\(\\nabla f\\) \\[ \\mathbf{x}_{n+1} = \\mathbf{x}_n - k\\nabla f(\\mathbf{x}_{n})\\]\n\\(k\\) is step size or learning rate"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "href": "meetups/Meetup-1/meetup-1.html#how-to-find-local-minimum-in-practice-1",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "How to find local minimum in practice?",
    "text": "How to find local minimum in practice?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(f(x,y) = \\exp\\left(-x^2-3y^2\\right)\\)\n\\(k=0.1\\)\n100 steps"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#advert",
    "href": "meetups/Meetup-1/meetup-1.html#advert",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Advert",
    "text": "Advert\n\nOnce per month (Tuesday, Wedneday, Thursday) “field trips” to New York Open Statistical Computing Meetup\nSend me a DM I’ll add you to the email list\nnyhackr.org"
  },
  {
    "objectID": "meetups/Meetup-1/meetup-1.html#thanks",
    "href": "meetups/Meetup-1/meetup-1.html#thanks",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-12/Untitled.html",
    "href": "meetups/Meetup-12/Untitled.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cvx\n\n\ndef f_obj(x,y):\n    return (x+2)**2 + (y-1)**2\n\n\ndef cline(x,y):\n    return x-y\n\nL = 100\nxmin = -5\nxmax = 5\nymin = -5\nymax =5\nxx = np.linspace(xmin, xmax, L)\nyy = np.linspace(ymin, ymax, L)\nX, Y = np.meshgrid(xx, yy)\n\nf_Z = f_obj(X,Y)\n\n\nf_Z.shape\n\n(100, 100)\n\n\n\nx_var = cvx.Variable()\ny_var = cvx.Variable()\n\nobj = cvx.square(x_var+2) + cvx.square(y_var-1)\nconstr = -x_var+y_var\n\nprob = cvx.Problem(cvx.Minimize(obj),[constr&lt;=0])\n\nprob.solve()\n\n\n\nZ_Reg = X &gt; Y\nZ_Reg_nan = np.where(Z_Reg, np.nan, Z_Reg)\nplt.contourf(X, Y, f_Z, levels=20, cmap='magma_r')\nplt.colorbar(extend=\"both\")\nplt.plot(xx,yy)\n\nplt.pcolormesh(X,Y,Z_Reg_nan,alpha=0.5)\nplt.scatter(x_var.value,y_var.value,s=100,marker=\"*\",color=\"Red\")\n\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('')\nplt.show()\n\n\n\n\n\n\n\n\n\nl = 4.0\nx_var = cvx.Variable()\ny_var = cvx.Variable()\n\nobj = cvx.square(x_var+2) + cvx.square(y_var-1)\nconstr = -x_var+y_var\n\nprob = cvx.Problem(cvx.Minimize(obj+l*constr))\n\nprob.solve()\n\n\n\nf_Z_tilt = f_obj(X,Y) + l*(X-Y)\nplt.contourf(X, Y, f_Z, levels=20, cmap='magma_r')\nplt.colorbar(extend=\"both\")\nplt.plot(xx,yy)\n\nplt.scatter(x_var.value,y_var.value,s=100,marker=\"*\",color=\"Red\")\n\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('')\nplt.show()\n\n\n\n\n\n\n\n\n\nprob.solution.dual_vars\n\n{1404: array(3.)}\n\n\n\nl=-0\nprob = cvx.Problem(cvx.Minimize(obj+l*constr))\n\nprob.solve()\n\n0.0\n\n\n\nprob.solution.dual_vars\n\n{}\n\n\n\nZ_Reg[Z_Reg == True] = np.nan*Z_Reg[Z_Reg == True]\n\n\nZ_Reg[np.where(Z_Reg)] = np.nan\n\n\nZ_Reg_nan = np.where(Z_Reg, np.nan, Z_Reg)\n\n\ncvx.squ\n\nnan\n\n\n\nZ_Reg_nan\n\narray([[ 0., nan, nan, ..., nan, nan, nan],\n       [ 0.,  0., nan, ..., nan, nan, nan],\n       [ 0.,  0.,  0., ..., nan, nan, nan],\n       ...,\n       [ 0.,  0.,  0., ...,  0., nan, nan],\n       [ 0.,  0.,  0., ...,  0.,  0., nan],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n\n\n\nx_var.value\n\narray(-2.49985707e-30)\n\n\n\ny_var.value\n\narray(2.49985707e-30)\n\n\n\nplt.scatter(x_var.value,y_var.value)\n\n\n\n\n\n\n\n\n\n\n# Beale function definition\ndef beale(x, y):\n    return ((1.5 - x + x*y)**2 + \n            (2.25 - x + x*y**2)**2 + \n            (2.625 - x + x*y**3)**2)\n\n# Create a grid of points\nx = np.linspace(-4.5, 4.5, 400)\ny = np.linspace(-4.5, 4.5, 400)\nX, Y = np.meshgrid(x, y)\nZ = beale(X, Y)\n\n# Plotting\nplt.figure(figsize=(8, 6))\ncp = plt.contourf(X, Y, Z, levels=np.logspace(-1, 5, 35), cmap='viridis')\nplt.colorbar(cp)\nplt.contour(X, Y, Z, levels=np.logspace(-1, 5, 35), colors='black', linewidths=0.5)\n\n# Global minimum\nplt.plot(3, 0.5, 'r*', markersize=10, label='Global Minimum (3, 0.5)')\n\nplt.title('Contour Plot of the Beale Function')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#week-summary",
    "href": "meetups/Meetup-8/meetup-8.html#week-summary",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Week Summary",
    "text": "Week Summary\n\nOperations that Preserve Convexity\nApplication to Disciplined Convex Programming\nKeep Working on Lab 4\nRead Section 3.2, skim and 3.4, skim 3.5 if you have time/interest\nI will make quasiconvex part of of homework optional"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#hw-2-review-visualization",
    "href": "meetups/Meetup-8/meetup-8.html#hw-2-review-visualization",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "HW 2 Review Visualization",
    "text": "HW 2 Review Visualization\n\nDon’t do this"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#hw-2-review-visualization-1",
    "href": "meetups/Meetup-8/meetup-8.html#hw-2-review-visualization-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "HW 2 Review Visualization",
    "text": "HW 2 Review Visualization\n\n\n\n\n\n\n\nDo this instead"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#hw-2-review-package-versus-exact",
    "href": "meetups/Meetup-8/meetup-8.html#hw-2-review-package-versus-exact",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "HW 2 Review Package versus Exact",
    "text": "HW 2 Review Package versus Exact\n\nPackages versus Exact Formula\nIntended method\n\n\\[\n\\begin{bmatrix}\n2A^TA & C^T \\\\\nC & 0\n\\end{bmatrix}\n\\begin{bmatrix} \\mathbf{\\theta} \\\\ \\mathbf{z}  \\end{bmatrix}\n=\n\\begin{bmatrix} 2A^T\\mathbf{x} \\\\ \\mathbf{d} \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#hw-2-review",
    "href": "meetups/Meetup-8/meetup-8.html#hw-2-review",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "HW 2 Review",
    "text": "HW 2 Review\n\nPackages versus Exact Formula\nPackage methods had varying results\n\nCVX successful\nquad_prog successful\nscipy.minimize unsuccessful\n\nSLSQP finicky"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#personal-story",
    "href": "meetups/Meetup-8/meetup-8.html#personal-story",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Personal Story",
    "text": "Personal Story\n\nWas trying to replicate and build upon a modeling study\nBased on optimization problem:\n\n\\[\n\\max_{\\nu,\\mathrm{x}} \\mu_{\\nu,\\mathrm{x}} \\\\\nS\\nu = 0\\,\\quad\\mathrm{x} \\in \\mathrm{Simplex}\n\\]\n\nFind traits and fluxes that maximize growth rates of prochlorococcus"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#personal-story-1",
    "href": "meetups/Meetup-8/meetup-8.html#personal-story-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Personal Story",
    "text": "Personal Story\n\n\n\n\n\nWitness Protection"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#personal-story-2",
    "href": "meetups/Meetup-8/meetup-8.html#personal-story-2",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Personal Story",
    "text": "Personal Story\n\nAuthor of study chucked problem into matlab without regard for the solver\nSolutions had pleasing levels of “noise”"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#personal-story-3",
    "href": "meetups/Meetup-8/meetup-8.html#personal-story-3",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Personal Story",
    "text": "Personal Story\n\nAuthor of study chucked problem into matlab without regard for the solver\nSolutions had pleasing levels of “noise”"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#personal-story-4",
    "href": "meetups/Meetup-8/meetup-8.html#personal-story-4",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Personal Story",
    "text": "Personal Story\n\nTurns out solver wasn’t appropriate for problem\nWas failing to converge, introducing randomness\n\n\n\n\n\n\nWitness Protection"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#how-to-show-a-function-is-convex",
    "href": "meetups/Meetup-8/meetup-8.html#how-to-show-a-function-is-convex",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "How to show a function is convex?",
    "text": "How to show a function is convex?\n\nApply the definition?\n\nRare, project onto line\n\nCheck the Hessian\n\nRare, simple cases only\n\nConstruct the function from simpler elements\n\n99+% of the time\nSeveral fundamental rules\nAll are special cases of a single rule"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#scalar-multiplication",
    "href": "meetups/Meetup-8/meetup-8.html#scalar-multiplication",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Scalar Multiplication",
    "text": "Scalar Multiplication\n\nIf \\(f\\) is convex and \\(\\alpha\\gt 0\\), then:\n\n\\(\\alpha f\\) is convex\n\nIf \\(\\alpha &lt; 0\\) the \\(\\alpha f\\) is concave"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#positive-weighted-sum",
    "href": "meetups/Meetup-8/meetup-8.html#positive-weighted-sum",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Positive Weighted Sum",
    "text": "Positive Weighted Sum\n\nIf \\(f_i\\) is convex, \\(\\alpha_i\\) positive, then\n\n\\[\nF(x) = \\sum_i f_i(x)\n\\]\n\n\\(F\\) is convex\nApplication: Regularize to your heart’s content \\[\nf(\\mathbf{x}) + \\lambda \\|\\mathbf{x}\\|_1\n\\]"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#integration",
    "href": "meetups/Meetup-8/meetup-8.html#integration",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Integration",
    "text": "Integration\n\nSuppose \\(g(\\mathbf{x},\\mathbf{y})\\) convex in \\(\\mathbf{x}\\) for fixed \\(\\mathbf{y}\\)\nSuppose \\(w(\\mathbf{y}) &gt; 0\\) \\[\nG(\\mathbf{x}) = \\int_{Y} \\mathbf{dy} w(\\mathbf{y})g(\\mathbf{x},\\mathbf{y})\n\\]\n\\(G(\\mathbf{x})\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#example-running-average",
    "href": "meetups/Meetup-8/meetup-8.html#example-running-average",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Example: Running Average",
    "text": "Example: Running Average\n\nLet \\(f\\) be a convex function and define: \\[\nF(x) = \\frac{1}{x}\\int_0^x dt f(t)\n\\]\nCan substitute \\(t=sx\\) into integral: \\[\nF(x) = \\frac{1}{x}\\int_0^1 ds x f(sx) = \\int_0^1 ds f(sx)\n\\]\n\\(f(sx)\\) is convex in \\(x\\) for fixed \\(s\\). Therefore \\(F(x)\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#composition-with-affine-function",
    "href": "meetups/Meetup-8/meetup-8.html#composition-with-affine-function",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Composition with Affine Function",
    "text": "Composition with Affine Function\n\nSuppose that \\(f(\\mathbf{x})\\) is convex\nThen \\(g(\\mathbf{y}) = f(A\\mathbf{y} + \\mathbf{d})\\) is convex\nThis also preserves concavity\nApplication: \\(\\|A\\mathbf{x}-b\\|_p\\) convex for any norm"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#pointwise-maximum",
    "href": "meetups/Meetup-8/meetup-8.html#pointwise-maximum",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Pointwise Maximum",
    "text": "Pointwise Maximum\n\nSuppose we have a bunch of convex functions \\(f_1\\), \\(f_2\\), …., \\(f_n\\) \\[\nF(x) = \\max\\left(f_1(x),f_2(x),\\cdots,f_n(x)\\right)\n\\]\n\\(F(x)\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#pointwise-maximum-1",
    "href": "meetups/Meetup-8/meetup-8.html#pointwise-maximum-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Pointwise Maximum",
    "text": "Pointwise Maximum"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#example-worst-case-scenario",
    "href": "meetups/Meetup-8/meetup-8.html#example-worst-case-scenario",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Example: Worst-Case Scenario",
    "text": "Example: Worst-Case Scenario\n\nSuppose \\(f_i\\) is the cost function of \\(i\\)th people in your organization\n\\(f_i\\) convex\n\\(x\\) is a choice you are making\nCan define worst case cost function: \\[\nf_W(x) = \\max(f_1(x),f_2(x),\\cdots,f_n(x))\n\\]\n\\(f_W(x)\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#pointwise-supremum",
    "href": "meetups/Meetup-8/meetup-8.html#pointwise-supremum",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Pointwise Supremum",
    "text": "Pointwise Supremum\n\nsupremum is least upper bound (think of it like infinite max)\nif \\(f(x,\\alpha)\\) is convex in \\(x\\) for all \\(\\alpha \\in A\\), then: \\[\nF(x) = \\sup_{\\alpha\\in A} f(x,\\alpha)\n\\]\nThen \\(F(x)\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#pointwise-supremum-examples",
    "href": "meetups/Meetup-8/meetup-8.html#pointwise-supremum-examples",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Pointwise Supremum examples",
    "text": "Pointwise Supremum examples\n\nPlacing a transmitter at a location \\(x\\)\nCare about maximum distance from the transmitter to a region \\(S\\)\n\\(d(x) = \\sup_{y\\in S}\\|x-y\\|\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#pointwise-supremum-examples-1",
    "href": "meetups/Meetup-8/meetup-8.html#pointwise-supremum-examples-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Pointwise Supremum Examples",
    "text": "Pointwise Supremum Examples"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#largest-eigenvalue-of-a-symmetric-matrix",
    "href": "meetups/Meetup-8/meetup-8.html#largest-eigenvalue-of-a-symmetric-matrix",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Largest eigenvalue of a symmetric matrix",
    "text": "Largest eigenvalue of a symmetric matrix\n\nEigenvalue satisfies \\(Sv = \\lambda v\\)\n\\(\\sup_{\\|v\\|=1} v^T S v = \\lambda_{\\mathrm{max}}\\)\nIntuition: \\(S\\) can be thought of according to its eigenvalues and eigenvectors\nBiggest eigenvalue corresponds to direction of maximum stretch\n\\(\\lambda_{\\mathrm{max}}(S)\\) is a convex function of \\(S\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#scalar-composition",
    "href": "meetups/Meetup-8/meetup-8.html#scalar-composition",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Scalar Composition",
    "text": "Scalar Composition\n\n\\(f\\) is a function from \\(\\mathbb{R}^n\\to\\mathbb{R}\\)\n\\(g\\) is a function from \\(\\mathbb{R}\\to\\mathbb{R}\\)\n\\(g(f(x))\\) is convex if\n\n\\(g\\) is convex and non-decreasing, \\(f\\) is convex\n\\(g\\) is convex and non-increasing, \\(f\\) is concave\n\n\\(g(f(x))\\) is concave if\n\n\\(g\\) is concave and non-increasing, \\(f\\) is convex\n\\(g\\) is concave and non-decreasing, \\(f\\) is concave"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#scalar-composition-1",
    "href": "meetups/Meetup-8/meetup-8.html#scalar-composition-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Scalar Composition",
    "text": "Scalar Composition\n\nTrick to remember, use chain rule \\[(g(f(x)))'' = (g'(f(x))f'(x))' =\\\\\ng''(f(x))f'(x)^2 + g'(f(x))f''(x)\\]\n\nSign of both needs to match for certainty over convexity\n\\(g''\\geq 0\\) only convex is possible, need \\(g'f''\\geq 0\\)\n\\(g''\\leq 0\\) only concave is possible, need \\(g'f''\\leq 0\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#scalar-composition-examples",
    "href": "meetups/Meetup-8/meetup-8.html#scalar-composition-examples",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Scalar Composition Examples",
    "text": "Scalar Composition Examples\n\n\\(e^{f(x)}\\) is convex if \\(f\\) is convex (exp convex increasing)\n\\(\\log(f(x))\\) is concave if \\(f\\) is concave (log concave increasing)\n\\(1/f(x)\\) is convex if \\(f\\) is concave (1/x convex decreasing)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#vector-composition-one-rule-to-rule-them-all",
    "href": "meetups/Meetup-8/meetup-8.html#vector-composition-one-rule-to-rule-them-all",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Vector Composition (One Rule to Rule them All)",
    "text": "Vector Composition (One Rule to Rule them All)\n\nLet \\(h(x_1,x_2,\\cdots,x_k):\\, \\mathbb{R}^k\\to\\mathbb{R}\\)\nLet \\(g(\\mathbf{x}) = [g_1(\\mathbf{x}),g_2(\\mathbf{x}),\\cdots,g_k(\\mathbf{x})]^T: \\mathbb{R}^n\\to\\mathbb{R}^k\\)\nReplacing “coordinates” \\(h\\) by \\(g_i\\)’s"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#vector-composition",
    "href": "meetups/Meetup-8/meetup-8.html#vector-composition",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Vector Composition",
    "text": "Vector Composition\n\n\\(f(\\mathbf{x}) = h(g_1(\\mathbf{x}),g_2(\\mathbf{x}),\\cdots,g_k(\\mathbf{x}))\\) is convex if:\n\n\\(h\\) is convex\n\\(g_i\\) is convex and \\(h\\) is non-decreasing in arg \\(i\\)\n\\(g_i\\) is concave and \\(h\\) is non-increasing in arg \\(i\\)\n\n\\(f\\) is concave if:\n\n\\(h\\) is concave\n\\(g_i\\) is concave and \\(h\\) is non-increasing in arg \\(i\\)\n\\(g_i\\) is convex and \\(h\\) is non-decreasing in arg \\(i\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#examples",
    "href": "meetups/Meetup-8/meetup-8.html#examples",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Examples",
    "text": "Examples\n\n\\(\\log\\left(e^{x_i} \\right)\\) is convex, increasing in all arguments\n\nTherefore \\(\\log\\left(e^{g_i(\\mathbf{x})}\\right)\\) is convex\n\n\\(f(x,y) = x^2/y\\) (quadratic over linear)\n\n\\(h(x) = \\frac{p(x)^2}{q(x)}\\) is convex if:\n\\(p\\) non-negative and convex\n\\(q\\) positive and concave"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking",
    "href": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Automatic Convexity Checking",
    "text": "Automatic Convexity Checking\n\nCVX works by building an evaluation tree for each function\nCVX starts at leaves and builds up, applying vector composition rule\nEach function is either convex, concave, or unknown\nSign of each expression is tracked (+,-,unknown)\nAtoms are sometimes increasing/decreasing depending on sign"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-1",
    "href": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-1",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Automatic Convexity Checking",
    "text": "Automatic Convexity Checking\n\n\\(2x^2+3\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-2",
    "href": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-2",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Automatic Convexity Checking",
    "text": "Automatic Convexity Checking\n\n\\(\\sqrt{1+x^2}\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-3",
    "href": "meetups/Meetup-8/meetup-8.html#automatic-convexity-checking-3",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Automatic Convexity Checking",
    "text": "Automatic Convexity Checking\n\n\\(\\sqrt{1+x^2}\\) is convex though…..\n\\(\\sqrt{1+x^2} = \\|[1,x]\\|_2\\)\nCan use convexity of the norm:\n\nnorm(hstack(1,x)) is DCP\nVector composition on Affine function \\(1\\)"
  },
  {
    "objectID": "meetups/Meetup-8/meetup-8.html#thanks",
    "href": "meetups/Meetup-8/meetup-8.html#thanks",
    "title": "DATA 609 Meetup 8: Constructing Convex Functions",
    "section": "Thanks",
    "text": "Thanks"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#week-summary",
    "href": "meetups/Meetup-2/meetup-2.html#week-summary",
    "title": "Meetup 2: Least Squares",
    "section": "Week Summary",
    "text": "Week Summary\n\nStarting on Least Squares\nChapter 12 of VMLS\nKeep working on Lab 1, due this Sunday at midnight"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "href": "meetups/Meetup-2/meetup-2.html#system-of-linear-equations",
    "title": "Meetup 2: Least Squares",
    "section": "System of Linear Equations",
    "text": "System of Linear Equations\n\nWhen can we solve: \\[ A\\mathbf{x} = \\mathbf{b}\\,?\\]\n\\(A\\) is an \\(m\\times n\\) matrix\n\\(\\mathbf{x}\\) is an \\(n\\)-vector\n\\(\\mathbf{b}\\) is an \\(m\\)-vector"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m=n\\) square\nExact solution if rows/columns are independent \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     &    \\cdots   & \\vdots \\\\\n                    \\vdots & \\cdots & \\cdots & \\vdots \\\\\n                    a_{n1} & \\cdots & \\cdots & a_{nn}\n                    \\end{pmatrix}\n                    \\]\n\\(\\mathbf{x} = A^{-1}\\mathbf{b}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-1",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&lt;n\\) wide\nGenerally multiple solutions because dependent columns \\[ A = \\begin{pmatrix} a_{11} & \\cdots & \\cdots & \\cdots & a_{1n} \\\\\n                     \\vdots &   \\cdots     & \\cdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & \\cdots & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]\nCommon ML situation (more parameters than data points)\nParticular solution plus the null space"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "href": "meetups/Meetup-2/meetup-2.html#tall-wide-and-square-matrices-2",
    "title": "Meetup 2: Least Squares",
    "section": "Tall, Wide, and Square Matrices",
    "text": "Tall, Wide, and Square Matrices\n\n\\(m&gt;n\\) tall\nGenerally no exact solution \\[ A = \\begin{pmatrix} a_{11} & \\cdots &  a_{1n} \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                     \\vdots &   \\cdots   & \\vdots \\\\\n                    a_{m1} & \\cdots & a_{mn}\n                    \\end{pmatrix}\n                    \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#inspiration-for-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Inspiration for Least Squares",
    "text": "Inspiration for Least Squares\n\nCan’t solve? Minimize the error \\[ \\mathbf{r} = A\\mathbf{x}-\\mathbf{b}\\]\nOptimization problem is: \\[\\textrm{Find:}\\quad \\min_{\\mathbf{x}} \\|A\\mathbf{x} - \\mathbf{b}\\|^2\\]\nThe \\(\\mathbf{x}\\) that minimizes this objective (the squared residual) is the least squares solution\nLinear regression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that:"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-1",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nTake derivative and set to 0: \\[\n\\frac{\\partial}{\\partial \\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2 =\n2A^T A\\left(\\mathbf{x} - A^T\\mathbf{b}\\right)\n\\]\nTwo ways to get that\n\nmatrixcalculus.org\nTake derivatives, work with indices"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "href": "meetups/Meetup-2/meetup-2.html#how-to-solve-it-2",
    "title": "Meetup 2: Least Squares",
    "section": "How to Solve It?",
    "text": "How to Solve It?\n\nGet System of Linear Equations: \\[ A^TA\\mathbf{x} = A^T\\mathbf{b} \\]\n\\(A^TA\\) is an \\(n\\times n\\) symmetric matrix called the Gram matrix\nInvertible if columns of \\(A\\) are independent:\n\n\\[ \\mathbf{x} = \\left(A^T A\\right)^{-1}A^T\\mathbf{b}\\]\n\nThis is the exact formula for the least squares solution"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#properties",
    "href": "meetups/Meetup-2/meetup-2.html#properties",
    "title": "Meetup 2: Least Squares",
    "section": "Properties",
    "text": "Properties\n\nMorse-Penrose Pseudoinverse: \\[A^+ = (A^TA)^{-1}A^T\\]\n\\(n\\times m\\) matrix\nIf \\(A\\) is invertible, \\(A^+ = A^{-1}\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#applications-linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Linear Regression",
    "text": "Applications: Linear Regression\n\nSuppose you have \\(m\\) observations, each with \\(n\\) features\n\\(\\mathbf{a}_{i}\\) is \\(i\\)th obs\nWant to predict the variable \\(\\mathbf{b}\\) using an Affine function: \\[ \\mathbf{a}_i^T\\mathbf{x} +c = b_i \\]\nor\n\\[\n  \\sum_{j=1}^m a_{ij} x_j + c = b_i\n  \\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "href": "meetups/Meetup-2/meetup-2.html#linear-regression",
    "title": "Meetup 2: Least Squares",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nAugmented matrix \\(A\\) and coefficients \\(\\mathbf{x}\\): \\[\nA = \\begin{pmatrix} a_{11} & \\cdots & a_{1n} & 1 \\\\\n                  \\vdots & \\cdots & \\vdots & 1 \\\\\n                  a_{m1} & \\cdots & a_{mn} & 1\n                  \\end{pmatrix}\n\\] \\[\n\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n & c \\end{pmatrix}^T\n\\]\nNow get standard least squares problem:\n\n\\[ \\min_{\\mathbf{x}}\\|A\\mathbf{x}-\\mathbf{b}\\|^2\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n\n\n\n\n\n\nGroup\n\n\n\n\n1\n18-25, play sports\n\n\n2\n26-30, play sports\n\n\n3\n31-35, play sports\n\n\n4\n36-40, play sports\n\n\n5\n41-45, play sports\n\n\n6\n18-25, no sports\n\n\n7\n26-30, no sports\n\n\n8\n31-35, no sports\n\n\n9\n36-40, no sports\n\n\n10\n41-45, no sports"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\n\n\nYou are tasked with spending a marketing budget\nThere are \\(10\\) different demographic groups to target\n4 different marketing channels\n\n\n\n\n\n\nChannel\n\n\n\n\n1\nInstagram\n\n\n2\nWine Mag\n\n\n3\nFacebook\n\n\n4\nUFC"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)\n\n\n\n   Instagram  Wine Mag  Facebook   UFC      Demo\n0        3.0       0.1       0.4  6.00   18-25 S\n1        2.5       0.3       0.6  6.00   26-30 S\n2        2.3       0.4       1.0  5.50   31-35 S\n3        1.8       0.6       1.3  5.00   36-40 S\n4        1.0       0.6       2.0  4.50   41-45 S\n5        3.3       0.8       0.5  0.20  18-25 NS\n6        2.6       1.1       0.8  0.12  26-30 NS\n7        2.2       1.3       1.1  0.10  31-35 NS\n8        1.4       2.3       1.3  0.40  36-40 NS\n9        1.2       4.2       1.7  0.20  41-45 NS"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#applications-marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Applications: Marketing Budget",
    "text": "Applications: Marketing Budget\n\nViews: $1 on channel \\(i\\) gets \\(R_{ij}\\) views in group \\(j\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nProblem: spend money to get \\(\\mathbf{v}_{dem}\\) impressions\nDecision variable \\(\\mathbf{s}\\) is spending in each channel\nSolve least squares problem: \\[\n\\min_{\\mathbf{s}} \\|R\\mathbf{s} - \\mathbf{v}_{dem}\\|^2\n\\]\n\\(R\\mathbf{s}\\) is views"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-1",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform\n\n\nv_dem = np.ones(10)\n\ns = np.linalg.lstsq(R,v_dem)[0]\n\ns_dat = pd.DataFrame(s, columns = [\"Spending\"])\ns_dat[\"Channel\"] = ['Instagram','Wine Mag','Facebook','UFC'] \n\n(\n  ggplot(s_dat,aes(x=\"Channel\",y=\"Spending\"))\n  + geom_point(size=5) \n  + theme_bw(base_size=14)\n)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-2",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-3",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose \\(\\mathbf{v}_{dem}\\) is uniform"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-4",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-5",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget",
    "text": "Marketing Budget\n\nSuppose we want to target only 41-45 Non-Sports Players"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "href": "meetups/Meetup-2/meetup-2.html#marketing-budget-issues",
    "title": "Meetup 2: Least Squares",
    "section": "Marketing Budget Issues",
    "text": "Marketing Budget Issues\n\nNeed Constraints\n\nNo negative money in adds: \\(\\mathbf{s} \\geq 0\\)\nNo infinite budget: \\(\\sum_{i}s_i \\leq s_{max}\\)\n\nNeed a better objective function\n\nWeird to target precise \\(\\mathbf{v}_{dem}\\)\nBetter to have different value for each ad impression"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\n\n\n\n\nUrban Transit\nInternet\nKnow when people enter and leave\nKnow path they took\nHow long does it take to cross each link?"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-1",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-2",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-3",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-4",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nFor trip \\(j\\) vector \\(\\mathbf{r}_j\\)\n\n\\(r_{ij} = 1\\) if \\(i\\)th “link” traversed on \\(j\\)th trip\n\\(r_{ij} = 0\\) otherwise\nMatrix \\(R\\) contains \\(r_{ij}\\)\n\nFor \\(t_j\\) is the time taken on trip \\(j\\)\nDecision variables \\(d_i\\) is the time taken on the \\(i\\)th link\nTime is \\(\\sum r_{ij}d_i\\)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "href": "meetups/Meetup-2/meetup-2.html#network-tomography-5",
    "title": "Meetup 2: Least Squares",
    "section": "Network Tomography",
    "text": "Network Tomography\n\nMinimize Least Squares Error:\n\n\\[\n\\textrm{Find:}\\quad \\min_{\\mathbf{d}} \\|R\\mathbf{d} - \\mathbf{t}\\|^2\n\\]\n\nMany similar problems in optimization theory\nThis one is the simplest"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "href": "meetups/Meetup-2/meetup-2.html#numerical-issues-with-least-squares",
    "title": "Meetup 2: Least Squares",
    "section": "Numerical Issues With Least Squares",
    "text": "Numerical Issues With Least Squares\n\nSuppose you need to solve a system of linear equations: \\[\nA\\mathbf{x} = \\mathbf{b}\n\\]\nWhich of these is better?\n\nnp.linalg.inv(A) @ b\nnp.linalg.solve(A, b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "href": "meetups/Meetup-2/meetup-2.html#matrix-inverses",
    "title": "Meetup 2: Least Squares",
    "section": "Matrix Inverses",
    "text": "Matrix Inverses\n\nConsider Singular Value Decomposition of \\(A\\): \\[\nA = U\\Sigma V^T\n\\]\n\\(U\\) and \\(V\\) are orthonormal matrices\n\\(\\Sigma\\) contains the singular values \\[\n\\Sigma = \\begin{bmatrix}\n  \\sigma_{1} & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "href": "meetups/Meetup-2/meetup-2.html#condition-number-and-accuracy",
    "title": "Meetup 2: Least Squares",
    "section": "Condition Number and Accuracy",
    "text": "Condition Number and Accuracy\n\nAccuracy of np.linalg.inv(A) @ b related to \\(\\frac{\\sigma_{max}}{\\sigma_{min}}\\)\nThis is called the “condition number” of the matrix \\(A\\)\n\n\n%timeit np.linalg.inv(A) @ b\n%timeit np.linalg.solve(A,b)\n\nx1 = np.linalg.inv(A) @ b\nx2 = np.linalg.solve(A,b)\n\nprint(\"Matrix Inverse Error: \",np.linalg.norm(A @ x1 - b))\nprint(\"Linear Solve Error: \",np.linalg.norm(A @ x2 - b))\n\n6.16 ms ± 653 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n1.72 ms ± 101 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nMatrix Inverse Error:  2.3788765877057263e-07\nLinear Solve Error:  1.820234192341143e-15\n\n\n\n100 million times more accurate, 5 times faster"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "href": "meetups/Meetup-2/meetup-2.html#issue-is-worse-for-ata",
    "title": "Meetup 2: Least Squares",
    "section": "Issue is worse for \\(A^TA\\)",
    "text": "Issue is worse for \\(A^TA\\)\n\nWhat about \\((A^TA)^{-1}\\)?\nLet’s look at SVD: \\[ A^TA = V \\Sigma^T\\Sigma U^T\\] \\[ \\Sigma^T\\Sigma = \\begin{bmatrix}\n  \\sigma_{1}^2 & & \\\\\n  & \\ddots & \\\\\n  & & \\sigma_{n}^2\n\\end{bmatrix}\n\\]\nCondition number problems 💀 💀 💀 💀"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "href": "meetups/Meetup-2/meetup-2.html#do-ill-conditioned-matrices-occur",
    "title": "Meetup 2: Least Squares",
    "section": "Do Ill-Conditioned Matrices Occur?",
    "text": "Do Ill-Conditioned Matrices Occur?\n\nVery Common\nCollinear predictors in linear regression\nMain issue in Deep Learning\nNoisy Inverse Problems\nSome of the solutions are basically just regularization"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#alternatives",
    "href": "meetups/Meetup-2/meetup-2.html#alternatives",
    "title": "Meetup 2: Least Squares",
    "section": "Alternatives",
    "text": "Alternatives\n\nSoftware packages generally have routines to solve least squares without computing ill-conditioned matrix inverse\nGood example is \\(A = QR\\) factorization\nHere \\(Q\\) is orthogonal and \\(R\\) is upper triangular\nThen \\[\nA^+ = (A^TA)^{-1}A^T = R^{-1} Q\n\\]\nIn practice np.linalg.linsolve(R,Q @ b)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "href": "meetups/Meetup-2/meetup-2.html#what-if-a-is-really-big",
    "title": "Meetup 2: Least Squares",
    "section": "What if \\(A\\) is really big?",
    "text": "What if \\(A\\) is really big?\n\nConstructing \\(A^TA\\) costs \\(mn^2\\) operations\nSolving \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) costs \\(n^3\\) operations\n\\(A^TA\\) could be a TB or larger…..\nSolution 1- variety of iterative methods (HW)\nSolution 2- Randomized Algorithms (Random Kaczmarz)"
  },
  {
    "objectID": "meetups/Meetup-2/meetup-2.html#thanks",
    "href": "meetups/Meetup-2/meetup-2.html#thanks",
    "title": "Meetup 2: Least Squares",
    "section": "Thanks!",
    "text": "Thanks!\n\n\n\n\nDATA 609"
  },
  {
    "objectID": "meetups/Meetup-2/Untitled.html",
    "href": "meetups/Meetup-2/Untitled.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import numpy as np\n\n\na = np.random.randn(10,10)\nb = np.random.randn(10)\n\n\na @ b\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nb.T @ a.T\n\narray([-1.06610598, -3.42056277, -4.64082216, -2.80315044,  3.59796177,\n        2.13703234, -1.96828452, -0.30080257,  0.1542152 , -0.56058065])\n\n\n\nnp.eye(10)\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\nnp.ones((10,10))\n\narray([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n\n\n\nnp.zeros((10,10))\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\nnp.linalg.norm(np.linalg.inv(a) @ a - np.eye(10))\n\n2.570038214105557e-15\n\n\n\nA = a\n\n\\[ Ax = b \\]\n\nx = np.linalg.lstsq(A,b)[0]\n\n/tmp/ipykernel_2637623/3825200175.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\nTo use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n  x = np.linalg.lstsq(A,b)[0]\n\n\n\nA @ x - b\n\narray([-2.22044605e-15,  3.35842465e-15,  5.55111512e-15, -5.38458167e-15,\n        1.77635684e-15,  2.99760217e-15,  2.66453526e-15, -2.38697950e-15,\n        6.21724894e-15,  4.66293670e-15])\n\n\n\\[\nA = U \\Sigma V^T\n\\]\n\\(A\\) is \\(m\\times n\\), \\(U\\) is \\(m\\times m\\) \\(\\Sigma\\) \\(m \\times n\\) \\(V\\) is \\(n \\times n\\)\n\\(U\\) and \\(V\\) are orthonormal matrices, which means:\n\\(U U^T = I\\), \\(V V^T = I\\)\n\\(\\Sigma\\) is 0 except for the diagonals which are the singular values of \\(A\\)\n\nA = np.random.randn(100,10)\n\n\nsvd_A = np.linalg.svd(A)\n\n\nsvd_A[1]\n\narray([13.1081352 , 11.81959892, 11.15343028, 10.83354228, 10.23738251,\n        9.27330655,  9.07488313,  8.12095293,  7.844576  ,  6.77092389])\n\n\n\nU = svd_A[0]\nV = svd_A[2].T\n\n\nnp.linalg.norm(U @ U.T -  np.eye(100))\n\n4.986008699316736e-15\n\n\n\nnp.linalg.norm(V @ V.T -  np.eye(10))\n\n2.4656943504538092e-15\n\n\n\\[\nA^+ = (A^T A)^{-1} A^T\n\\]\n\\[A = U \\Sigma V^T\\]\n\\[\nA^+ = (V \\Sigma^T U^T U \\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = (V \\Sigma^T\\Sigma V^T)^{-1} V \\Sigma^T U^T  \n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} V^T V \\Sigma^T U^T\n\\]\n\\[\nA^+ = V (\\Sigma^T\\Sigma)^{-1} \\Sigma^T U^T\n\\]\n\\[\nA^+ = V\\Sigma^+ U^T\n\\]\n\\[\nx = V\\Sigma^+ U^T b\n\\]\n\\[\n\\Sigma^+ U^T b = V^T x\n\\]\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = V^T x\n\\]\n\\(y= V^Tx\\)\n\\[\n(\\Sigma^T\\Sigma)^{-1}\\Sigma^T U^T b = y\n\\]\nx = V @ np.linalg.solve(S.T @ S, U.T @ b)\n\n?np.random.randn\n\n\nDocstring:\nrandn(d0, d1, ..., dn)\nReturn a sample (or samples) from the \"standard normal\" distribution.\n.. note::\n    This is a convenience function for users porting code from Matlab,\n    and wraps `standard_normal`. That function takes a\n    tuple to specify the size of the output, which is consistent with\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n.. note::\n    New code should use the\n    `~numpy.random.Generator.standard_normal`\n    method of a `~numpy.random.Generator` instance instead;\n    please see the :ref:`random-quick-start`.\nIf positive int_like arguments are provided, `randn` generates an array\nof shape ``(d0, d1, ..., dn)``, filled\nwith random floats sampled from a univariate \"normal\" (Gaussian)\ndistribution of mean 0 and variance 1. A single float randomly sampled\nfrom the distribution is returned if no argument is provided.\nParameters\n----------\nd0, d1, ..., dn : int, optional\n    The dimensions of the returned array, must be non-negative.\n    If no argument is given a single Python float is returned.\nReturns\n-------\nZ : ndarray or float\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n    the standard normal distribution, or a single such float if\n    no parameters were supplied.\nSee Also\n--------\nstandard_normal : Similar, but takes a tuple as its argument.\nnormal : Also accepts mu and sigma arguments.\nrandom.Generator.standard_normal: which should be used for new code.\nNotes\n-----\nFor random samples from the normal distribution with mean ``mu`` and\nstandard deviation ``sigma``, use::\n    sigma * np.random.randn(...) + mu\nExamples\n--------\n&gt;&gt;&gt; np.random.randn()\n2.1923875335537315  # random\nTwo-by-four array of samples from the normal distribution with\nmean 3 and standard deviation 2.5:\n&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\nType:      builtin_function_or_method\n\n\n\n\nimport matplotlib.pyplot as plt\nx1 = np.linspace(0,1,1000)\ny1 = 3*x1 + np.random.randn(1000)*(x1+0.1) \ny2 = 3*x1 + np.random.randn(1000)*0.5\n\nplt.subplot(1,2,1)\nplt.scatter(x1,y2,s=0.1)\nplt.title(\"Homoscedastic\")\nplt.subplot(1,2,2)\nplt.scatter(x1,y1,s=0.1)\nplt.title(\"Heteroscedastic\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\ncentral_park\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 central_park\n\nNameError: name 'central_park' is not defined\n\n\n\n\nimport sklearn\n\n\nsklearn.datasets\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 sklearn.datasets\n\nAttributeError: module 'sklearn' has no attribute 'datasets'\n\n\n\n\nimport scipy\n\n\nscipy.s"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nWhy can’t you build a car with:\n\nSpeed of Ferrari\nPracticality of minivan\n\n\n\nFerrari Concept"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-1",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nSize brings weight and poor aerodynamics\nPower hurts durability\nCost through the roof\n\n\nFerrari Concept"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-2",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nThis theme applies throughout life and is at the heart of optimization\nYour marketing team gives you a budget\nYour investors don’t want to risk everything\nYour rocketship only has so much fuel"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-3",
    "href": "meetups/Meetup-4/meetup-4.html#everything-in-life-has-constraints-3",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Everything in life has constraints",
    "text": "Everything in life has constraints\n\nWithout constraints, your optimization solution may be meaningless\nFiguring out the important constraints is a key part of modeling\nToday we will start to deal with a simple type of constraint in the context of least squares"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#weekly-summary",
    "href": "meetups/Meetup-4/meetup-4.html#weekly-summary",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Weekly Summary",
    "text": "Weekly Summary\n\nHomework 2 due next Sunday 2/23 at midnight\nReadings for the week are Chapter 16 and 17 of VMLS\nWe will talk about multi-objective problems later (skipped chapter 15)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week!",
    "text": "Events Happening this Week!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-1",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week",
    "text": "Events Happening this Week\n\nCareer Readiness Bootcamp\n\n\nWednesday Februar 19th, Room 407 319 West 31st Street\n\nIn person link\nVirtual link"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-2",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening This Week!",
    "text": "Events Happening This Week!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-3",
    "href": "meetups/Meetup-4/meetup-4.html#events-happening-this-week-3",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Events Happening this Week",
    "text": "Events Happening this Week\n\nNew York Open Statistical Computing Meetup\n\n\n6:30-8:30+, Pless Hall NYU\n$7 tickets must RSVP\nWe have a group that attends these monthly meetups\nI won’t be there this week not sure how the attendance will be\n\nRegister by Clicking Here"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#constrained-least-squares",
    "href": "meetups/Meetup-4/meetup-4.html#constrained-least-squares",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Constrained Least Squares",
    "text": "Constrained Least Squares\n\nWe will learn how to solve least squares with linear equality constraints today\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 \\\\\nC\\mathbf{x} = \\mathbf{d}\n\\]\n\nHere \\(C\\) is a wide matrix\n\\(\\mathbf{x}\\) restricted to hyperplane"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#alternative-formulation",
    "href": "meetups/Meetup-4/meetup-4.html#alternative-formulation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Alternative Formulation",
    "text": "Alternative Formulation\n\nCan also think of this as having \\(p\\) linear constraints\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2, \\\\\n\\mathbf{c}_i^T\\mathbf{x} = d_i,\\quad i=1,...,p\n\\]\n\nHere \\(C\\) is a wide matrix\n\\(\\mathbf{x}\\) restricted to hyperplane"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#why-just-equality",
    "href": "meetups/Meetup-4/meetup-4.html#why-just-equality",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Why Just Equality?",
    "text": "Why Just Equality?\n\nInequality constraints are useful too:\n\n\\[\n\\min_{\\mathbf{x}}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 \\\\\n\\mathbf{c}_i^T\\mathbf{x} \\leq d_i,\\quad i=1,...,p\n\\]\n\nCould do it for one inequality constraint\nBut for more than one you have a complicated region\nNeed methods from later in the semester"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-b-splines",
    "href": "meetups/Meetup-4/meetup-4.html#example-b-splines",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example: B-Splines",
    "text": "Example: B-Splines\n\nSuppose you have a noisy dataset and you want to fit it with a smoother curve\nTalked about polynomials last week \\[\nf(x) = \\sum_{i=0}^p \\theta_i x^i\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features",
    "href": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Polynomials are Bad Features",
    "text": "Polynomials are Bad Features\n\nPolynomials have strong symmetries\nPolynomials “blow-up” at edge of domain"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features-1",
    "href": "meetups/Meetup-4/meetup-4.html#polynomials-are-bad-features-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Polynomials are Bad Features",
    "text": "Polynomials are Bad Features\n\n\nWouldn’t it make more sense to use features with more local structure?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-b-splines-1",
    "href": "meetups/Meetup-4/meetup-4.html#example-b-splines-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example B-Splines",
    "text": "Example B-Splines\n\nIdea of B-Splines is to have local features\nDivide Domain into regions"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region",
    "href": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Do a fit within each region",
    "text": "Do a fit within each region\n\nHave a small number of basis functions within each region\nPiecewise Constant: Basis is \\(1\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region-1",
    "href": "meetups/Meetup-4/meetup-4.html#do-a-fit-within-each-region-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Do a fit within each region",
    "text": "Do a fit within each region\n\nB-Spline uses more polynomials\nCubic is most common \\(1\\), \\(x^2\\), \\(x^3\\), \\(x^4\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#piecewise-cubic-fit",
    "href": "meetups/Meetup-4/meetup-4.html#piecewise-cubic-fit",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Piecewise Cubic Fit",
    "text": "Piecewise Cubic Fit\n\n\nNot the jumps at each of the knots\nTo make the fit function “smooth” we need to add constraints"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#problem-that-was-solved",
    "href": "meetups/Meetup-4/meetup-4.html#problem-that-was-solved",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Problem that was Solved",
    "text": "Problem that was Solved\n\nProblem so far: \\[f_{ij}(x) = x^{i-1},\\quad \\mathrm{if: }\\quad  \\mathrm{knot}_j \\leq x &lt; \\mathrm{knot}_{j+1},\\\\\nf_{ij}(x) = 0,\\quad \\mathrm{otherwise} \\]\n\\(i \\in 1\\,...\\,4\\), \\(j \\in 1\\, ... \\mathrm{number\\, of\\, knots}\\) \\[ A_{k,(4(j-1)+i)} = f_{ij}(x_k) \\] \\[ \\min_{\\theta} \\|A\\mathbf{\\theta} - \\mathbf{x}\\|^2\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#adding-constraints",
    "href": "meetups/Meetup-4/meetup-4.html#adding-constraints",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Adding Constraints",
    "text": "Adding Constraints\n\nWant the predictions to match at each knot: \\[ \\sum_{i=1}^{4} \\theta_{ij}\\left(\\mathrm{knot}_j\\right)^{i-1} = \\sum_{i=1}^{4} \\theta_{i,j+1}\\left(\\mathrm{knot_{j}}\\right)^{i-1} \\]\nAlso want derivatives to match at each knot: \\[\n\\sum_{i=2}^{4} (i-1)\\theta_{ij}\\left(\\mathrm{knot}_j\\right)^{i-2} = \\sum_{i=2}^{4} (i-1)\\theta_{i,j+1}\\left(\\mathrm{knot_{j}}\\right)^{i-2}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#full-problem",
    "href": "meetups/Meetup-4/meetup-4.html#full-problem",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Full Problem",
    "text": "Full Problem\n\nEach knot produces two linear equations\nCan write overall as \\(C\\mathbf{\\theta} = \\mathbf{d}\\)\nFull Problem:\n\n\\[\\mathrm{find:}\\quad \\min_{\\mathbf{\\theta}}\\|A\\mathbf{\\theta} - \\mathbf{x}\\|^2 \\\\\n\\mathrm{subject\\, to:\\quad C\\mathbf{\\theta} = \\mathbf{d}}\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#how-to-solve-it",
    "href": "meetups/Meetup-4/meetup-4.html#how-to-solve-it",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "How to solve it?",
    "text": "How to solve it?\n\nBrute force way\n\n\\(C\\mathbf{\\theta} = \\mathbf{d}\\) can be solved\nGet a “hyperplane” of solutions because underdetermined\nReformulate least squares and solve\n\nOr use Lagrange Multipliers"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#lagrange-multipliers",
    "href": "meetups/Meetup-4/meetup-4.html#lagrange-multipliers",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Lagrange Multipliers",
    "text": "Lagrange Multipliers\n\nConstrained problem into unconstrained\n\n\\[\\mathrm{find:}\\quad \\min_{\\mathrm{\\theta}} \\mathbf{f}(\\mathbf{\\theta}) \\\\\n\\mathrm{subject\\, to:} \\quad \\mathbf{g}(\\mathbf{\\theta}) = 0 \\]\n\nLagrangian: \\[ L(\\mathbf{\\theta},\\mathbf{z}) = f(\\mathbf{\\theta})-\\mathbf{z}^Tg(\\mathbf{\\theta})\\]\nCritical points of \\(L\\) are potential minima of the constrained problem"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#why-does-it-work",
    "href": "meetups/Meetup-4/meetup-4.html#why-does-it-work",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Why does it work?",
    "text": "Why does it work?\n\nCritical points of \\(L\\): \\[\n\\frac{\\partial L(\\mathbf{\\theta},\\mathbf{z})}{\\partial\\theta} = \\nabla \\mathbf{f}(\\theta) - \\mathbf{z}^T\\nabla \\mathbf{g}(\\theta) = 0 \\\\\n\\frac{\\partial L(\\mathbf{\\theta},\\mathbf{z})}{\\partial\\theta} = \\mathbf{g}(\\theta) = 0\n\\]\nGet back the constraint equations\nFirst equation says that gradient of objective has to be made parallel to gradients of constraints"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#geometric-explanation",
    "href": "meetups/Meetup-4/meetup-4.html#geometric-explanation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Geometric Explanation",
    "text": "Geometric Explanation\n\n\\(\\mathbf{g(\\theta)} = 0\\) defines a lower dimensional shape\n\nThe more equations the less dimensionality of the shape\n\\(n\\) equations gives points (0D)\n\\(n-1\\) equations gives curves (1D)\n\\(n-2\\) equations gives surfaces (2D)\n\n\\(\\mathbf{z}^T\\nabla\\mathbf{g}(\\theta)\\) is a linear combination of constraint gradients"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#geometric-explanation-1",
    "href": "meetups/Meetup-4/meetup-4.html#geometric-explanation-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Geometric Explanation",
    "text": "Geometric Explanation"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#lagrange-multiplier-for-least-squares",
    "href": "meetups/Meetup-4/meetup-4.html#lagrange-multiplier-for-least-squares",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Lagrange Multiplier For Least Squares",
    "text": "Lagrange Multiplier For Least Squares\n\\[\nL(\\mathbf{\\theta},\\mathbf{z}) = \\|A\\mathbf{\\theta}-\\mathbf{x}\\|^2 + \\mathbf{z}^T\\left(C\\mathbf{\\theta}-\\mathbf{d}\\right)\n\\]\n\nGives us:\n\n\\[\n\\frac{\\partial L(\\theta,\\mathbf{z})}{\\partial \\theta} =\n  2A^T\\left(A\\mathbf{\\theta} - \\mathbf{x}\\right) + C^T\\mathbf{z} = 0,\\\\\n  \\frac{\\partial L(\\theta,\\mathbf{z})}{\\partial \\theta} = C\\theta -\\mathbf{d} = 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#kkt-matrix",
    "href": "meetups/Meetup-4/meetup-4.html#kkt-matrix",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "KKT Matrix",
    "text": "KKT Matrix\n\nLagrange multiplier equations are linear, can write a matrix equation: \\[\n\\begin{bmatrix}\n2A^TA & C^T \\\\\nC & 0\n\\end{bmatrix}\n\\begin{bmatrix} \\mathbf{\\theta} \\\\ \\mathbf{z}  \\end{bmatrix}\n=\n\\begin{bmatrix} 2A^T\\mathbf{x} \\\\ \\mathbf{d} \\end{bmatrix}\n\\]\nBottom row implements the constraints\nMatrix is called KKT matrix"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#conditions-on-a-and-c",
    "href": "meetups/Meetup-4/meetup-4.html#conditions-on-a-and-c",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Conditions on A and C",
    "text": "Conditions on A and C\n\nWhen is KKT matrix invertible?\n\\(n+p\\times n+p\\)\n\\(C\\) has indepdendent\n\nOtherwise can’t find \\(\\mathbf{z}\\)\nImplies \\(C\\) is wide\n\n\\(\\begin{bmatrix} A \\\\ C\\end{bmatrix}\\) has independent columns\n\nGo wrong if not enough data"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices",
    "href": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "\\(\\mathbf{z}\\) are Shadow Prices",
    "text": "\\(\\mathbf{z}\\) are Shadow Prices\n\nWhat is the meaning of the \\(\\mathbf{z}\\) from the solution?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices-1",
    "href": "meetups/Meetup-4/meetup-4.html#mathbfz-are-shadow-prices-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "\\(\\mathbf{z}\\) are Shadow Prices",
    "text": "\\(\\mathbf{z}\\) are Shadow Prices\n\nWhat is the meaning of the \\(\\mathbf{z}\\) from the solution?"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#shadow-prices",
    "href": "meetups/Meetup-4/meetup-4.html#shadow-prices",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Shadow Prices",
    "text": "Shadow Prices\n\nLet’s say your constraint is negotiable\nChange your constraint to \\(C\\theta = \\mathbf{d}+\\mathbf{\\delta}\\)\nObjective changes by: \\[ \\|A\\theta_{new}-\\mathbf{x}\\|^2 - \\|A\\theta_{old} - \\mathbf{x}\\|^2 \\approx \\mathbf{z}^T\\delta \\]\nIf your objective function was a cost then \\(\\mathbf{z}\\) can be interpreted as a price per unit of constraint."
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-knots",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-knots",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Splines: Knots",
    "text": "Implementing the B-Splines: Knots\n\nnum_knots = 15\nknot_list &lt;- quantile(cherry_blossoms_2$year,\n                      probs = seq(from = 0, to = 1, \n                                length.out = num_knots))\nregions = num_knots\nnum_spline = 4\n\ncherry_blossoms_2 = cherry_blossoms_2 |&gt; mutate(region = cut(year,breaks = knot_list,right=FALSE,labels=FALSE)) |&gt; mutate(\n  region = if_else(is.na(region),num_knots,region))\n\ncherry_blossoms_2 = cherry_blossoms_2 |&gt; mutate(\n  adj_year = 2*(year - min(year))/(max(year)-min(year)) - 1)\n\n\nnormalizing the year variable is important for the condition number of resulting Gram matrix and constraints!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-a",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-splines-a",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Splines: A",
    "text": "Implementing the B-Splines: A\n\nA = matrix(0,nrow = nrow(cherry_blossoms_2),ncol = num_spline*(regions))\n\nfor (n in 1:nrow(cherry_blossoms_2)){\n  region = cherry_blossoms_2$region[n]\n  year = cherry_blossoms_2$year[n]\n  adj_year = cherry_blossoms_2$adj_year[n]\n  A[n,(4*(region-1) + 1):(4*(region))] = c(1,adj_year,adj_year^2,adj_year^3) \n}\n\n\nFor each data point, we find the corresponding region\nFor each region for values of theta\nCalculate basis functions"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#implementing-the-b-spline-c",
    "href": "meetups/Meetup-4/meetup-4.html#implementing-the-b-spline-c",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Implementing the B-Spline: C",
    "text": "Implementing the B-Spline: C\n\nC = matrix(0,nrow = 2*(regions-1),ncol = 4*regions)\n\nmin_year = min(cherry_blossoms_2$year)\nmax_year = max(cherry_blossoms_2$year)\n\nadj_knot_list = 2*(knot_list-min_year)/(max_year-min_year) - 1\n\nfor (i in 1:(regions-1)){\n  C[2*i-1,(4*(i-1)+1):(4*i+4) ] = c(1,adj_knot_list[i+1],\n      adj_knot_list[i+1]^2, adj_knot_list[i+1]^3,\n  -1, -adj_knot_list[i+1], -adj_knot_list[i+1]^2,\n  -adj_knot_list[i+1]^3)\n\n    C[2*i,(4*(i-1)+1):(4*i+4)] = c(0,1,2*adj_knot_list[i+1],\n    3*adj_knot_list[i+1]^2, 0, -1, -2*adj_knot_list[i+1],\n    -3*adj_knot_list[i+1]^2)\n}"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#forming-the-kkt-matrix-and-rhs",
    "href": "meetups/Meetup-4/meetup-4.html#forming-the-kkt-matrix-and-rhs",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Forming the KKT Matrix and RHS",
    "text": "Forming the KKT Matrix and RHS\n\nn_thetas = ncol(A)\np_constraints = nrow(C)\nKKT = matrix(0,nrow = n_thetas+p_constraints ,n_thetas+p_constraints)\n\nKKT[1:n_thetas,1:n_thetas] = 2*t(A) %*% A\nKKT[1:n_thetas,(n_thetas+1):(n_thetas+p_constraints)] = t(C)\nKKT[(n_thetas+1):(n_thetas+p_constraints),1:n_thetas] = C\n\nrhs_ext = matrix(0,nrow = n_thetas+p_constraints,ncol = 1)\n\ndoy = cherry_blossoms_2$doy\n\nrhs_ext[1:n_thetas,1] = 2*t(A) %*% doy"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#solving-the-problem",
    "href": "meetups/Meetup-4/meetup-4.html#solving-the-problem",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\ntheta_ext = solve(KKT,rhs_ext)\ntheta_sol = theta_ext[1:n_thetas]\nz_sol = theta_ext[(n_thetas+1):(n_thetas+p_constraints)]\n\ndoy_pred = A %*% theta_sol\n\ncherry_blossoms_2 |&gt; mutate(doy_pred = doy_pred) |&gt; \n  ggplot(aes(x=year,y=doy)) + geom_point(color=\"red\",alpha=0.2) +\n  geom_vline(data = knot_df,mapping = aes(xintercept=knots)) +\n  geom_line(mapping = aes(x= year, y = doy_pred), color = \"blue\") +\n    labs(title=\"B-Spline Fit\") +\n    ylab(\"Day of Year\") +\n  theme_clean(base_size = 16)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#solving-the-problem-1",
    "href": "meetups/Meetup-4/meetup-4.html#solving-the-problem-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Solving the Problem",
    "text": "Solving the Problem\n\n\nJumps are gone!"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#example-portfolio-optimization",
    "href": "meetups/Meetup-4/meetup-4.html#example-portfolio-optimization",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Example: Portfolio Optimization",
    "text": "Example: Portfolio Optimization\n\nYou have a budget of \\(d\\) dollars to invest in financial assets\nPortfolio allocation weights \\(\\mathbf{w}\\) describe how much you invest in each asset\nYou will hold the portfolio over some well defined period"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-terminology",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-terminology",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Terminology",
    "text": "Portfolio Terminology\n\nShort positions, \\(w_i &lt; 0\\) for some \\(i\\)\nLeverage = \\(\\sum_{i=1}^n |w_i|\\)\n\nMeasures how much money you have borrowed\nHave to pay interest in real world\n\nRisk Free Asset\n\nUsually one asset is a cash or treasury equivalent with low to no risk and return"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-returns",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-returns",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Returns",
    "text": "Portfolio Returns\n\nOn day \\(t\\), value of asset \\(i\\) changes by \\(R_{it}\\)\n\\(R_{it} = 1.05\\) is a \\(5\\%\\) gain, \\(0.91\\) a \\(9\\%\\) loss, \\(0\\) a 100% loss, etc\nAfter an investment period of \\(T\\) days, overall portfolio value: \\[\nd_T = d\\sum_{i=1}^n\\Pi_{t=1}^T R_{it}w_i\n\\]\nNote that returns are multiplicative over time\nOften annualize return: \\(\\rho_{A} = \\rho_T^{T_y/T}\\), where \\(T_y\\) trading days per year"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#portfolio-risk",
    "href": "meetups/Meetup-4/meetup-4.html#portfolio-risk",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Portfolio Risk",
    "text": "Portfolio Risk\n\nDay to day variability of portfolio is called risk\nDefine \\(\\bar{\\rho} = \\frac{1}{T}\\sum_{t=1}^T \\sum_{i=1}^n R_{it}w_t\\)\nCompute variance \\[\n\\mathrm{var}\\left(\\rho\\right) = \\frac{1}{T}\\sum_{t=1}^T\\left(\\sum_{i=1}^n R_{it}w_i - \\bar\\rho \\right)^2\n\\]\nAnnualized risk defined as \\(\\sqrt{T_{y}}\\sqrt{\\mathrm{var(\\rho)}}\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#covariance-formulation",
    "href": "meetups/Meetup-4/meetup-4.html#covariance-formulation",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Covariance Formulation",
    "text": "Covariance Formulation\n\nCan also measure risk based on covariance matrix between daily returns of each asset: \\[\n\\Gamma_{ij} = \\frac{1}{T}\\sum_{i=1}^T \\left(R_{it} -\\bar{\\rho}_i\\right)\\left(R_{jt} - \\bar{\\rho}_j\\right)\n\\]\nThen: \\[\n\\mathrm{var}(\\rho) = \\mathbf{w}^T\\Gamma\\mathbf{w}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#markowitz-portfolio-optimization",
    "href": "meetups/Meetup-4/meetup-4.html#markowitz-portfolio-optimization",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Markowitz Portfolio Optimization",
    "text": "Markowitz Portfolio Optimization\n\nGoal: Use past data to estimate future returns \\(\\rho_i\\) and correlations \\(\\Gamma\\)\nCalculate portfolio weights \\(\\mathbf{w}\\) to minimize risk while achieving a target return\n\n\\[\n\\mathrm{find:}\\quad \\min_{\\mathbf{w}} \\mathbf{w}^T\\Gamma\\mathbf{w} \\\\\n\\mathrm{subject\\, to:}\\quad \\sum_{i=1}^n w_i = 1 \\\\\n\\sum_{i=1}^n \\rho_i w_i = \\rho_{g}\n\\]\n\nThis is a least squares (actually least norm) problem with \\(\\Gamma\\) taking role of Gram matrix \\(A^TA\\)"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nWhich asset would you pick?\n\n\n\nAndrew Lo AM"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-1",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-1",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nMedium reward “no risk” asset was Bernie Madoff\n\n\n\nAndrew Lo AM"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-2",
    "href": "meetups/Meetup-4/meetup-4.html#risk-return-tradeoff-2",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Risk Return Tradeoff",
    "text": "Risk Return Tradeoff\n\nMinimizing risk at a high return target leads to high leverage\nRisk to go bust out of sample"
  },
  {
    "objectID": "meetups/Meetup-4/meetup-4.html#thanks",
    "href": "meetups/Meetup-4/meetup-4.html#thanks",
    "title": "Meetup 4: Constrained Least Squares",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#week-summary",
    "href": "meetups/Meetup-6/meetup-6.html#week-summary",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Week Summary",
    "text": "Week Summary\n\nCovering Section 2.3 of cvxbook\nMaterial is unusual, normal to feel confused\nWe will use cvx for first time\n\ncvxpy\ncvxr\n\nLab 3 Due this Sunday"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#start-thinking-about-miniproject",
    "href": "meetups/Meetup-6/meetup-6.html#start-thinking-about-miniproject",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Start Thinking About Miniproject",
    "text": "Start Thinking About Miniproject\n\nOver next month, start thinking about how your interests relate with something we have covered or are going to cover\nLeast Squares, Convex Optimization, or Deep Learning/non-convex optimization\nAsk me if you want suggestions\nTarget complexity is homework assignment"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#addendum-on-cones",
    "href": "meetups/Meetup-6/meetup-6.html#addendum-on-cones",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Addendum on Cones",
    "text": "Addendum on Cones\n\nNot all cones are convex!"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-1",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-2",
    "href": "meetups/Meetup-6/meetup-6.html#when-is-a-cone-convex-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "When is a cone convex?",
    "text": "When is a cone convex?\n\nConvex iff closed under addition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex?",
    "text": "How to show a set is convex?\n\nSuppose you have a problem and some constraints\n\n\nBrute force math:\n\n\nTry to prove that for every \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in set, \\(\\theta\\mathbf{x}_1+(1-\\theta)\\mathbf{x}_2\\) is in the set, if \\(0\\leq\\theta\\leq1\\)\nOnly for simplest sets"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-1",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex",
    "text": "How to show a set is convex\n\nUsing convex functions (next weeks)\n\n\nGraph of a convex function\nTechnically how CVX works"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-2",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-convex-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is convex",
    "text": "How to show a set is convex\n\nShow how to build your set from simpler convex sets\n\n\nIntersection\nTransformations: Linear/Affine, Perspective, Linear Fractional Mapping"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-not-convex",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-show-a-set-is-not-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to show a set is (not) convex",
    "text": "How to show a set is (not) convex\n\nIf you are stuck, brute force numerics\n\n\nGenerate random points in set\nCheck random convex combinations\nWait a long time"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection",
    "href": "meetups/Meetup-6/meetup-6.html#intersection",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection",
    "text": "Intersection\n\nIntersection is shared points\n\n\n\n\\[ C_3 = C_1 \\cap C_2 \\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersections-of-convex-are-convex",
    "href": "meetups/Meetup-6/meetup-6.html#intersections-of-convex-are-convex",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersections of Convex are Convex",
    "text": "Intersections of Convex are Convex\n\n\n\n\n\nEven an infinite intersection"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nSuppose we can several locations where we are allowed to build some quantity of renewable energy plants."
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-1",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\n\\(p_i(t)\\) power function for each plant over a typical day"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-2",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\n\\(p_i(t)\\) power function for each plant over a typical day"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-3",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-3",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nLet \\(w_i\\) stand for the size of renewable plant we build at location \\(i\\)."
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-4",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-example-4",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints Example",
    "text": "Infinite Constraints Example\n\nThe total power generated at time \\(t\\): \\[\np_{tot}(t) = \\mathbf{p}(t)^T\\mathbf{w}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints",
    "text": "Infinite Constraints\n\nGuarantee that the level of power above demand level:\n\n\\[\n\\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#infinite-constraints-1",
    "href": "meetups/Meetup-6/meetup-6.html#infinite-constraints-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Infinite Constraints",
    "text": "Infinite Constraints\n\nFor each individual time point \\(t\\), this inequality describes a convex set\nBecause \\(\\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t)\\) linear in \\(\\mathbf{w}\\)\nTake intersection over \\(t\\) \\[\nC = \\bigcap_t \\{\\mathbf{w}\\,|\\, \\mathbf{p}(t)^T\\mathbf{w} \\geq d_{min}(t) \\}\n\\]\n\\(C\\) is a convex set"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nCan apply a function \\(f\\) to an entire set to generate a new set\nDoesn’t have to be in the same space: \\(f:\\, \\mathbb{R}^m \\mapsto \\mathbb{R}^n\\)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set-1",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nImage of \\(C\\) under \\(f\\):\n\n\\[f(C) = \\{x\\, |\\, x = f(y)\\, \\mathrm{for}\\, y \\in C\\}\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#mapping-a-set-2",
    "href": "meetups/Meetup-6/meetup-6.html#mapping-a-set-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Mapping a Set",
    "text": "Mapping a Set\n\nOther Direction: Preimage of \\(C\\) under \\(f\\):\n\n\\[ f^{-1}(C) = \\{x\\, |\\, f(x) \\in C\\} \\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#linearaffine-transformations",
    "href": "meetups/Meetup-6/meetup-6.html#linearaffine-transformations",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Linear/Affine Transformations",
    "text": "Linear/Affine Transformations\n\nIf \\(C\\) is convex, and \\(f\\) is affine \\(f(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b}\\)\n\n\\(f(C)\\) is convex\n\\(f^{-1}(C)\\) is convex\n\nCan scale and shift convex sets"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#implication",
    "href": "meetups/Meetup-6/meetup-6.html#implication",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Implication:",
    "text": "Implication:\n\nConsider following set of constraints:\n\n\\[\nA\\mathbf{x} \\leq \\mathbf{d} \\\\\nx_1^2 + x_2^2 \\leq 4\n\\]\n\nFirst constraint is convex set\nSecond one describes a set where \\(x_1\\) and \\(x_2\\) are inside a circle, but rest of coordinates unconstrained"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#implication-1",
    "href": "meetups/Meetup-6/meetup-6.html#implication-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Implication:",
    "text": "Implication:\n\nDefine operator \\(P\\) that is identity on \\(x_1\\) and \\(x_2\\) and ignores the rest:\n\n\\[\nP = \\begin{bmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\\n                    0 & 1 & 0 & \\cdots & 0 \\end{bmatrix}\n\\]\n\nCircle of radius \\(2\\) in \\(\\mathbb{R}^2\\) is convex\nPreimage of circle under \\(P\\) is also convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#cartesian-product",
    "href": "meetups/Meetup-6/meetup-6.html#cartesian-product",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Cartesian Product",
    "text": "Cartesian Product\n\nHave two sets of variables \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\).\nHave two convex sets \\(C_1\\) and \\(C_2\\), over each set of variables\nCartesian product of \\(C_1\\) and \\(C_2\\) is convex: \\[\nC_1\\times C_2 = \\{ \\begin{bmatrix} \\mathbf{x}_1 \\\\ \\mathbf{x}_2 \\end{bmatrix} \\, | \\, \\mathbf{x}_1 \\in C_1, \\mathbf{x}_2 \\in C_2 \\}\n\\]\nCan freely combine convex sets in different groups of variables"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line",
    "href": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection with an arbitrary line",
    "text": "Intersection with an arbitrary line\n\n\\(C\\) is convex if and only if \\(C \\cap L\\) is convex for every line \\(L\\)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line-1",
    "href": "meetups/Meetup-6/meetup-6.html#intersection-with-an-arbitrary-line-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intersection with an arbitrary line",
    "text": "Intersection with an arbitrary line\n\n\\(C\\) is convex if and only if \\(C \\cap L\\) is convex for every line \\(L\\)\nIf \\(C\\) is convex, then it is true by intersection\nIf all intersections are convex, then all intersections are segments so \\(C\\) is convex by the definition"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#application-quadratic-inequality",
    "href": "meetups/Meetup-6/meetup-6.html#application-quadratic-inequality",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Application: Quadratic Inequality",
    "text": "Application: Quadratic Inequality\n\nConsider \\(A\\succeq 0\\)\n\\[\n\\{\\mathbf{x}\\, |\\, \\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\}\n\\]\nWhy intuitively should this be convex?\n\n\\(\\mathbf{x}^TA\\mathbf{x} \\leq \\alpha\\) is an ellipsoid"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality\n\nConsider \\(A\\succeq 0\\)\n\\[\n\\{\\mathbf{x}\\, |\\, \\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\}\n\\]\nDefine line \\(L = \\{\\mathbf{x}\\, |\\, \\mathbf{x} = t\\mathbf{v} + \\mathbf{v}_0,\\, t\\in\\mathbb{R} \\}\\)\nSubstitute: \\[\nt^2 (\\mathbf{v}^TA\\mathbf{v} ) + t(2\\mathbf{v}^TA\\mathbf{v_0} + \\mathbf{v}^T\\mathbf{b}) \\leq \\alpha - \\mathbf{v_0}^T A\\mathbf{v_0} - \\mathbf{v}_0^T\\mathbf{b}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-1",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality\n\nTwo possibilities:\n\\(\\mathbf{v}^T A \\mathbf{v} = 0\\)\n\nThen it is convex because polyhedron\n\n\\(\\mathbf{v}^TA\\mathbf{v} \\geq 0\\)\n\\(\\mathbf{x}^T A \\mathbf{x} + \\mathbf{x}^T b \\leq \\alpha\\) is convex\nDoesn’t work if inequality is in other direction"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-2",
    "href": "meetups/Meetup-6/meetup-6.html#quadratic-inequality-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Quadratic Inequality",
    "text": "Quadratic Inequality"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-problem",
    "href": "meetups/Meetup-6/meetup-6.html#example-problem",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example Problem",
    "text": "Example Problem\n\nSuppose we have a variable \\(x\\) which takes values \\(a_i\\) for \\(i=1,\\cdots,n\\)\n\\(p(x=a_i) = p_i\\) is a probability distribution\nIs the set \\(\\{\\mathbf{p}\\, |\\, \\mathrm{var}(x) \\geq \\alpha\\}\\) convex?"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#how-to-approach",
    "href": "meetups/Meetup-6/meetup-6.html#how-to-approach",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "How to approach?",
    "text": "How to approach?\n\n\\(\\mathrm{var}(x) = \\sum_{i} p_i(a_i - \\bar{x})^2\\)\nExpand it out more: \\[\n\\mathrm{var}(x) = \\sum_{i} \\left(p_i a_i^2 \\right) - \\left(\\sum_i p_i a_i\\right)^2 \\geq \\alpha\n\\]\nThis is a quadratic inequality with matrix \\(A=\\mathrm{diag}(a_i^2)\\)\nThus convex\nThen intersection with simplex also convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-problem-1",
    "href": "meetups/Meetup-6/meetup-6.html#example-problem-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example Problem",
    "text": "Example Problem\n\nWhat if we wanted variance lower than \\(\\alpha\\)?\nNot convex in general"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#perspective-function",
    "href": "meetups/Meetup-6/meetup-6.html#perspective-function",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Perspective Function",
    "text": "Perspective Function\n\n\\(f(\\mathbf{x},t) = \\frac{\\mathbf{x}}{t}\\), for \\(t&gt;0\\)\nInterpretation: Image taken through a pinhole camera with hole at origin, surface at \\(t=-1\\)\n\\(f(C)\\) and \\(f^{-1}(C)\\) both convex if \\(C\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#linear-fractional-function",
    "href": "meetups/Meetup-6/meetup-6.html#linear-fractional-function",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Linear Fractional Function",
    "text": "Linear Fractional Function\n\nPerspective is special case\n\\(f(\\mathbf{x}) = \\frac{A\\mathbf{x}+\\mathbf{b}}{\\mathbf{c}^T\\mathbf{x}+\\mathbf{d}}\\)\n\nfor \\(\\mathbf{c}^T\\mathbf{x} + \\mathbf{d} \\gt 0\\)\n\n\\(f(C)\\) and \\(f^{-1}(C)\\) both convex if \\(C\\) is convex"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#example-conditional-probability",
    "href": "meetups/Meetup-6/meetup-6.html#example-conditional-probability",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Example: Conditional Probability",
    "text": "Example: Conditional Probability\n\nConsider discrete probability distribution over two variables: \\[p(i,j) = \\mathrm{prob(x=i,y=j)}\\]\nConditional probability \\(p(i|j) = \\frac{p(i,j)}{\\sum_{i'}p(i',j)}\\)\nLinear fractional function of joint probabilities\nA convex set of joint probabilities becomes a convex set of conditional probabilities"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nCVX is software that solves convex optimization problems\nBased on disciplined convex programming or DCP\nHas library of functions with known curvature\nCVX verifies convexity of objectives and constraints"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-1",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-1",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nBuild expressions out of variables and constants\n\n\nimport cvxpy as cvx\n\n# 6 dimensional vector of unknowns\nx = cvx.Variable(6)\n\n# 12x6 dimensional matrix of unknowns\nA = cvx.Variable((12,6))\n\n# Scalar variable\ny = cvx.Variable()\n\n# Constants are numpy ndarrays\n\nx_mean = np.random.randn(6)"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-2",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-2",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\natomic functions define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check convexity?\nobj = cvx.Minimize(cvx.norm(x))\nprint(obj.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-3",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-3",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check convexity of constraints?\n\nprint((x - x_mean &gt;= - margin).is_dcp())\nprint((x - x_mean &lt;=   margin).is_dcp())\n\nTrue\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-4",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-4",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check everything at once?\n\nprint(problem.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-5",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-5",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nUse atomic functions to define objective and constraints\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Check everything at once?\n\nprint(problem.is_dcp())\n\nTrue"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-6",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-6",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nSolve using problem.solve()\n\n\nmargin = 0.2\n\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n# Solve the problem\n\nsol = problem.solve()\nprint(sol) # objective\nx.value # optimum\n\n2.085815389132131\n\n\narray([ 5.00536948e-01,  1.22249859e+00, -1.18787445e+00,  1.26308054e-05,\n        4.98878729e-01,  9.72450571e-01])"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-7",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-7",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\nHow about weighted least squares?\n\n\nmargin = 0.2\nw = np.sqrt(np.random.random(6))\nproblem = cvx.Problem(\n  cvx.Minimize(cvx.norm(np.diag(w) @ x)), # Objective (this one is least squares)\n  [x - x_mean &gt;= -margin , # These are the constraints\n  x - x_mean &lt;= margin]\n)\n\n\\[\n\\min_{x} \\|\\mathrm{diag}(w_i)\\mathbf{x}\\| \\\\\n\\mathbf{x}-\\mathbf{x}_0 \\geq -\\sigma \\\\\n\\mathbf{x} - \\mathbf{x}_0 \\leq \\sigma\n\\]"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-8",
    "href": "meetups/Meetup-6/meetup-6.html#intro-to-cvx-8",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Intro to CVX",
    "text": "Intro to CVX\n\n# Solve the problem\n\nsol = problem.solve()\nprint(sol) # objective\nx.value # optimum\n\n1.110221316741478\n\n\narray([ 5.00536948e-01,  1.22249859e+00, -1.18787445e+00, -5.62329444e-05,\n        4.98878728e-01,  9.72450572e-01])"
  },
  {
    "objectID": "meetups/Meetup-6/meetup-6.html#thanks",
    "href": "meetups/Meetup-6/meetup-6.html#thanks",
    "title": "DATA 609 Meetup 6: Constructing Convex Sets",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#week-summary",
    "href": "meetups/Meetup7/meetup-7.html#week-summary",
    "title": "DATA 609 Meetup 7",
    "section": "Week Summary",
    "text": "Week Summary\n\nWorking on grading….\nThis week: definition and examples of convex functions\nNext week: How to construct convex functions\nReading: Section 3.1, CVX tutorials\nLab 4 available"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function",
    "href": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function",
    "title": "DATA 609 Meetup 7",
    "section": "What is a convex function?",
    "text": "What is a convex function?\n\nStart with function \\(f:\\, \\mathbb{R}^N\\to\\mathbb{R}\\)\nGraph of \\(f\\) is below line between any two points on graph:"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function-1",
    "href": "meetups/Meetup7/meetup-7.html#what-is-a-convex-function-1",
    "title": "DATA 609 Meetup 7",
    "section": "What is a convex function?",
    "text": "What is a convex function?\n\\[\nf(\\theta \\mathbf{x}_1 + (1-\\theta)\\mathbf{x}_2) \\leq \\theta f(\\mathbf{x}_1) + (1-\\theta)f(\\mathbf{x}_2)\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#concave",
    "href": "meetups/Meetup7/meetup-7.html#concave",
    "title": "DATA 609 Meetup 7",
    "section": "Concave",
    "text": "Concave\n\n\\(f\\) is concave if \\(-f\\) is convex\n\\(f\\) curves down instead of up"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#strict-concavity",
    "href": "meetups/Meetup7/meetup-7.html#strict-concavity",
    "title": "DATA 609 Meetup 7",
    "section": "Strict Concavity",
    "text": "Strict Concavity\n\nEqualty only at endpoints, i.e. for \\(\\theta \\neq 0, 1\\): \\[\nf(\\theta \\mathbf{x}_1 + (1-\\theta)\\mathbf{x}_2) \\leq \\theta f(\\mathbf{x}_1) + (1-\\theta)f(\\mathbf{x}_2)\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#graph-in-higher-dimensions",
    "href": "meetups/Meetup7/meetup-7.html#graph-in-higher-dimensions",
    "title": "DATA 609 Meetup 7",
    "section": "Graph in Higher Dimensions",
    "text": "Graph in Higher Dimensions\n\ngraph of a function \\(f: \\mathbf{dom}(f)\\to\\mathbb{R}\\): \\[G(f) = \\{(\\mathbf{x},f(\\mathbf{x}))\\,|\\, \\mathbf{x}\\in\\mathbf{dom}(f)\\}\\]\n1D: \\((x,y=f(x))\\), familiar graph\n2D: \\((x,y,z=f(x,y))\\), 3D surface/landscape\n3D+: \\(\\left(x_1,x_2,x_3,...,x_n,z=f(x_1,x_2,x_3,...,x_n)\\right)\\)\n\nHypersurface"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#tangent-lines",
    "href": "meetups/Meetup7/meetup-7.html#tangent-lines",
    "title": "DATA 609 Meetup 7",
    "section": "Tangent Lines",
    "text": "Tangent Lines\n\nGraph lies above tangent lines/planes/hyperplanes"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-to-global-info",
    "href": "meetups/Meetup7/meetup-7.html#local-to-global-info",
    "title": "DATA 609 Meetup 7",
    "section": "Local to Global Info",
    "text": "Local to Global Info\n\nTangent line is a local property"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-to-global-info-1",
    "href": "meetups/Meetup7/meetup-7.html#local-to-global-info-1",
    "title": "DATA 609 Meetup 7",
    "section": "Local to Global Info",
    "text": "Local to Global Info\n\nBut for Convex gives global information!"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-min-is-global-min",
    "href": "meetups/Meetup7/meetup-7.html#local-min-is-global-min",
    "title": "DATA 609 Meetup 7",
    "section": "Local Min is Global Min",
    "text": "Local Min is Global Min\n\nLocal minimum implies flat tangent"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#what-if-there-is-no-single-tangent",
    "href": "meetups/Meetup7/meetup-7.html#what-if-there-is-no-single-tangent",
    "title": "DATA 609 Meetup 7",
    "section": "What if there is no single tangent?",
    "text": "What if there is no single tangent?\n\n\nSlightly different argument is needed"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#local-min-is-a-global-min",
    "href": "meetups/Meetup7/meetup-7.html#local-min-is-a-global-min",
    "title": "DATA 609 Meetup 7",
    "section": "Local Min is a Global Min",
    "text": "Local Min is a Global Min\n\nTake \\(f:\\,\\mathbb{R}^2\\to\\mathbb{R}\\)\nAssume there is a local min that isn’t the global min"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-affine",
    "href": "meetups/Meetup7/meetup-7.html#examples-affine",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Affine",
    "text": "Examples: Affine\n\nAffine \\(f(\\mathbf{x}) = \\mathbf{c}^T\\mathbf{x} + \\mathbf{d}\\)\nBoth convex and concave"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConvex for \\(\\alpha\\geq 1\\) on \\(\\mathbb{R}_+\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions-1",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions-1",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConcave for \\(0&lt;\\alpha\\leq 1\\) on \\(\\mathbb{R}_+\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-power-functions-2",
    "href": "meetups/Meetup7/meetup-7.html#examples-power-functions-2",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Power Functions",
    "text": "Examples: Power Functions\n\nPower Functions: \\(f(x) = x^{\\alpha}\\)\n\nConvex for \\(\\alpha\\leq 0\\) on \\(\\mathbb{R}_{++}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-xalpha",
    "href": "meetups/Meetup7/meetup-7.html#examples-xalpha",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: \\(|x|^{\\alpha}\\)",
    "text": "Examples: \\(|x|^{\\alpha}\\)\n\n\\(f(x) = |x|^{\\alpha}\\) is convex on \\(\\mathbb{R}\\) for \\(\\alpha\\geq 1\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-ealpha-x",
    "href": "meetups/Meetup7/meetup-7.html#examples-ealpha-x",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: \\(e^{\\alpha x}\\)",
    "text": "Examples: \\(e^{\\alpha x}\\)\n\n\\(e^{\\alpha x}\\) is convex for any \\(\\alpha\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-rectified-linear-unit",
    "href": "meetups/Meetup7/meetup-7.html#examples-rectified-linear-unit",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Rectified Linear Unit",
    "text": "Examples: Rectified Linear Unit\n\n\\(\\max(0,x)\\) is convex\n\\(\\min(0,x)\\) is concave"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-log-and-entropy",
    "href": "meetups/Meetup7/meetup-7.html#examples-log-and-entropy",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: log and entropy",
    "text": "Examples: log and entropy\n\n\\(f(x) = \\log(x)\\) is concave on \\(\\mathbb{R}_{++}\\)\n\\(f(x) = -x\\log(x)\\) is concave on \\(\\mathbb{R}_{+}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#examples-norms",
    "href": "meetups/Meetup7/meetup-7.html#examples-norms",
    "title": "DATA 609 Meetup 7",
    "section": "Examples: Norms",
    "text": "Examples: Norms\n\nAll norms are convex on \\(\\mathbb{R}^N\\to\\mathbb{R}\\) \\[ \\|\\mathbf{x}\\| = \\left(\\sum_i x_i^2\\right)^{1/2} , \\|\\mathbf{x}\\|_p = \\left(\\sum_i x_i ^p\\right)^{1/p} \\\\\n\\|\\mathbf{x}\\|_{\\infty} = \\max(x_1,x_2,...,x_n)\n\\]\nSum of squares also \\(\\|\\mathbf{x}\\|^2 = \\mathbf{x}^T\\mathbf{x}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nConsider Least Squares Problem\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2\n\\]\n\nSuppose \\(\\mathbf{x}\\) has many entries\n\\(A\\) could even be wide\nYou only want to use a few features\nYour data may have outliers"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-1",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-1",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nApply the following penalty:\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 + \\lambda \\sum_i |x_i|\n\\]\n\nEquivalent to a norm:\n\n\\[\n\\min_{\\mathbf{x}} \\|A\\mathbf{x}-\\mathbf{b}\\|^2 + \\lambda\\|\\mathbf{x}\\|_1\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-2",
    "href": "meetups/Meetup7/meetup-7.html#lasso-and-sparse-regression-2",
    "title": "DATA 609 Meetup 7",
    "section": "Lasso and Sparse Regression",
    "text": "Lasso and Sparse Regression\n\nWe will learn that adding convex functions preserves convexity\nLasso regularization of convex problem also convex\nHas a tendency to produce robust solutions with sparse coefficients \\(\\mathbf{x}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#softmax-and-geometric-mean",
    "href": "meetups/Meetup7/meetup-7.html#softmax-and-geometric-mean",
    "title": "DATA 609 Meetup 7",
    "section": "Softmax and Geometric Mean",
    "text": "Softmax and Geometric Mean\n\n\\(\\log\\left(\\sum_{i=1}^n e^{x_i}\\right)\\) is convex\n\nThis is a smoothed maximum function\n\n\\(\\left(\\Pi_{i=1}^n x_i\\right)^{1/n}\\)\n\nGeometric mean"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#first-and-second-order-conditions",
    "href": "meetups/Meetup7/meetup-7.html#first-and-second-order-conditions",
    "title": "DATA 609 Meetup 7",
    "section": "First and Second Order Conditions",
    "text": "First and Second Order Conditions\n\nFirst Order Condition: \\(f\\) is convex if and only if graph lies above all tangent lines\nSecond Order Condition: \\(f\\) is convex if and only if the Hessian is positive semi-definite at all points in (convex) domain \\[\n\\left(\\nabla^2{f}\\right)_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} \\succeq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#second-order-condition",
    "href": "meetups/Meetup7/meetup-7.html#second-order-condition",
    "title": "DATA 609 Meetup 7",
    "section": "Second Order Condition",
    "text": "Second Order Condition\n\nShow that \\(f(x) = e^{\\alpha x}\\) is convex\nTake first derivative:\n\n\\(f'(x) = \\alpha e^{\\alpha x}\\)\n\nTake second derivative:\n\n\\(f''(x) = \\alpha^2 e^{\\alpha x}\\)\n\n\\(f''(x) \\geq 0\\), so \\(f\\) is convex\nAlso implies sum of convex is convex"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#quadratic-over-linear",
    "href": "meetups/Meetup7/meetup-7.html#quadratic-over-linear",
    "title": "DATA 609 Meetup 7",
    "section": "Quadratic over Linear",
    "text": "Quadratic over Linear\n\n\\(f(x,y) = \\frac{x^2}{y}\\), \\(\\mathbf{dom}(f) = \\mathbb{R}\\times\\mathbb{R}_{++}\\)\nStart taking derivatives \\[\n\\nabla f = \\begin{bmatrix} \\frac{2x}{y} & -\\frac{x^2}{y^2} \\end{bmatrix}^T\n\\]\n\n\\[\\nabla^2 f = \\begin{bmatrix} \\frac{2}{y} & -\\frac{2x}{y^2} \\\\\n-\\frac{2x}{y^2} & \\frac{2x^2}{y^3} \\end{bmatrix}\n\\]\n\nPositive semidefinite because \\(\\mathrm{det}(\\nabla^2 f)\\geq 0\\) and \\(Tr(\\nabla^2 f) &gt; 0\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#restriction-to-line",
    "href": "meetups/Meetup7/meetup-7.html#restriction-to-line",
    "title": "DATA 609 Meetup 7",
    "section": "Restriction to Line",
    "text": "Restriction to Line\n\n\\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) is convex if and only if it is convex when restricted to every single line in \\(\\mathbb{R}^n\\)\n\\(g(t) = f(\\mathbf{b}+t\\mathbf{v})\\) is convex for all vectors \\(\\mathbf{v}\\), \\(\\mathbf{b}\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#epigraph",
    "href": "meetups/Meetup7/meetup-7.html#epigraph",
    "title": "DATA 609 Meetup 7",
    "section": "Epigraph",
    "text": "Epigraph\n\n\\(\\mathbf{epi}(f)\\) is set of points above the graph of \\(f\\) \\[\n\\{(\\mathbf{x},z)\\,| \\mathbf{x}\\in\\mathbf{dom}(f),\\, z\\geq f(\\mathbf{x})\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#epigraph-1",
    "href": "meetups/Meetup7/meetup-7.html#epigraph-1",
    "title": "DATA 609 Meetup 7",
    "section": "Epigraph",
    "text": "Epigraph\n\n\\(f\\) is convex if and only if \\(\\mathbf{epi}(f)\\) is a convex set\nAllows for construction of more convex sets\n\\(f\\) is concave if and only if \\(\\mathbf{sub}(f)\\) is a convex set\nThis is convex: \\[\n\\{(x,y)| \\log(x) \\leq y \\ \\leq e^{\\alpha x}\\}\n\\]"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#subsuper-level-sets",
    "href": "meetups/Meetup7/meetup-7.html#subsuper-level-sets",
    "title": "DATA 609 Meetup 7",
    "section": "Sub/Super Level Sets",
    "text": "Sub/Super Level Sets\n\nSolution set of inequality defines sub: \\[\\mathbf{sub}_{\\alpha}(f) = \\{\\mathbf{x}| f(\\mathbf{x}\\leq \\alpha)\\}\\]\nSuper-level set has oppositve inequality\n\\(\\alpha\\)-sublevel sets of a convex function are convex\n\\(\\alpha\\)-superlevel sets of a concave function are convex"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensens-inequality-project",
    "href": "meetups/Meetup7/meetup-7.html#jensens-inequality-project",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen’s Inequality (Project?)",
    "text": "Jensen’s Inequality (Project?)\n\nUseful result in probability and statistics:\nIf \\(f\\) is a convex function and \\(x\\) a random variable, then: \\[\n\\mathrm{E}[f(x)]\\geq f(\\mathrm{E}[x])\n\\]\nBecause expectation is \\(\\sum_i p_i x_i\\), convex combination of values of \\(x\\)"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensens-inequality",
    "href": "meetups/Meetup7/meetup-7.html#jensens-inequality",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen’s Inequality",
    "text": "Jensen’s Inequality\n\nwikipedia"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#convexityconcavity-important-in-life",
    "href": "meetups/Meetup7/meetup-7.html#convexityconcavity-important-in-life",
    "title": "DATA 609 Meetup 7",
    "section": "Convexity/Concavity important in life",
    "text": "Convexity/Concavity important in life"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensen-and-fragility",
    "href": "meetups/Meetup7/meetup-7.html#jensen-and-fragility",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen and Fragility",
    "text": "Jensen and Fragility\n\nSuppose something in your life is random\nConsider your payoff function \\(f(x)\\)\nIf your \\(f\\) is convex, Jensen says randomness helps you\nYour Outcome = \\(\\mathrm{E}[f(x)]\\) better than outcome of average \\(f(\\mathrm{E}[x])\\)\nIf your payoff is concave, Jensen says randomness hurts you"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#car-traffic-example",
    "href": "meetups/Meetup7/meetup-7.html#car-traffic-example",
    "title": "DATA 609 Meetup 7",
    "section": "Car Traffic Example",
    "text": "Car Traffic Example\n\n\n\nFew cars, drive speed limit\nCars above threshold, speed drops\nExpected Traffic is 30 cars\nExpected Time 65 minutes\n\n\n| Cars | Time       |\n|------|------------|\n| 1    | 30 minutes |\n| 10   | 30 minutes |\n| 20   | 30 minutes |\n| 30   | 40 minutes |\n| 40   | 1 hour     |\n| 50   | 1.5 hours  |\n| 60   | 3 hours    |"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#jensen-and-fragility-1",
    "href": "meetups/Meetup7/meetup-7.html#jensen-and-fragility-1",
    "title": "DATA 609 Meetup 7",
    "section": "Jensen and Fragility",
    "text": "Jensen and Fragility\n\nConvex payouts are called anti-fragile\nConcave payouts are called fragile\nMany times in life we don’t can’t fully quantify the risks we face\nBut sometimes we can control the outcome for ourselves should something bad happen\nMoral- make your payouts convex if you can"
  },
  {
    "objectID": "meetups/Meetup7/meetup-7.html#thanks",
    "href": "meetups/Meetup7/meetup-7.html#thanks",
    "title": "DATA 609 Meetup 7",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Python3.html",
    "href": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Python3.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import random\nimport numpy as np\n\nclass Network:\n    def __init__(self, sizes):\n        # sizes = list of number of neurons in each layer\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        # Biases: one (n,1) vector per layer (except input)\n        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n        # Weights: one (next_layer, this_layer) matrix\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n    def feedforward(self, a):\n        # Pass input through network\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a) + b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n        # training_data = list of (x, y) tuples\n        if test_data:\n            n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)\n            ]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n            else:\n                print(f\"Epoch {j} complete\")\n\n    def update_mini_batch(self, mini_batch, eta):\n        # Apply a single step of gradient descent\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            x = x.reshape((-1, 1))  # Ensure x is (784, 1)\n            y = y.reshape((-1, 1)) if y.ndim == 1 else y  # optional for y\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w - (eta / len(mini_batch)) * nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b - (eta / len(mini_batch)) * nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        # Return (nabla_b, nabla_w) representing gradient\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        activation = x\n        activations = [x]  # list to store all activations, layer by layer\n        zs = []  # list to store all z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation) + b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # Backward pass\n        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        # Return number of correct classifications\n        test_results = [(np.argmax(self.feedforward(x.reshape((-1, 1)))), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        return (output_activations - y)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\n\nimport gzip\n\n\ndef load_mnist_images(filename):\n    with gzip.open(filename, 'rb') as f:\n        # Skip the header: first 16 bytes\n        f.read(16)\n        # Read the rest and reshape\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        images = data.reshape(-1, 28*28).astype(np.float32)\n        # Normalize pixel values from [0, 255] to [0.0, 1.0]\n        images /= 255.0\n        return images\n\ndef load_mnist_labels(filename):\n    with gzip.open(filename, 'rb') as f:\n        # Skip the header: first 8 bytes\n        f.read(8)\n        labels = np.frombuffer(f.read(), dtype=np.uint8)\n        return labels\n\ndef load_data():\n    # Path to your MNIST data files (downloaded from the site)\n    train_images = load_mnist_images('../data/MNIST/raw/train-images-idx3-ubyte.gz')\n    train_labels = load_mnist_labels('../data/MNIST/raw/train-labels-idx1-ubyte.gz')\n    test_images = load_mnist_images('../data/MNIST/raw/t10k-images-idx3-ubyte.gz')\n    test_labels = load_mnist_labels('../data/MNIST/raw/t10k-labels-idx1-ubyte.gz')\n\n    # Package into (image, label) tuples\n    training_data = list(zip(train_images, train_labels))\n    test_data = list(zip(test_images, test_labels))\n    return training_data, test_data\n\n\ntraining_data, test_data = load_data()\n\n# Initialize the network\nnet = Network([784, 30, 10])\n\n# Train!\nnet.SGD(training_data, epochs=30, mini_batch_size=10, eta=3.0, test_data=test_data)\n\nEpoch 0: 1489 / 10000\nEpoch 1: 881 / 10000\nEpoch 2: 722 / 10000\nEpoch 3: 813 / 10000\nEpoch 4: 706 / 10000\nEpoch 5: 836 / 10000\nEpoch 6: 976 / 10000\nEpoch 7: 992 / 10000\nEpoch 8: 855 / 10000\nEpoch 9: 893 / 10000\nEpoch 10: 965 / 10000\nEpoch 11: 971 / 10000\nEpoch 12: 699 / 10000\nEpoch 13: 978 / 10000\nEpoch 14: 976 / 10000\nEpoch 15: 974 / 10000\nEpoch 16: 978 / 10000\nEpoch 17: 962 / 10000\nEpoch 18: 942 / 10000\nEpoch 19: 898 / 10000\nEpoch 20: 695 / 10000\nEpoch 21: 791 / 10000\nEpoch 22: 866 / 10000\nEpoch 23: 953 / 10000\nEpoch 24: 832 / 10000\nEpoch 25: 1044 / 10000\nEpoch 26: 718 / 10000\nEpoch 27: 1042 / 10000\nEpoch 28: 283 / 10000\nEpoch 29: 766 / 10000\n\n\n\nnet\n\n&lt;__main__.Network at 0x7566664eb790&gt;\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass Network(nn.Module):\n    def __init__(self, sizes):\n        super(Network, self).__init__()\n        self.sizes = sizes\n        self.num_layers = len(sizes)\n        \n        self.layers = nn.ModuleList()\n        for i in range(self.num_layers - 1):\n            layer = nn.Linear(sizes[i], sizes[i+1])\n            nn.init.xavier_uniform_(layer.weight)   # Xavier initialization\n            #nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n\n            nn.init.zeros_(layer.bias)               # Bias to zero\n            self.layers.append(layer)\n    \n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = F.relu(layer(x))   # hidden layers with sigmoid activation\n        x = self.layers[-1](x) \n        #x = F.softmax(self.layers[-1](x), dim=1)  # output layer with softmax\n        return x\n\n\n\n\n\ndef train(network, train_data, epochs, mini_batch_size, eta, test_data=None):\n    optimizer = optim.SGD(network.parameters(),momentum=0.8,nesterov=True, lr=eta,weight_decay=1e-4)\n    \n    step_size = 2\n    gamma = 0.7\n    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    #optimizer = optim.Adam(network.parameters(),be,nesterov=True, lr=eta,weight_decay=1e-5)\n\n    \n    loss_fn = nn.CrossEntropyLoss()\n    \n    loss_history = []      # to store average loss per epoch\n    accuracy_history = []  # to store test accuracy per epoch\n    grad_norm_history = [] # to store gradient norms per epoch\n\n    for epoch in range(epochs):\n        network.train()\n        running_loss = 0.0\n        batch_count = 0\n        total_grad_norm = 0.0\n        \n        for data, target in train_data:\n            optimizer.zero_grad()\n            output = network(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            \n            # Compute gradient norm (L2 norm)\n\n            # Get gradients for first and last layer weights\n            first_weight_grad = network.layers[0].weight.grad\n            last_weight_grad = network.layers[-1].weight.grad\n\n            # Compute L2 norms\n            first_norm = first_weight_grad.norm(2).item() if first_weight_grad is not None else 0.0\n            last_norm = last_weight_grad.norm(2).item() if last_weight_grad is not None else 1e-8  # avoid div by zero\n\n            # Compute ratio: first / last\n            grad_ratio = first_norm / last_norm\n            total_grad_norm += grad_ratio\n\n            \n            #grad_norm = sum(p.grad.norm()**2 for p in network.parameters() if p.grad is not None).sqrt()\n            #total_grad_norm += grad_norm.item()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            batch_count += 1\n        \n        avg_loss = running_loss / batch_count\n        loss_history.append(avg_loss)\n        \n        avg_grad_norm = total_grad_norm / batch_count\n        grad_norm_history.append(avg_grad_norm)\n        \n        # Evaluate test accuracy if test data is provided\n        if test_data:\n            acc = evaluate(network, test_data)\n            accuracy_history.append(acc)\n            print(f\"Epoch {epoch+1}: Avg loss: {avg_loss:.4f} | Test Accuracy: {acc:.2f}% | Grad Norm: {avg_grad_norm:.4f}\")\n        else:\n            print(f\"Epoch {epoch+1}: Avg loss: {avg_loss:.4f} | Grad Norm: {avg_grad_norm:.4f}\")\n\n    return loss_history, accuracy_history, grad_norm_history\n\n\n\ndef evaluate(network, test_data):\n    network.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_data:\n            output = network(data)\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            total += data.size(0)\n    acc = 100. * correct / total\n    print(f\"Test Accuracy: {acc:.2f}%\")\n    return acc\n\n\ndef compute_layer_gradient_norms(model):\n    \"\"\"\n    \n    \"\"\"\n    layer_grad_norms = []\n\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.data.norm(2).item()\n            layer_grad_norms.append((name, grad_norm))\n        else:\n            layer_grad_norms.append((name, 0.0))  # No gradient yet\n\n    return layer_grad_norms\n\n\ndef get_gradients_for_batch(model, data_batch, loss_fn, device=None):\n    \"\"\"\n    Computes and returns gradients of the model parameters for a given batch.\n    \n    Parameters:\n        model (nn.Module): Trained model.\n        data_batch (tuple): A (inputs, targets) tuple from a DataLoader.\n        loss_fn: Loss function (e.g., nn.CrossEntropyLoss()).\n        device: torch.device to use. If None, infer from model.\n\n    Returns:\n        dict: {parameter_name: gradient_norm}\n    \"\"\"\n    model.eval()  # Ensure deterministic behavior (no dropout, etc.)\n\n    # Infer device from model if not given\n    if device is None:\n        device = next(model.parameters()).device\n\n    x, y = data_batch\n    x, y = x.to(device), y.to(device)\n\n    # Clear existing gradients\n    model.zero_grad()\n\n    # Forward pass\n    output = model(x)\n    loss = loss_fn(output, y)\n\n    # Backward pass\n    loss.backward()\n\n    # Collect gradient norms\n    grad_norms = {}\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norms[name] = param.grad.norm(2).item()\n        else:\n            grad_norms[name] = 0.0  # No gradient (e.g., not involved in computation)\n\n    return grad_norms\n\n\nfrom torchvision import datasets, transforms\n\n\n\n# Load MNIST\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, download =True,transform=transform)\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n\n# Initialize network\nnet = Network([784, 30, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=100, eta=0.002, test_data=test_loader)\n\nTest Accuracy: 93.51%\nEpoch 1: Avg loss: 1.6057 | Test Accuracy: 93.51% | Grad Norm: 0.1792\nTest Accuracy: 94.11%\nEpoch 2: Avg loss: 1.5316 | Test Accuracy: 94.11% | Grad Norm: 0.1474\nTest Accuracy: 94.74%\nEpoch 3: Avg loss: 1.5196 | Test Accuracy: 94.74% | Grad Norm: 0.1333\nTest Accuracy: 94.83%\nEpoch 4: Avg loss: 1.5124 | Test Accuracy: 94.83% | Grad Norm: 0.1205\nTest Accuracy: 95.29%\nEpoch 5: Avg loss: 1.5080 | Test Accuracy: 95.29% | Grad Norm: 0.1134\nTest Accuracy: 95.40%\nEpoch 6: Avg loss: 1.5040 | Test Accuracy: 95.40% | Grad Norm: 0.1063\nTest Accuracy: 95.40%\nEpoch 7: Avg loss: 1.5010 | Test Accuracy: 95.40% | Grad Norm: 0.1013\nTest Accuracy: 95.89%\nEpoch 8: Avg loss: 1.4983 | Test Accuracy: 95.89% | Grad Norm: 0.0941\nTest Accuracy: 95.63%\nEpoch 9: Avg loss: 1.4966 | Test Accuracy: 95.63% | Grad Norm: 0.0912\nTest Accuracy: 95.89%\nEpoch 10: Avg loss: 1.4944 | Test Accuracy: 95.89% | Grad Norm: 0.0878\nTest Accuracy: 95.63%\nEpoch 11: Avg loss: 1.4929 | Test Accuracy: 95.63% | Grad Norm: 0.0871\nTest Accuracy: 95.91%\nEpoch 12: Avg loss: 1.4916 | Test Accuracy: 95.91% | Grad Norm: 0.0809\nTest Accuracy: 95.80%\nEpoch 13: Avg loss: 1.4905 | Test Accuracy: 95.80% | Grad Norm: 0.0820\nTest Accuracy: 95.76%\nEpoch 14: Avg loss: 1.4895 | Test Accuracy: 95.76% | Grad Norm: 0.0770\nTest Accuracy: 95.91%\nEpoch 15: Avg loss: 1.4883 | Test Accuracy: 95.91% | Grad Norm: 0.0745\nTest Accuracy: 95.86%\nEpoch 16: Avg loss: 1.4874 | Test Accuracy: 95.86% | Grad Norm: 0.0732\nTest Accuracy: 95.79%\nEpoch 17: Avg loss: 1.4866 | Test Accuracy: 95.79% | Grad Norm: 0.0733\nTest Accuracy: 95.90%\nEpoch 18: Avg loss: 1.4856 | Test Accuracy: 95.90% | Grad Norm: 0.0700\nTest Accuracy: 95.69%\nEpoch 19: Avg loss: 1.4851 | Test Accuracy: 95.69% | Grad Norm: 0.0699\nTest Accuracy: 96.00%\nEpoch 20: Avg loss: 1.4841 | Test Accuracy: 96.00% | Grad Norm: 0.0639\nTest Accuracy: 95.85%\nEpoch 21: Avg loss: 1.4834 | Test Accuracy: 95.85% | Grad Norm: 0.0620\nTest Accuracy: 95.88%\nEpoch 22: Avg loss: 1.4835 | Test Accuracy: 95.88% | Grad Norm: 0.0638\nTest Accuracy: 95.94%\nEpoch 23: Avg loss: 1.4824 | Test Accuracy: 95.94% | Grad Norm: 0.0586\nTest Accuracy: 96.00%\nEpoch 24: Avg loss: 1.4817 | Test Accuracy: 96.00% | Grad Norm: 0.0572\nTest Accuracy: 95.99%\nEpoch 25: Avg loss: 1.4816 | Test Accuracy: 95.99% | Grad Norm: 0.0578\nTest Accuracy: 95.95%\nEpoch 26: Avg loss: 1.4813 | Test Accuracy: 95.95% | Grad Norm: 0.0566\nTest Accuracy: 96.00%\nEpoch 27: Avg loss: 1.4803 | Test Accuracy: 96.00% | Grad Norm: 0.0540\nTest Accuracy: 95.87%\nEpoch 28: Avg loss: 1.4804 | Test Accuracy: 95.87% | Grad Norm: 0.0526\nTest Accuracy: 96.17%\nEpoch 29: Avg loss: 1.4804 | Test Accuracy: 96.17% | Grad Norm: 0.0547\nTest Accuracy: 95.93%\nEpoch 30: Avg loss: 1.4798 | Test Accuracy: 95.93% | Grad Norm: 0.0501\n\n\n([1.6057081339557966,\n  1.5315928565661112,\n  1.5196338770985602,\n  1.5124150550365447,\n  1.5079818081061045,\n  1.5040260973374049,\n  1.5010399503707885,\n  1.4983220419486363,\n  1.4966399672230084,\n  1.4944003034035365,\n  1.4929161854783695,\n  1.4916214998165767,\n  1.49053826914231,\n  1.489475950797399,\n  1.4883430277903875,\n  1.4873599539399147,\n  1.4866095430453619,\n  1.4856439775427182,\n  1.4851038131515184,\n  1.4840867054462432,\n  1.4833633563717206,\n  1.4835044819315275,\n  1.4823911802371343,\n  1.4817010447978973,\n  1.481590430120627,\n  1.4812918134133022,\n  1.480328282435735,\n  1.4804061188499134,\n  1.4804016767144204,\n  1.4797668040593466],\n [93.51,\n  94.11,\n  94.74,\n  94.83,\n  95.29,\n  95.4,\n  95.4,\n  95.89,\n  95.63,\n  95.89,\n  95.63,\n  95.91,\n  95.8,\n  95.76,\n  95.91,\n  95.86,\n  95.79,\n  95.9,\n  95.69,\n  96.0,\n  95.85,\n  95.88,\n  95.94,\n  96.0,\n  95.99,\n  95.95,\n  96.0,\n  95.87,\n  96.17,\n  95.93],\n [0.17917525013443084,\n  0.1473925621828627,\n  0.13327104301230913,\n  0.12051591246481379,\n  0.11341907756493311,\n  0.10629571524920176,\n  0.10130780162154951,\n  0.09407913160028238,\n  0.09116776634996525,\n  0.08778071492000102,\n  0.08712783897689223,\n  0.08093162566800492,\n  0.08200921437090779,\n  0.07703609316606165,\n  0.07451635456864866,\n  0.0732404430225203,\n  0.07334993403181325,\n  0.06996818388912016,\n  0.06994367368905512,\n  0.06386078603882756,\n  0.06201146851345269,\n  0.06381645275375292,\n  0.05856926358767453,\n  0.05717879024385698,\n  0.057792931182410974,\n  0.05664145387527281,\n  0.05402444312549633,\n  0.052626746541882284,\n  0.05466655417354091,\n  0.05011175901814942])\n\n\n\n# Deep with no scheduler\n\nnet = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\nsol = train(net, train_loader, epochs=30, mini_batch_size=1000, eta=.008, test_data=test_loader)\n\nTest Accuracy: 94.67%\nEpoch 1: Avg loss: 0.3531 | Test Accuracy: 94.67% | Grad Norm: 4.3995\nTest Accuracy: 96.84%\nEpoch 2: Avg loss: 0.1269 | Test Accuracy: 96.84% | Grad Norm: 4.4527\nTest Accuracy: 96.56%\nEpoch 3: Avg loss: 0.0883 | Test Accuracy: 96.56% | Grad Norm: 4.2291\nTest Accuracy: 96.99%\nEpoch 4: Avg loss: 0.0691 | Test Accuracy: 96.99% | Grad Norm: 4.0776\nTest Accuracy: 97.74%\nEpoch 5: Avg loss: 0.0560 | Test Accuracy: 97.74% | Grad Norm: 4.0665\nTest Accuracy: 97.85%\nEpoch 6: Avg loss: 0.0468 | Test Accuracy: 97.85% | Grad Norm: 3.9744\nTest Accuracy: 97.00%\nEpoch 7: Avg loss: 0.0388 | Test Accuracy: 97.00% | Grad Norm: 3.9607\nTest Accuracy: 97.43%\nEpoch 8: Avg loss: 0.0356 | Test Accuracy: 97.43% | Grad Norm: 3.9747\nTest Accuracy: 97.96%\nEpoch 9: Avg loss: 0.0311 | Test Accuracy: 97.96% | Grad Norm: 3.8163\nTest Accuracy: 97.84%\nEpoch 10: Avg loss: 0.0265 | Test Accuracy: 97.84% | Grad Norm: 3.7710\nTest Accuracy: 97.69%\nEpoch 11: Avg loss: 0.0248 | Test Accuracy: 97.69% | Grad Norm: 3.7800\nTest Accuracy: 98.24%\nEpoch 12: Avg loss: 0.0216 | Test Accuracy: 98.24% | Grad Norm: 3.5618\nTest Accuracy: 97.94%\nEpoch 13: Avg loss: 0.0156 | Test Accuracy: 97.94% | Grad Norm: 3.5916\nTest Accuracy: 97.85%\nEpoch 14: Avg loss: 0.0225 | Test Accuracy: 97.85% | Grad Norm: 3.6446\nTest Accuracy: 97.92%\nEpoch 15: Avg loss: 0.0207 | Test Accuracy: 97.92% | Grad Norm: 3.6226\nTest Accuracy: 98.02%\nEpoch 16: Avg loss: 0.0178 | Test Accuracy: 98.02% | Grad Norm: 3.5322\nTest Accuracy: 97.99%\nEpoch 17: Avg loss: 0.0172 | Test Accuracy: 97.99% | Grad Norm: 3.5446\nTest Accuracy: 97.47%\nEpoch 18: Avg loss: 0.0146 | Test Accuracy: 97.47% | Grad Norm: 3.4665\nTest Accuracy: 98.07%\nEpoch 19: Avg loss: 0.0177 | Test Accuracy: 98.07% | Grad Norm: 3.5616\nTest Accuracy: 98.19%\nEpoch 20: Avg loss: 0.0106 | Test Accuracy: 98.19% | Grad Norm: 3.3028\nTest Accuracy: 97.89%\nEpoch 21: Avg loss: 0.0125 | Test Accuracy: 97.89% | Grad Norm: 3.3507\nTest Accuracy: 98.34%\nEpoch 22: Avg loss: 0.0141 | Test Accuracy: 98.34% | Grad Norm: 3.3476\nTest Accuracy: 98.27%\nEpoch 23: Avg loss: 0.0114 | Test Accuracy: 98.27% | Grad Norm: 3.2853\nTest Accuracy: 98.46%\nEpoch 24: Avg loss: 0.0133 | Test Accuracy: 98.46% | Grad Norm: 3.3265\nTest Accuracy: 98.29%\nEpoch 25: Avg loss: 0.0099 | Test Accuracy: 98.29% | Grad Norm: 3.1394\nTest Accuracy: 98.05%\nEpoch 26: Avg loss: 0.0142 | Test Accuracy: 98.05% | Grad Norm: 3.2508\nTest Accuracy: 98.22%\nEpoch 27: Avg loss: 0.0100 | Test Accuracy: 98.22% | Grad Norm: 3.2143\nTest Accuracy: 98.38%\nEpoch 28: Avg loss: 0.0121 | Test Accuracy: 98.38% | Grad Norm: 3.4105\nTest Accuracy: 98.38%\nEpoch 29: Avg loss: 0.0142 | Test Accuracy: 98.38% | Grad Norm: 3.4176\nTest Accuracy: 97.34%\nEpoch 30: Avg loss: 0.0106 | Test Accuracy: 97.34% | Grad Norm: 3.2589\n\n\n\n# Deep with no scheduler\n\nnet = Network([784, 30,30 , 30, 30,30,30,30,30,30, 10])\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=.003, test_data=test_loader)\n\nTest Accuracy: 91.04%\nEpoch 1: Avg loss: 0.7249 | Test Accuracy: 91.04% | Grad Norm: 2.9745\nTest Accuracy: 92.85%\nEpoch 2: Avg loss: 0.2586 | Test Accuracy: 92.85% | Grad Norm: 3.6478\nTest Accuracy: 94.38%\nEpoch 3: Avg loss: 0.1985 | Test Accuracy: 94.38% | Grad Norm: 3.7299\nTest Accuracy: 95.48%\nEpoch 4: Avg loss: 0.1675 | Test Accuracy: 95.48% | Grad Norm: 3.8025\nTest Accuracy: 95.81%\nEpoch 5: Avg loss: 0.1475 | Test Accuracy: 95.81% | Grad Norm: 3.9114\nTest Accuracy: 96.00%\nEpoch 6: Avg loss: 0.1331 | Test Accuracy: 96.00% | Grad Norm: 3.9403\nTest Accuracy: 95.69%\nEpoch 7: Avg loss: 0.1210 | Test Accuracy: 95.69% | Grad Norm: 3.8848\nTest Accuracy: 96.26%\nEpoch 8: Avg loss: 0.1115 | Test Accuracy: 96.26% | Grad Norm: 3.9401\nTest Accuracy: 96.53%\nEpoch 9: Avg loss: 0.1028 | Test Accuracy: 96.53% | Grad Norm: 3.8600\nTest Accuracy: 96.36%\nEpoch 10: Avg loss: 0.0960 | Test Accuracy: 96.36% | Grad Norm: 4.0626\nTest Accuracy: 96.41%\nEpoch 11: Avg loss: 0.0927 | Test Accuracy: 96.41% | Grad Norm: 4.0147\nTest Accuracy: 96.24%\nEpoch 12: Avg loss: 0.0878 | Test Accuracy: 96.24% | Grad Norm: 3.9885\nTest Accuracy: 96.63%\nEpoch 13: Avg loss: 0.0824 | Test Accuracy: 96.63% | Grad Norm: 3.9443\nTest Accuracy: 96.40%\nEpoch 14: Avg loss: 0.0781 | Test Accuracy: 96.40% | Grad Norm: 3.9439\nTest Accuracy: 96.56%\nEpoch 15: Avg loss: 0.0753 | Test Accuracy: 96.56% | Grad Norm: 3.9172\nTest Accuracy: 96.56%\nEpoch 16: Avg loss: 0.0738 | Test Accuracy: 96.56% | Grad Norm: 3.9409\nTest Accuracy: 96.63%\nEpoch 17: Avg loss: 0.0690 | Test Accuracy: 96.63% | Grad Norm: 3.8667\nTest Accuracy: 96.75%\nEpoch 18: Avg loss: 0.0656 | Test Accuracy: 96.75% | Grad Norm: 3.8483\nTest Accuracy: 96.87%\nEpoch 19: Avg loss: 0.0625 | Test Accuracy: 96.87% | Grad Norm: 3.9605\nTest Accuracy: 96.68%\nEpoch 20: Avg loss: 0.0633 | Test Accuracy: 96.68% | Grad Norm: 3.9385\nTest Accuracy: 96.88%\nEpoch 21: Avg loss: 0.0600 | Test Accuracy: 96.88% | Grad Norm: 3.9684\nTest Accuracy: 96.88%\nEpoch 22: Avg loss: 0.0597 | Test Accuracy: 96.88% | Grad Norm: 3.9766\n\n\n\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=1e-5, test_data=test_loader)\n\nTest Accuracy: 98.21%\nEpoch 1: Avg loss: 1.4661 | Test Accuracy: 98.21% | Grad Norm: 0.0436\nTest Accuracy: 98.22%\nEpoch 2: Avg loss: 1.4661 | Test Accuracy: 98.22% | Grad Norm: 0.0414\nTest Accuracy: 98.23%\nEpoch 3: Avg loss: 1.4661 | Test Accuracy: 98.23% | Grad Norm: 0.0394\nTest Accuracy: 98.22%\nEpoch 4: Avg loss: 1.4661 | Test Accuracy: 98.22% | Grad Norm: 0.0383\nTest Accuracy: 98.23%\nEpoch 5: Avg loss: 1.4660 | Test Accuracy: 98.23% | Grad Norm: 0.0363\nTest Accuracy: 98.24%\nEpoch 6: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0350\nTest Accuracy: 98.24%\nEpoch 7: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0336\nTest Accuracy: 98.23%\nEpoch 8: Avg loss: 1.4660 | Test Accuracy: 98.23% | Grad Norm: 0.0323\nTest Accuracy: 98.24%\nEpoch 9: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0315\nTest Accuracy: 98.25%\nEpoch 10: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0305\nTest Accuracy: 98.24%\nEpoch 11: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0294\nTest Accuracy: 98.25%\nEpoch 12: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0287\nTest Accuracy: 98.25%\nEpoch 13: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0283\nTest Accuracy: 98.25%\nEpoch 14: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0277\nTest Accuracy: 98.24%\nEpoch 15: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0272\nTest Accuracy: 98.24%\nEpoch 16: Avg loss: 1.4660 | Test Accuracy: 98.24% | Grad Norm: 0.0269\nTest Accuracy: 98.25%\nEpoch 17: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0266\nTest Accuracy: 98.25%\nEpoch 18: Avg loss: 1.4660 | Test Accuracy: 98.25% | Grad Norm: 0.0264\nTest Accuracy: 98.24%\nEpoch 19: Avg loss: 1.4659 | Test Accuracy: 98.24% | Grad Norm: 0.0261\nTest Accuracy: 98.25%\nEpoch 20: Avg loss: 1.4659 | Test Accuracy: 98.25% | Grad Norm: 0.0258\nTest Accuracy: 98.26%\nEpoch 21: Avg loss: 1.4659 | Test Accuracy: 98.26% | Grad Norm: 0.0256\nTest Accuracy: 98.26%\nEpoch 22: Avg loss: 1.4659 | Test Accuracy: 98.26% | Grad Norm: 0.0254\nTest Accuracy: 98.27%\nEpoch 23: Avg loss: 1.4659 | Test Accuracy: 98.27% | Grad Norm: 0.0254\nTest Accuracy: 98.28%\nEpoch 24: Avg loss: 1.4659 | Test Accuracy: 98.28% | Grad Norm: 0.0253\nTest Accuracy: 98.28%\nEpoch 25: Avg loss: 1.4659 | Test Accuracy: 98.28% | Grad Norm: 0.0250\nTest Accuracy: 98.28%\nEpoch 26: Avg loss: 1.4659 | Test Accuracy: 98.28% | Grad Norm: 0.0251\nTest Accuracy: 98.29%\nEpoch 27: Avg loss: 1.4659 | Test Accuracy: 98.29% | Grad Norm: 0.0250\nTest Accuracy: 98.29%\nEpoch 28: Avg loss: 1.4659 | Test Accuracy: 98.29% | Grad Norm: 0.0247\nTest Accuracy: 98.30%\nEpoch 29: Avg loss: 1.4659 | Test Accuracy: 98.30% | Grad Norm: 0.0247\nTest Accuracy: 98.30%\nEpoch 30: Avg loss: 1.4659 | Test Accuracy: 98.30% | Grad Norm: 0.0244\n\n\n\nnet1 = net\n\n\nsol = train(net, train_loader, epochs=1, mini_batch_size=100, eta=.003, test_data=test_loader)\n\nTest Accuracy: 20.76%\nEpoch 1: Avg loss: 2.2209 | Test Accuracy: 20.76% | Grad Norm: 0.2816\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[159], line 1\n----&gt; 1 sol = train(net, train_loader, epochs=1, mini_batch_size=100, eta=.003, test_data=test_loader)\n\nCell In[154], line 81, in train(network, train_data, epochs, mini_batch_size, eta, test_data)\n     78     else:\n     79         print(f\"Epoch {epoch+1}: Avg loss: {avg_loss:.4f} | Grad Norm: {avg_grad_norm:.4f}\")\n---&gt; 81 return loss_history, accuracy_history, grad_norm_history, layer_norms\n\nNameError: name 'layer_norms' is not defined\n\n\n\n\n0.003\n\n\nloss_fn = nn.CrossEntropyLoss()\ndata_batch = next(iter(train_loader))\n\ndef get_full_dataset_gradients(model, dataloader, loss_fn, device=None, average=True):\n    \"\"\"\n    Computes gradients for the entire dataset by iterating over the dataloader.\n\n    Parameters:\n        model (nn.Module): Trained model.\n        dataloader (DataLoader): Full training or evaluation data loader.\n        loss_fn: Loss function, e.g., nn.CrossEntropyLoss().\n        device: torch.device (optional). If None, inferred from model.\n        average (bool): Whether to return average or total gradient norms.\n\n    Returns:\n        dict: {parameter_name: gradient_norm}\n    \"\"\"\n    model.eval()\n\n    if device is None:\n        device = next(model.parameters()).device\n\n    model.zero_grad()\n    total_samples = 0\n\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        output = model(x)\n        loss = loss_fn(output, y)\n        loss.backward()\n\n        total_samples += x.size(0)\n\n    grad_norms = {}\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            norm = param.grad.norm(2).item()\n            if average:\n                norm = norm / total_samples\n            grad_norms[name] = norm\n        else:\n            grad_norms[name] = 0.0\n\n    return grad_norms\n\ngradients = get_full_dataset_gradients(net, train_loader, loss_fn)\n\n\n\nfor name, norm in gradients.items():\n    print(f\"{name}: grad norm = {norm}\")\n\nlayers.0.weight: grad norm = 0.0074\nlayers.0.bias: grad norm = 0.0009\nlayers.1.weight: grad norm = 0.0067\nlayers.1.bias: grad norm = 0.0003\nlayers.2.weight: grad norm = 0.0043\nlayers.2.bias: grad norm = 0.0001\nlayers.3.weight: grad norm = 0.0022\nlayers.3.bias: grad norm = 0.0000\nlayers.4.weight: grad norm = 0.0015\nlayers.4.bias: grad norm = 0.0000\nlayers.5.weight: grad norm = 0.0011\nlayers.5.bias: grad norm = 0.0000\nlayers.6.weight: grad norm = 0.0008\nlayers.6.bias: grad norm = 0.0000\nlayers.7.weight: grad norm = 0.0007\nlayers.7.bias: grad norm = 0.0000\nlayers.8.weight: grad norm = 0.0005\nlayers.8.bias: grad norm = 0.0000\nlayers.9.weight: grad norm = 0.0005\nlayers.9.bias: grad norm = 0.0000\n\n\n\ngradients\n\n{'layers.0.weight': 0.007423374430338542,\n 'layers.0.bias': 0.0009399686177571614,\n 'layers.1.weight': 0.006660091654459636,\n 'layers.1.bias': 0.0002851175626118978,\n 'layers.2.weight': 0.00425323715209961,\n 'layers.2.bias': 8.349174658457439e-05,\n 'layers.3.weight': 0.002219520314534505,\n 'layers.3.bias': 2.8872899214426675e-05,\n 'layers.4.weight': 0.0014801395416259766,\n 'layers.4.bias': 1.2344685196876526e-05,\n 'layers.5.weight': 0.0011400601704915365,\n 'layers.5.bias': 6.288351118564606e-06,\n 'layers.6.weight': 0.0008085358301798503,\n 'layers.6.bias': 3.2734158138434093e-06,\n 'layers.7.weight': 0.0006653710683186849,\n 'layers.7.bias': 2.397415538628896e-06,\n 'layers.8.weight': 0.0005273423830668132,\n 'layers.8.bias': 1.6459631423155466e-06,\n 'layers.9.weight': 0.0004727630615234375,\n 'layers.9.bias': 7.188314571976661e-07}\n\n\n\n# Deep with scheduler\n\nnet = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=.003, test_data=test_loader)\n\nTest Accuracy: 94.16%\nEpoch 1: Avg loss: 0.4080 | Test Accuracy: 94.16% | Grad Norm: 3.5094\nTest Accuracy: 97.29%\nEpoch 2: Avg loss: 0.1194 | Test Accuracy: 97.29% | Grad Norm: 4.3444\nTest Accuracy: 97.28%\nEpoch 3: Avg loss: 0.0778 | Test Accuracy: 97.28% | Grad Norm: 4.5199\nTest Accuracy: 97.95%\nEpoch 4: Avg loss: 0.0573 | Test Accuracy: 97.95% | Grad Norm: 4.7296\nTest Accuracy: 97.55%\nEpoch 5: Avg loss: 0.0439 | Test Accuracy: 97.55% | Grad Norm: 4.6359\nTest Accuracy: 98.24%\nEpoch 6: Avg loss: 0.0357 | Test Accuracy: 98.24% | Grad Norm: 4.7456\nTest Accuracy: 97.87%\nEpoch 7: Avg loss: 0.0258 | Test Accuracy: 97.87% | Grad Norm: 4.5653\nTest Accuracy: 97.69%\nEpoch 8: Avg loss: 0.0223 | Test Accuracy: 97.69% | Grad Norm: 4.6940\nTest Accuracy: 97.74%\nEpoch 9: Avg loss: 0.0189 | Test Accuracy: 97.74% | Grad Norm: 4.7128\nTest Accuracy: 98.21%\nEpoch 10: Avg loss: 0.0173 | Test Accuracy: 98.21% | Grad Norm: 4.7422\nTest Accuracy: 98.34%\nEpoch 11: Avg loss: 0.0170 | Test Accuracy: 98.34% | Grad Norm: 4.6506\nTest Accuracy: 98.41%\nEpoch 12: Avg loss: 0.0122 | Test Accuracy: 98.41% | Grad Norm: 4.4010\nTest Accuracy: 98.16%\nEpoch 13: Avg loss: 0.0084 | Test Accuracy: 98.16% | Grad Norm: 4.3918\nTest Accuracy: 97.91%\nEpoch 14: Avg loss: 0.0101 | Test Accuracy: 97.91% | Grad Norm: 4.5528\nTest Accuracy: 98.25%\nEpoch 15: Avg loss: 0.0087 | Test Accuracy: 98.25% | Grad Norm: 4.4357\nTest Accuracy: 98.26%\nEpoch 16: Avg loss: 0.0104 | Test Accuracy: 98.26% | Grad Norm: 4.4736\nTest Accuracy: 98.18%\nEpoch 17: Avg loss: 0.0085 | Test Accuracy: 98.18% | Grad Norm: 4.3984\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[183], line 4\n      1 # Deep with scheduler\n      3 net = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\n----&gt; 4 sol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=.003, test_data=test_loader)\n\nCell In[182], line 58, in train(network, train_data, epochs, mini_batch_size, eta, test_data)\n     56 output = network(data)\n     57 loss = loss_fn(output, target)\n---&gt; 58 loss.backward()\n     60 # Compute gradient norm (L2 norm)\n     61 \n     62 # Get gradients for first and last layer weights\n     63 first_weight_grad = network.layers[0].weight.grad\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/_tensor.py:581, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n    571 if has_torch_function_unary(self):\n    572     return handle_torch_function(\n    573         Tensor.backward,\n    574         (self,),\n   (...)\n    579         inputs=inputs,\n    580     )\n--&gt; 581 torch.autograd.backward(\n    582     self, gradient, retain_graph, create_graph, inputs=inputs\n    583 )\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/autograd/__init__.py:340, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    331 inputs = (\n    332     (inputs,)\n    333     if isinstance(inputs, (torch.Tensor, graph.GradientEdge))\n   (...)\n    336     else ()\n    337 )\n    339 grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, len(tensors))\n--&gt; 340 grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)\n    341 if retain_graph is None:\n    342     retain_graph = create_graph\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/autograd/__init__.py:220, in _make_grads(outputs, grads, is_grads_batched)\n    217     else:\n    218         assert isinstance(out, torch.Tensor)\n    219         new_grads.append(\n--&gt; 220             torch.ones_like(out, memory_format=torch.preserve_format)\n    221         )\n    222 else:\n    223     new_grads.append(None)\n\nKeyboardInterrupt: \n\n\n\n\nnet = Network([784, 30, 10])\n# This was with standard SGD\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.01, test_data=test_loader)\n\nTest Accuracy: 29.91%\nEpoch 1: Avg loss: 2.2479 | Test Accuracy: 29.91% | Grad Norm: 0.1611\nTest Accuracy: 48.47%\nEpoch 2: Avg loss: 2.0879 | Test Accuracy: 48.47% | Grad Norm: 0.2028\nTest Accuracy: 65.52%\nEpoch 3: Avg loss: 1.9488 | Test Accuracy: 65.52% | Grad Norm: 0.2276\nTest Accuracy: 66.79%\nEpoch 4: Avg loss: 1.8553 | Test Accuracy: 66.79% | Grad Norm: 0.2130\nTest Accuracy: 67.25%\nEpoch 5: Avg loss: 1.8236 | Test Accuracy: 67.25% | Grad Norm: 0.1918\nTest Accuracy: 74.21%\nEpoch 6: Avg loss: 1.7944 | Test Accuracy: 74.21% | Grad Norm: 0.2123\nTest Accuracy: 74.91%\nEpoch 7: Avg loss: 1.7602 | Test Accuracy: 74.91% | Grad Norm: 0.2248\nTest Accuracy: 75.40%\nEpoch 8: Avg loss: 1.7414 | Test Accuracy: 75.40% | Grad Norm: 0.2174\nTest Accuracy: 86.94%\nEpoch 9: Avg loss: 1.7121 | Test Accuracy: 86.94% | Grad Norm: 0.2557\nTest Accuracy: 88.78%\nEpoch 10: Avg loss: 1.6699 | Test Accuracy: 88.78% | Grad Norm: 0.2846\nTest Accuracy: 89.41%\nEpoch 11: Avg loss: 1.6421 | Test Accuracy: 89.41% | Grad Norm: 0.2823\nTest Accuracy: 89.89%\nEpoch 12: Avg loss: 1.6242 | Test Accuracy: 89.89% | Grad Norm: 0.2757\nTest Accuracy: 90.06%\nEpoch 13: Avg loss: 1.6119 | Test Accuracy: 90.06% | Grad Norm: 0.2709\nTest Accuracy: 90.30%\nEpoch 14: Avg loss: 1.6029 | Test Accuracy: 90.30% | Grad Norm: 0.2668\nTest Accuracy: 90.57%\nEpoch 15: Avg loss: 1.5959 | Test Accuracy: 90.57% | Grad Norm: 0.2641\nTest Accuracy: 90.92%\nEpoch 16: Avg loss: 1.5902 | Test Accuracy: 90.92% | Grad Norm: 0.2599\nTest Accuracy: 90.97%\nEpoch 17: Avg loss: 1.5855 | Test Accuracy: 90.97% | Grad Norm: 0.2580\nTest Accuracy: 91.18%\nEpoch 18: Avg loss: 1.5815 | Test Accuracy: 91.18% | Grad Norm: 0.2552\nTest Accuracy: 91.39%\nEpoch 19: Avg loss: 1.5780 | Test Accuracy: 91.39% | Grad Norm: 0.2533\nTest Accuracy: 91.53%\nEpoch 20: Avg loss: 1.5750 | Test Accuracy: 91.53% | Grad Norm: 0.2524\nTest Accuracy: 91.67%\nEpoch 21: Avg loss: 1.5723 | Test Accuracy: 91.67% | Grad Norm: 0.2501\nTest Accuracy: 91.70%\nEpoch 22: Avg loss: 1.5698 | Test Accuracy: 91.70% | Grad Norm: 0.2482\nTest Accuracy: 91.78%\nEpoch 23: Avg loss: 1.5676 | Test Accuracy: 91.78% | Grad Norm: 0.2459\nTest Accuracy: 91.89%\nEpoch 24: Avg loss: 1.5656 | Test Accuracy: 91.89% | Grad Norm: 0.2456\nTest Accuracy: 91.95%\nEpoch 25: Avg loss: 1.5637 | Test Accuracy: 91.95% | Grad Norm: 0.2442\nTest Accuracy: 92.06%\nEpoch 26: Avg loss: 1.5620 | Test Accuracy: 92.06% | Grad Norm: 0.2422\nTest Accuracy: 92.22%\nEpoch 27: Avg loss: 1.5604 | Test Accuracy: 92.22% | Grad Norm: 0.2408\nTest Accuracy: 92.25%\nEpoch 28: Avg loss: 1.5589 | Test Accuracy: 92.25% | Grad Norm: 0.2407\nTest Accuracy: 92.30%\nEpoch 29: Avg loss: 1.5575 | Test Accuracy: 92.30% | Grad Norm: 0.2388\nTest Accuracy: 92.36%\nEpoch 30: Avg loss: 1.5563 | Test Accuracy: 92.36% | Grad Norm: 0.2367\n\n\n\nnet = Network([784, 30, 10])\n# This was with SGD with momentum and Nesterov acceleration\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.01, test_data=test_loader)\n\nTest Accuracy: 57.63%\nEpoch 1: Avg loss: 2.0109 | Test Accuracy: 57.63% | Grad Norm: 0.1930\nTest Accuracy: 73.91%\nEpoch 2: Avg loss: 1.8296 | Test Accuracy: 73.91% | Grad Norm: 0.1953\nTest Accuracy: 82.53%\nEpoch 3: Avg loss: 1.7292 | Test Accuracy: 82.53% | Grad Norm: 0.2221\nTest Accuracy: 84.42%\nEpoch 4: Avg loss: 1.6547 | Test Accuracy: 84.42% | Grad Norm: 0.2291\nTest Accuracy: 90.59%\nEpoch 5: Avg loss: 1.6267 | Test Accuracy: 90.59% | Grad Norm: 0.2318\nTest Accuracy: 91.58%\nEpoch 6: Avg loss: 1.5813 | Test Accuracy: 91.58% | Grad Norm: 0.2520\nTest Accuracy: 92.12%\nEpoch 7: Avg loss: 1.5659 | Test Accuracy: 92.12% | Grad Norm: 0.2413\nTest Accuracy: 92.41%\nEpoch 8: Avg loss: 1.5574 | Test Accuracy: 92.41% | Grad Norm: 0.2343\nTest Accuracy: 92.76%\nEpoch 9: Avg loss: 1.5513 | Test Accuracy: 92.76% | Grad Norm: 0.2271\nTest Accuracy: 93.06%\nEpoch 10: Avg loss: 1.5467 | Test Accuracy: 93.06% | Grad Norm: 0.2221\nTest Accuracy: 93.33%\nEpoch 11: Avg loss: 1.5428 | Test Accuracy: 93.33% | Grad Norm: 0.2187\nTest Accuracy: 93.51%\nEpoch 12: Avg loss: 1.5395 | Test Accuracy: 93.51% | Grad Norm: 0.2157\nTest Accuracy: 93.68%\nEpoch 13: Avg loss: 1.5366 | Test Accuracy: 93.68% | Grad Norm: 0.2102\nTest Accuracy: 93.79%\nEpoch 14: Avg loss: 1.5340 | Test Accuracy: 93.79% | Grad Norm: 0.2066\nTest Accuracy: 93.96%\nEpoch 15: Avg loss: 1.5318 | Test Accuracy: 93.96% | Grad Norm: 0.2042\nTest Accuracy: 93.99%\nEpoch 16: Avg loss: 1.5298 | Test Accuracy: 93.99% | Grad Norm: 0.2016\nTest Accuracy: 94.16%\nEpoch 17: Avg loss: 1.5279 | Test Accuracy: 94.16% | Grad Norm: 0.1977\nTest Accuracy: 94.25%\nEpoch 18: Avg loss: 1.5261 | Test Accuracy: 94.25% | Grad Norm: 0.1961\nTest Accuracy: 94.27%\nEpoch 19: Avg loss: 1.5245 | Test Accuracy: 94.27% | Grad Norm: 0.1930\nTest Accuracy: 94.38%\nEpoch 20: Avg loss: 1.5231 | Test Accuracy: 94.38% | Grad Norm: 0.1912\nTest Accuracy: 94.39%\nEpoch 21: Avg loss: 1.5217 | Test Accuracy: 94.39% | Grad Norm: 0.1884\nTest Accuracy: 94.51%\nEpoch 22: Avg loss: 1.5204 | Test Accuracy: 94.51% | Grad Norm: 0.1875\nTest Accuracy: 94.56%\nEpoch 23: Avg loss: 1.5192 | Test Accuracy: 94.56% | Grad Norm: 0.1840\nTest Accuracy: 94.67%\nEpoch 24: Avg loss: 1.5180 | Test Accuracy: 94.67% | Grad Norm: 0.1823\nTest Accuracy: 94.77%\nEpoch 25: Avg loss: 1.5170 | Test Accuracy: 94.77% | Grad Norm: 0.1821\nTest Accuracy: 94.81%\nEpoch 26: Avg loss: 1.5160 | Test Accuracy: 94.81% | Grad Norm: 0.1794\nTest Accuracy: 94.92%\nEpoch 27: Avg loss: 1.5150 | Test Accuracy: 94.92% | Grad Norm: 0.1772\nTest Accuracy: 94.87%\nEpoch 28: Avg loss: 1.5141 | Test Accuracy: 94.87% | Grad Norm: 0.1770\nTest Accuracy: 94.89%\nEpoch 29: Avg loss: 1.5132 | Test Accuracy: 94.89% | Grad Norm: 0.1747\nTest Accuracy: 94.96%\nEpoch 30: Avg loss: 1.5123 | Test Accuracy: 94.96% | Grad Norm: 0.1732\n\n\n\nnet = Network([784, 30, 10])\n# This was with SGD with momentum and Nesterov acceleration\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.1, test_data=test_loader)\n\nTest Accuracy: 84.42%\nEpoch 1: Avg loss: 1.7048 | Test Accuracy: 84.42% | Grad Norm: 0.2265\nTest Accuracy: 93.93%\nEpoch 2: Avg loss: 1.6000 | Test Accuracy: 93.93% | Grad Norm: 0.1920\nTest Accuracy: 94.41%\nEpoch 3: Avg loss: 1.5281 | Test Accuracy: 94.41% | Grad Norm: 0.1855\nTest Accuracy: 94.88%\nEpoch 4: Avg loss: 1.5188 | Test Accuracy: 94.88% | Grad Norm: 0.1677\nTest Accuracy: 95.17%\nEpoch 5: Avg loss: 1.5133 | Test Accuracy: 95.17% | Grad Norm: 0.1591\nTest Accuracy: 95.08%\nEpoch 6: Avg loss: 1.5092 | Test Accuracy: 95.08% | Grad Norm: 0.1496\nTest Accuracy: 95.46%\nEpoch 7: Avg loss: 1.5056 | Test Accuracy: 95.46% | Grad Norm: 0.1451\nTest Accuracy: 95.43%\nEpoch 8: Avg loss: 1.5031 | Test Accuracy: 95.43% | Grad Norm: 0.1357\nTest Accuracy: 95.65%\nEpoch 9: Avg loss: 1.5011 | Test Accuracy: 95.65% | Grad Norm: 0.1371\nTest Accuracy: 95.66%\nEpoch 10: Avg loss: 1.4988 | Test Accuracy: 95.66% | Grad Norm: 0.1275\nTest Accuracy: 95.68%\nEpoch 11: Avg loss: 1.4971 | Test Accuracy: 95.68% | Grad Norm: 0.1251\nTest Accuracy: 95.65%\nEpoch 12: Avg loss: 1.4956 | Test Accuracy: 95.65% | Grad Norm: 0.1231\nTest Accuracy: 96.09%\nEpoch 13: Avg loss: 1.4941 | Test Accuracy: 96.09% | Grad Norm: 0.1160\nTest Accuracy: 95.79%\nEpoch 14: Avg loss: 1.4931 | Test Accuracy: 95.79% | Grad Norm: 0.1148\nTest Accuracy: 95.77%\nEpoch 15: Avg loss: 1.4920 | Test Accuracy: 95.77% | Grad Norm: 0.1122\nTest Accuracy: 96.13%\nEpoch 16: Avg loss: 1.4908 | Test Accuracy: 96.13% | Grad Norm: 0.1064\nTest Accuracy: 95.88%\nEpoch 17: Avg loss: 1.4898 | Test Accuracy: 95.88% | Grad Norm: 0.1026\nTest Accuracy: 96.12%\nEpoch 18: Avg loss: 1.4890 | Test Accuracy: 96.12% | Grad Norm: 0.1007\nTest Accuracy: 96.13%\nEpoch 19: Avg loss: 1.4881 | Test Accuracy: 96.13% | Grad Norm: 0.0965\nTest Accuracy: 96.08%\nEpoch 20: Avg loss: 1.4873 | Test Accuracy: 96.08% | Grad Norm: 0.0951\nTest Accuracy: 96.21%\nEpoch 21: Avg loss: 1.4865 | Test Accuracy: 96.21% | Grad Norm: 0.0921\nTest Accuracy: 96.09%\nEpoch 22: Avg loss: 1.4858 | Test Accuracy: 96.09% | Grad Norm: 0.0910\nTest Accuracy: 96.20%\nEpoch 23: Avg loss: 1.4853 | Test Accuracy: 96.20% | Grad Norm: 0.0840\nTest Accuracy: 96.04%\nEpoch 24: Avg loss: 1.4846 | Test Accuracy: 96.04% | Grad Norm: 0.0842\nTest Accuracy: 96.20%\nEpoch 25: Avg loss: 1.4843 | Test Accuracy: 96.20% | Grad Norm: 0.0862\nTest Accuracy: 96.37%\nEpoch 26: Avg loss: 1.4835 | Test Accuracy: 96.37% | Grad Norm: 0.0795\nTest Accuracy: 96.08%\nEpoch 27: Avg loss: 1.4830 | Test Accuracy: 96.08% | Grad Norm: 0.0801\nTest Accuracy: 96.37%\nEpoch 28: Avg loss: 1.4828 | Test Accuracy: 96.37% | Grad Norm: 0.0797\nTest Accuracy: 96.41%\nEpoch 29: Avg loss: 1.4821 | Test Accuracy: 96.41% | Grad Norm: 0.0718\nTest Accuracy: 96.30%\nEpoch 30: Avg loss: 1.4816 | Test Accuracy: 96.30% | Grad Norm: 0.0703\n\n\n\nnet = Network([784, 30, 10])\n# This was with SGD with no momentum and a very high learning rate\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.3, test_data=test_loader)\n\nTest Accuracy: 93.92%\nEpoch 1: Avg loss: 1.5867 | Test Accuracy: 93.92% | Grad Norm: 0.1998\nTest Accuracy: 94.81%\nEpoch 2: Avg loss: 1.5253 | Test Accuracy: 94.81% | Grad Norm: 0.1450\nTest Accuracy: 94.61%\nEpoch 3: Avg loss: 1.5164 | Test Accuracy: 94.61% | Grad Norm: 0.1280\nTest Accuracy: 94.91%\nEpoch 4: Avg loss: 1.5115 | Test Accuracy: 94.91% | Grad Norm: 0.1180\nTest Accuracy: 95.51%\nEpoch 5: Avg loss: 1.5075 | Test Accuracy: 95.51% | Grad Norm: 0.1134\nTest Accuracy: 95.67%\nEpoch 6: Avg loss: 1.5042 | Test Accuracy: 95.67% | Grad Norm: 0.1091\nTest Accuracy: 95.60%\nEpoch 7: Avg loss: 1.5018 | Test Accuracy: 95.60% | Grad Norm: 0.1012\nTest Accuracy: 95.52%\nEpoch 8: Avg loss: 1.5008 | Test Accuracy: 95.52% | Grad Norm: 0.0992\nTest Accuracy: 95.44%\nEpoch 9: Avg loss: 1.4996 | Test Accuracy: 95.44% | Grad Norm: 0.0998\nTest Accuracy: 95.48%\nEpoch 10: Avg loss: 1.4980 | Test Accuracy: 95.48% | Grad Norm: 0.0940\nTest Accuracy: 95.90%\nEpoch 11: Avg loss: 1.4969 | Test Accuracy: 95.90% | Grad Norm: 0.0912\nTest Accuracy: 95.95%\nEpoch 12: Avg loss: 1.4955 | Test Accuracy: 95.95% | Grad Norm: 0.0873\nTest Accuracy: 95.87%\nEpoch 13: Avg loss: 1.4940 | Test Accuracy: 95.87% | Grad Norm: 0.0854\nTest Accuracy: 95.89%\nEpoch 14: Avg loss: 1.4933 | Test Accuracy: 95.89% | Grad Norm: 0.0833\nTest Accuracy: 95.93%\nEpoch 15: Avg loss: 1.4931 | Test Accuracy: 95.93% | Grad Norm: 0.0826\nTest Accuracy: 95.93%\nEpoch 16: Avg loss: 1.4915 | Test Accuracy: 95.93% | Grad Norm: 0.0759\nTest Accuracy: 95.77%\nEpoch 17: Avg loss: 1.4904 | Test Accuracy: 95.77% | Grad Norm: 0.0747\nTest Accuracy: 95.77%\nEpoch 18: Avg loss: 1.4910 | Test Accuracy: 95.77% | Grad Norm: 0.0786\nTest Accuracy: 96.17%\nEpoch 19: Avg loss: 1.4895 | Test Accuracy: 96.17% | Grad Norm: 0.0693\nTest Accuracy: 95.66%\nEpoch 20: Avg loss: 1.4892 | Test Accuracy: 95.66% | Grad Norm: 0.0717\nTest Accuracy: 95.97%\nEpoch 21: Avg loss: 1.4885 | Test Accuracy: 95.97% | Grad Norm: 0.0686\nTest Accuracy: 95.83%\nEpoch 22: Avg loss: 1.4879 | Test Accuracy: 95.83% | Grad Norm: 0.0702\nTest Accuracy: 96.05%\nEpoch 23: Avg loss: 1.4881 | Test Accuracy: 96.05% | Grad Norm: 0.0711\nTest Accuracy: 96.24%\nEpoch 24: Avg loss: 1.4870 | Test Accuracy: 96.24% | Grad Norm: 0.0633\nTest Accuracy: 95.85%\nEpoch 25: Avg loss: 1.4856 | Test Accuracy: 95.85% | Grad Norm: 0.0605\nTest Accuracy: 95.87%\nEpoch 26: Avg loss: 1.4856 | Test Accuracy: 95.87% | Grad Norm: 0.0604\nTest Accuracy: 96.04%\nEpoch 27: Avg loss: 1.4851 | Test Accuracy: 96.04% | Grad Norm: 0.0584\nTest Accuracy: 96.08%\nEpoch 28: Avg loss: 1.4843 | Test Accuracy: 96.08% | Grad Norm: 0.0549\nTest Accuracy: 95.57%\nEpoch 29: Avg loss: 1.4842 | Test Accuracy: 95.57% | Grad Norm: 0.0573\nTest Accuracy: 95.93%\nEpoch 30: Avg loss: 1.4845 | Test Accuracy: 95.93% | Grad Norm: 0.0588\n\n\n\nnet = Network([784, 30, 10])\n# This was with SGD with no momentum and a very high learning rate\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=10, eta=0.3, test_data=test_loader)\n\nTest Accuracy: 93.23%\nEpoch 1: Avg loss: 1.6325 | Test Accuracy: 93.23% | Grad Norm: 0.1882\nTest Accuracy: 94.34%\nEpoch 2: Avg loss: 1.5264 | Test Accuracy: 94.34% | Grad Norm: 0.1537\nTest Accuracy: 94.52%\nEpoch 3: Avg loss: 1.5156 | Test Accuracy: 94.52% | Grad Norm: 0.1305\nTest Accuracy: 94.79%\nEpoch 4: Avg loss: 1.5104 | Test Accuracy: 94.79% | Grad Norm: 0.1228\nTest Accuracy: 95.44%\nEpoch 5: Avg loss: 1.5074 | Test Accuracy: 95.44% | Grad Norm: 0.1202\nTest Accuracy: 95.29%\nEpoch 6: Avg loss: 1.5034 | Test Accuracy: 95.29% | Grad Norm: 0.1094\nTest Accuracy: 95.41%\nEpoch 7: Avg loss: 1.5018 | Test Accuracy: 95.41% | Grad Norm: 0.1051\nTest Accuracy: 95.48%\nEpoch 8: Avg loss: 1.5001 | Test Accuracy: 95.48% | Grad Norm: 0.1034\nTest Accuracy: 95.48%\nEpoch 9: Avg loss: 1.4982 | Test Accuracy: 95.48% | Grad Norm: 0.1002\nTest Accuracy: 95.88%\nEpoch 10: Avg loss: 1.4968 | Test Accuracy: 95.88% | Grad Norm: 0.0946\nTest Accuracy: 95.64%\nEpoch 11: Avg loss: 1.4950 | Test Accuracy: 95.64% | Grad Norm: 0.0905\nTest Accuracy: 95.88%\nEpoch 12: Avg loss: 1.4941 | Test Accuracy: 95.88% | Grad Norm: 0.0868\nTest Accuracy: 95.79%\nEpoch 13: Avg loss: 1.4924 | Test Accuracy: 95.79% | Grad Norm: 0.0829\nTest Accuracy: 95.73%\nEpoch 14: Avg loss: 1.4917 | Test Accuracy: 95.73% | Grad Norm: 0.0820\nTest Accuracy: 95.82%\nEpoch 15: Avg loss: 1.4917 | Test Accuracy: 95.82% | Grad Norm: 0.0816\nTest Accuracy: 95.79%\nEpoch 16: Avg loss: 1.4907 | Test Accuracy: 95.79% | Grad Norm: 0.0796\nTest Accuracy: 95.80%\nEpoch 17: Avg loss: 1.4898 | Test Accuracy: 95.80% | Grad Norm: 0.0747\nTest Accuracy: 95.56%\nEpoch 18: Avg loss: 1.4888 | Test Accuracy: 95.56% | Grad Norm: 0.0725\nTest Accuracy: 95.93%\nEpoch 19: Avg loss: 1.4884 | Test Accuracy: 95.93% | Grad Norm: 0.0716\nTest Accuracy: 96.08%\nEpoch 20: Avg loss: 1.4879 | Test Accuracy: 96.08% | Grad Norm: 0.0700\nTest Accuracy: 95.66%\nEpoch 21: Avg loss: 1.4868 | Test Accuracy: 95.66% | Grad Norm: 0.0671\nTest Accuracy: 95.75%\nEpoch 22: Avg loss: 1.4856 | Test Accuracy: 95.75% | Grad Norm: 0.0618\nTest Accuracy: 95.96%\nEpoch 23: Avg loss: 1.4864 | Test Accuracy: 95.96% | Grad Norm: 0.0682\nTest Accuracy: 95.74%\nEpoch 24: Avg loss: 1.4854 | Test Accuracy: 95.74% | Grad Norm: 0.0624\nTest Accuracy: 95.80%\nEpoch 25: Avg loss: 1.4860 | Test Accuracy: 95.80% | Grad Norm: 0.0675\nTest Accuracy: 95.74%\nEpoch 26: Avg loss: 1.4846 | Test Accuracy: 95.74% | Grad Norm: 0.0611\nTest Accuracy: 95.94%\nEpoch 27: Avg loss: 1.4844 | Test Accuracy: 95.94% | Grad Norm: 0.0615\nTest Accuracy: 95.94%\nEpoch 28: Avg loss: 1.4838 | Test Accuracy: 95.94% | Grad Norm: 0.0593\nTest Accuracy: 96.17%\nEpoch 29: Avg loss: 1.4837 | Test Accuracy: 96.17% | Grad Norm: 0.0566\nTest Accuracy: 95.93%\nEpoch 30: Avg loss: 1.4833 | Test Accuracy: 95.93% | Grad Norm: 0.0579\n\n\n\nnet = Network([784, 30, 10])\n# This was with SGD with no momentum and a very high learning rate\n# Train\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=0.3, test_data=test_loader)\n\nTest Accuracy: 91.09%\nEpoch 1: Avg loss: 1.7524 | Test Accuracy: 91.09% | Grad Norm: 0.2078\nTest Accuracy: 93.60%\nEpoch 2: Avg loss: 1.5566 | Test Accuracy: 93.60% | Grad Norm: 0.2304\nTest Accuracy: 94.23%\nEpoch 3: Avg loss: 1.5345 | Test Accuracy: 94.23% | Grad Norm: 0.2026\nTest Accuracy: 94.96%\nEpoch 4: Avg loss: 1.5242 | Test Accuracy: 94.96% | Grad Norm: 0.1849\nTest Accuracy: 94.89%\nEpoch 5: Avg loss: 1.5179 | Test Accuracy: 94.89% | Grad Norm: 0.1732\nTest Accuracy: 95.26%\nEpoch 6: Avg loss: 1.5134 | Test Accuracy: 95.26% | Grad Norm: 0.1645\nTest Accuracy: 95.23%\nEpoch 7: Avg loss: 1.5101 | Test Accuracy: 95.23% | Grad Norm: 0.1612\nTest Accuracy: 95.26%\nEpoch 8: Avg loss: 1.5073 | Test Accuracy: 95.26% | Grad Norm: 0.1561\nTest Accuracy: 95.61%\nEpoch 9: Avg loss: 1.5049 | Test Accuracy: 95.61% | Grad Norm: 0.1504\nTest Accuracy: 95.55%\nEpoch 10: Avg loss: 1.5029 | Test Accuracy: 95.55% | Grad Norm: 0.1458\nTest Accuracy: 95.82%\nEpoch 11: Avg loss: 1.5013 | Test Accuracy: 95.82% | Grad Norm: 0.1457\nTest Accuracy: 95.76%\nEpoch 12: Avg loss: 1.4994 | Test Accuracy: 95.76% | Grad Norm: 0.1378\nTest Accuracy: 95.74%\nEpoch 13: Avg loss: 1.4980 | Test Accuracy: 95.74% | Grad Norm: 0.1332\nTest Accuracy: 95.85%\nEpoch 14: Avg loss: 1.4967 | Test Accuracy: 95.85% | Grad Norm: 0.1310\nTest Accuracy: 96.00%\nEpoch 15: Avg loss: 1.4957 | Test Accuracy: 96.00% | Grad Norm: 0.1284\nTest Accuracy: 95.60%\nEpoch 16: Avg loss: 1.4945 | Test Accuracy: 95.60% | Grad Norm: 0.1243\nTest Accuracy: 95.90%\nEpoch 17: Avg loss: 1.4934 | Test Accuracy: 95.90% | Grad Norm: 0.1206\nTest Accuracy: 95.86%\nEpoch 18: Avg loss: 1.4925 | Test Accuracy: 95.86% | Grad Norm: 0.1174\nTest Accuracy: 96.02%\nEpoch 19: Avg loss: 1.4917 | Test Accuracy: 96.02% | Grad Norm: 0.1177\nTest Accuracy: 96.04%\nEpoch 20: Avg loss: 1.4909 | Test Accuracy: 96.04% | Grad Norm: 0.1135\nTest Accuracy: 96.04%\nEpoch 21: Avg loss: 1.4899 | Test Accuracy: 96.04% | Grad Norm: 0.1111\nTest Accuracy: 96.12%\nEpoch 22: Avg loss: 1.4895 | Test Accuracy: 96.12% | Grad Norm: 0.1062\nTest Accuracy: 95.95%\nEpoch 23: Avg loss: 1.4886 | Test Accuracy: 95.95% | Grad Norm: 0.1022\nTest Accuracy: 96.06%\nEpoch 24: Avg loss: 1.4882 | Test Accuracy: 96.06% | Grad Norm: 0.1041\nTest Accuracy: 96.11%\nEpoch 25: Avg loss: 1.4875 | Test Accuracy: 96.11% | Grad Norm: 0.0986\nTest Accuracy: 95.97%\nEpoch 26: Avg loss: 1.4871 | Test Accuracy: 95.97% | Grad Norm: 0.1008\nTest Accuracy: 96.05%\nEpoch 27: Avg loss: 1.4866 | Test Accuracy: 96.05% | Grad Norm: 0.0974\nTest Accuracy: 96.17%\nEpoch 28: Avg loss: 1.4861 | Test Accuracy: 96.17% | Grad Norm: 0.0949\nTest Accuracy: 95.98%\nEpoch 29: Avg loss: 1.4856 | Test Accuracy: 95.98% | Grad Norm: 0.0924\nTest Accuracy: 95.99%\nEpoch 30: Avg loss: 1.4851 | Test Accuracy: 95.99% | Grad Norm: 0.0888\n\n\n\n\n# Initialize network\nnet = Network([784, 30, 10])\n# Nesterov with higher learning rate\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=100, eta=0.03, test_data=test_loader)\n\nTest Accuracy: 76.43%\nEpoch 1: Avg loss: 1.8002 | Test Accuracy: 76.43% | Grad Norm: 0.2058\nTest Accuracy: 77.04%\nEpoch 2: Avg loss: 1.7029 | Test Accuracy: 77.04% | Grad Norm: 0.1687\nTest Accuracy: 92.23%\nEpoch 3: Avg loss: 1.5921 | Test Accuracy: 92.23% | Grad Norm: 0.2415\nTest Accuracy: 93.17%\nEpoch 4: Avg loss: 1.5495 | Test Accuracy: 93.17% | Grad Norm: 0.2240\nTest Accuracy: 93.58%\nEpoch 5: Avg loss: 1.5385 | Test Accuracy: 93.58% | Grad Norm: 0.2117\nTest Accuracy: 94.10%\nEpoch 6: Avg loss: 1.5316 | Test Accuracy: 94.10% | Grad Norm: 0.2007\nTest Accuracy: 94.09%\nEpoch 7: Avg loss: 1.5265 | Test Accuracy: 94.09% | Grad Norm: 0.1922\nTest Accuracy: 94.60%\nEpoch 8: Avg loss: 1.5225 | Test Accuracy: 94.60% | Grad Norm: 0.1884\nTest Accuracy: 94.78%\nEpoch 9: Avg loss: 1.5194 | Test Accuracy: 94.78% | Grad Norm: 0.1826\nTest Accuracy: 94.94%\nEpoch 10: Avg loss: 1.5164 | Test Accuracy: 94.94% | Grad Norm: 0.1774\nTest Accuracy: 95.04%\nEpoch 11: Avg loss: 1.5141 | Test Accuracy: 95.04% | Grad Norm: 0.1738\nTest Accuracy: 95.08%\nEpoch 12: Avg loss: 1.5121 | Test Accuracy: 95.08% | Grad Norm: 0.1697\nTest Accuracy: 95.14%\nEpoch 13: Avg loss: 1.5102 | Test Accuracy: 95.14% | Grad Norm: 0.1653\nTest Accuracy: 95.21%\nEpoch 14: Avg loss: 1.5084 | Test Accuracy: 95.21% | Grad Norm: 0.1611\nTest Accuracy: 95.30%\nEpoch 15: Avg loss: 1.5070 | Test Accuracy: 95.30% | Grad Norm: 0.1587\nTest Accuracy: 95.37%\nEpoch 16: Avg loss: 1.5056 | Test Accuracy: 95.37% | Grad Norm: 0.1556\nTest Accuracy: 95.39%\nEpoch 17: Avg loss: 1.5044 | Test Accuracy: 95.39% | Grad Norm: 0.1545\nTest Accuracy: 95.55%\nEpoch 18: Avg loss: 1.5032 | Test Accuracy: 95.55% | Grad Norm: 0.1491\nTest Accuracy: 95.61%\nEpoch 19: Avg loss: 1.5022 | Test Accuracy: 95.61% | Grad Norm: 0.1479\nTest Accuracy: 95.58%\nEpoch 20: Avg loss: 1.5011 | Test Accuracy: 95.58% | Grad Norm: 0.1463\nTest Accuracy: 95.64%\nEpoch 21: Avg loss: 1.5002 | Test Accuracy: 95.64% | Grad Norm: 0.1422\nTest Accuracy: 95.64%\nEpoch 22: Avg loss: 1.4993 | Test Accuracy: 95.64% | Grad Norm: 0.1398\nTest Accuracy: 95.62%\nEpoch 23: Avg loss: 1.4985 | Test Accuracy: 95.62% | Grad Norm: 0.1392\nTest Accuracy: 95.65%\nEpoch 24: Avg loss: 1.4977 | Test Accuracy: 95.65% | Grad Norm: 0.1364\nTest Accuracy: 95.79%\nEpoch 25: Avg loss: 1.4970 | Test Accuracy: 95.79% | Grad Norm: 0.1352\nTest Accuracy: 95.73%\nEpoch 26: Avg loss: 1.4963 | Test Accuracy: 95.73% | Grad Norm: 0.1336\nTest Accuracy: 95.80%\nEpoch 27: Avg loss: 1.4956 | Test Accuracy: 95.80% | Grad Norm: 0.1289\nTest Accuracy: 95.77%\nEpoch 28: Avg loss: 1.4949 | Test Accuracy: 95.77% | Grad Norm: 0.1272\nTest Accuracy: 95.87%\nEpoch 29: Avg loss: 1.4943 | Test Accuracy: 95.87% | Grad Norm: 0.1255\nTest Accuracy: 95.87%\nEpoch 30: Avg loss: 1.4938 | Test Accuracy: 95.87% | Grad Norm: 0.1255\n\n\n([1.800204691628615,\n  1.7028700497547786,\n  1.5920979805588722,\n  1.54946473522981,\n  1.5385413242181143,\n  1.5315651088158289,\n  1.52650787405173,\n  1.5224893161654471,\n  1.519438457250595,\n  1.516427211523056,\n  1.5141158724427224,\n  1.512060732503732,\n  1.5102043944994608,\n  1.508429942548275,\n  1.5069557313720385,\n  1.5056361530621847,\n  1.5044344010154407,\n  1.5031717133720717,\n  1.5021622280478477,\n  1.501119362195333,\n  1.5001897305250167,\n  1.4993380693197251,\n  1.4985141108632087,\n  1.497682001252969,\n  1.4970344960490862,\n  1.4962749486168225,\n  1.4955746414462725,\n  1.4949430261651675,\n  1.4943470559716225,\n  1.4937834073702494],\n [76.43,\n  77.04,\n  92.23,\n  93.17,\n  93.58,\n  94.1,\n  94.09,\n  94.6,\n  94.78,\n  94.94,\n  95.04,\n  95.08,\n  95.14,\n  95.21,\n  95.3,\n  95.37,\n  95.39,\n  95.55,\n  95.61,\n  95.58,\n  95.64,\n  95.64,\n  95.62,\n  95.65,\n  95.79,\n  95.73,\n  95.8,\n  95.77,\n  95.87,\n  95.87],\n [0.20581414950755425,\n  0.16871541996622302,\n  0.2414942494224136,\n  0.2240473490561514,\n  0.21166856849451626,\n  0.20072671716084006,\n  0.1921971125874552,\n  0.1883950100892204,\n  0.18261653598578415,\n  0.17743887459334412,\n  0.17377534187049604,\n  0.16974949874098336,\n  0.16531859722120376,\n  0.16114312754284038,\n  0.15874298534259773,\n  0.1556003766371529,\n  0.1544842336561075,\n  0.14907820942316174,\n  0.1479095913441124,\n  0.14627366931884414,\n  0.14217055178209678,\n  0.13978232955163064,\n  0.13919855443627482,\n  0.136386921235188,\n  0.13516474480250934,\n  0.13358020712598834,\n  0.12890525200358025,\n  0.1271706948821969,\n  0.12548835482439244,\n  0.12545906078375993])\n\n\n\ntrain_losses, test_accuracies = train(net, train_loader, epochs=30, mini_batch_size=10, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 96.06%\nEpoch 1: Avg loss: 1.5120 | Test Accuracy: 96.06% | Grad Norm: 0.1748\nTest Accuracy: 96.15%\nEpoch 2: Avg loss: 1.5100 | Test Accuracy: 96.15% | Grad Norm: 0.1707\nTest Accuracy: 96.15%\nEpoch 3: Avg loss: 1.5095 | Test Accuracy: 96.15% | Grad Norm: 0.1695\nTest Accuracy: 96.19%\nEpoch 4: Avg loss: 1.5091 | Test Accuracy: 96.19% | Grad Norm: 0.1679\nTest Accuracy: 96.18%\nEpoch 5: Avg loss: 1.5088 | Test Accuracy: 96.18% | Grad Norm: 0.1674\nTest Accuracy: 96.19%\nEpoch 6: Avg loss: 1.5085 | Test Accuracy: 96.19% | Grad Norm: 0.1671\nTest Accuracy: 96.22%\nEpoch 7: Avg loss: 1.5083 | Test Accuracy: 96.22% | Grad Norm: 0.1678\nTest Accuracy: 96.21%\nEpoch 8: Avg loss: 1.5081 | Test Accuracy: 96.21% | Grad Norm: 0.1660\nTest Accuracy: 96.23%\nEpoch 9: Avg loss: 1.5079 | Test Accuracy: 96.23% | Grad Norm: 0.1664\nTest Accuracy: 96.26%\nEpoch 10: Avg loss: 1.5077 | Test Accuracy: 96.26% | Grad Norm: 0.1654\nTest Accuracy: 96.25%\nEpoch 11: Avg loss: 1.5075 | Test Accuracy: 96.25% | Grad Norm: 0.1655\nTest Accuracy: 96.20%\nEpoch 12: Avg loss: 1.5073 | Test Accuracy: 96.20% | Grad Norm: 0.1649\nTest Accuracy: 96.24%\nEpoch 13: Avg loss: 1.5072 | Test Accuracy: 96.24% | Grad Norm: 0.1643\nTest Accuracy: 96.22%\nEpoch 14: Avg loss: 1.5070 | Test Accuracy: 96.22% | Grad Norm: 0.1645\nTest Accuracy: 96.27%\nEpoch 15: Avg loss: 1.5068 | Test Accuracy: 96.27% | Grad Norm: 0.1639\nTest Accuracy: 96.27%\nEpoch 16: Avg loss: 1.5067 | Test Accuracy: 96.27% | Grad Norm: 0.1646\nTest Accuracy: 96.25%\nEpoch 17: Avg loss: 1.5065 | Test Accuracy: 96.25% | Grad Norm: 0.1632\nTest Accuracy: 96.24%\nEpoch 18: Avg loss: 1.5064 | Test Accuracy: 96.24% | Grad Norm: 0.1635\nTest Accuracy: 96.27%\nEpoch 19: Avg loss: 1.5062 | Test Accuracy: 96.27% | Grad Norm: 0.1625\nTest Accuracy: 96.28%\nEpoch 20: Avg loss: 1.5061 | Test Accuracy: 96.28% | Grad Norm: 0.1630\nTest Accuracy: 96.28%\nEpoch 21: Avg loss: 1.5060 | Test Accuracy: 96.28% | Grad Norm: 0.1621\nTest Accuracy: 96.29%\nEpoch 22: Avg loss: 1.5058 | Test Accuracy: 96.29% | Grad Norm: 0.1621\nTest Accuracy: 96.30%\nEpoch 23: Avg loss: 1.5057 | Test Accuracy: 96.30% | Grad Norm: 0.1623\nTest Accuracy: 96.26%\nEpoch 24: Avg loss: 1.5056 | Test Accuracy: 96.26% | Grad Norm: 0.1615\nTest Accuracy: 96.27%\nEpoch 25: Avg loss: 1.5054 | Test Accuracy: 96.27% | Grad Norm: 0.1624\nTest Accuracy: 96.29%\nEpoch 26: Avg loss: 1.5053 | Test Accuracy: 96.29% | Grad Norm: 0.1614\nTest Accuracy: 96.29%\nEpoch 27: Avg loss: 1.5052 | Test Accuracy: 96.29% | Grad Norm: 0.1602\nTest Accuracy: 96.30%\nEpoch 28: Avg loss: 1.5051 | Test Accuracy: 96.30% | Grad Norm: 0.1613\nTest Accuracy: 96.25%\nEpoch 29: Avg loss: 1.5049 | Test Accuracy: 96.25% | Grad Norm: 0.1605\nTest Accuracy: 96.31%\nEpoch 30: Avg loss: 1.5048 | Test Accuracy: 96.31% | Grad Norm: 0.1602\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 train_losses, test_accuracies = train(net, train_loader, epochs=30, mini_batch_size=10, eta=0.001, test_data=test_loader)\n\nValueError: too many values to unpack (expected 2)\n\n\n\n\na = train(net, train_loader, epochs=1, mini_batch_size=10, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 95.72%\nEpoch 1: Avg loss: 1.4899 | Test Accuracy: 95.72% | Grad Norm: 0.0397\n\n\n\nlayer_grad_norms = []\n\nfor name, param in net.named_parameters():\n    print(name)\n    print(param)\n    if param.grad is not None:\n        grad_norm = param.grad.data.norm(2).item()\n        layer_grad_norms.append((name, grad_norm))\n    else:\n        layer_grad_norms.append((name, 0.0))  # No gradient yet\n\nreturn layer_grad_norms\n\nlayers.0.weight\nParameter containing:\ntensor([[-0.0277,  0.0186,  0.1492,  ..., -0.0462,  0.0174, -0.0015],\n        [-0.0329,  0.0962, -0.0132,  ..., -0.0648,  0.0253,  0.0341],\n        [-0.0173, -0.0853, -0.0423,  ...,  0.0738,  0.0114, -0.0589],\n        ...,\n        [ 0.1535, -0.1095, -0.0395,  ...,  0.0335,  0.0795,  0.0824],\n        [-0.0223,  0.0950, -0.0893,  ..., -0.1585,  0.0372, -0.0335],\n        [-0.0198,  0.0132, -0.0937,  ...,  0.0433, -0.0662, -0.0414]],\n       requires_grad=True)\nlayers.0.bias\nParameter containing:\ntensor([ 2.3832e-02, -3.7323e-04,  1.2277e-02,  1.8319e-02, -5.1513e-02,\n        -2.3629e-02,  1.4832e-02,  6.8761e-03, -1.2110e-02, -1.2635e-02,\n        -9.4455e-04, -4.4049e-02, -6.4198e-02, -1.6436e-01, -1.6507e-01,\n        -2.7542e-02,  1.5059e-01, -6.0168e-02, -3.9797e-03, -2.4990e-02,\n        -2.9195e-01, -1.0166e-01, -4.9884e-02, -1.6592e-01, -3.3845e-02,\n         5.9276e-03, -4.0219e-02, -9.5545e-02,  1.2567e-02, -2.0245e-01,\n        -4.1579e-02, -9.2223e-02,  1.1554e-01,  1.8571e-02, -1.0655e-02,\n         1.7328e-02, -1.1535e-01, -4.8733e-02, -5.1659e-02, -1.1494e-01,\n         1.0849e-01, -1.4069e-02,  6.6728e-02, -2.9440e-04, -4.4624e-02,\n         1.3083e-01, -7.4154e-02,  8.4000e-02, -1.6311e-01, -1.0475e-01,\n        -4.9350e-02,  1.4453e-02, -2.7412e-03, -2.7809e-02, -2.8654e-02,\n        -6.9064e-02, -4.3939e-02,  4.3201e-03, -1.6227e-01, -1.3430e-01,\n        -9.5933e-02, -2.0832e-03, -1.0092e-01, -1.8770e-01,  3.9871e-02,\n        -1.9100e-02, -1.4649e-02, -5.1564e-02, -1.1432e-01,  7.2691e-02,\n        -1.1487e-01, -7.7698e-02,  5.5250e-02, -1.6274e-02, -1.1286e-01,\n        -7.1349e-02, -2.1757e-02,  5.5312e-02, -2.9633e-02,  8.4041e-02,\n         9.3945e-03, -1.2500e-01, -5.3607e-02, -1.1421e-01,  2.1070e-02,\n         3.6210e-02, -1.3263e-01, -1.2989e-01,  7.7408e-02,  4.2892e-02,\n         5.3963e-03, -1.7807e-01, -2.5772e-01, -5.4491e-02, -1.3474e-01,\n        -2.3726e-02,  7.4340e-03,  1.0570e-02, -6.6804e-02, -1.1311e-02,\n        -8.3612e-02,  5.9596e-02,  9.6855e-04, -1.3488e-01,  8.6554e-02,\n        -2.1538e-02, -4.6016e-02,  5.8769e-02, -6.0230e-02, -5.6201e-02,\n         9.9794e-03, -9.2099e-02, -9.5633e-03, -6.8934e-03, -1.6703e-01,\n        -8.8996e-03, -4.5101e-02, -1.3016e-01, -3.9658e-03,  8.2293e-02,\n        -1.9673e-02, -2.1972e-01, -1.7756e-01,  7.5067e-02, -1.8714e-02,\n        -4.0988e-02, -9.6159e-05, -6.6666e-02, -4.5227e-02, -3.7415e-02,\n        -1.8056e-01, -7.8020e-02,  6.5739e-02, -6.0611e-02,  2.6667e-02,\n        -7.9634e-02,  7.8923e-02, -2.3328e-01,  2.2107e-02,  3.5781e-02,\n        -1.4766e-02, -2.4327e-03,  3.6188e-02, -2.8073e-03, -9.3851e-02,\n         3.5435e-02, -5.0998e-02, -2.1381e-01,  2.9832e-03,  3.7133e-02,\n        -1.5049e-01, -7.7870e-02, -1.0665e-01,  2.5553e-02, -4.6339e-03,\n        -1.2648e-01, -4.1999e-02,  5.7969e-03, -2.0520e-02, -2.0639e-02,\n         8.3997e-02, -7.5077e-02, -3.1162e-02, -7.8091e-04,  6.8792e-03,\n        -6.1211e-03, -1.5982e-01, -3.1444e-02, -3.7581e-02,  1.4392e-02,\n        -3.1285e-02, -2.9293e-01, -2.9776e-02, -7.9170e-03, -3.1782e-02,\n         1.7612e-04,  2.7610e-02,  3.9527e-02, -1.1132e-01, -7.6001e-02,\n        -8.2587e-02, -6.5646e-02, -7.7155e-02, -1.6972e-01,  2.0083e-01,\n         5.7286e-02, -1.7126e-01, -9.8549e-02,  3.6326e-02, -1.3767e-02,\n         1.6465e-02, -6.7347e-04, -3.5425e-02,  2.4498e-02,  1.0026e-01,\n        -7.3113e-02,  4.3011e-05, -1.0960e-01,  2.0420e-01, -6.6743e-03,\n        -1.1513e-02, -7.4681e-02, -7.2470e-02, -1.7773e-02,  5.7757e-02,\n         1.0694e-01, -5.0625e-02, -1.9161e-02,  3.3249e-02, -2.3076e-02,\n        -6.0484e-02, -2.6394e-02,  1.5041e-02, -1.7765e-02, -2.4254e-02,\n        -4.9849e-02,  3.2220e-02,  4.0435e-04,  9.7431e-03, -7.8602e-03,\n         8.3068e-02,  1.4041e-01,  3.6472e-02,  4.1121e-02, -6.0264e-03,\n        -8.5728e-02,  2.6422e-02, -4.1375e-02,  7.6736e-02, -3.7764e-02,\n         8.1073e-02, -9.5080e-02, -3.3567e-02, -2.3839e-01,  2.4073e-02,\n        -6.4826e-02,  2.4406e-02, -6.5078e-03, -3.2249e-02, -1.4667e-02,\n         4.7009e-04, -1.1545e-01,  4.0913e-03, -2.0655e-02, -2.2490e-01,\n        -6.1001e-02, -5.5207e-02, -7.5740e-02, -7.7023e-02,  1.6743e-02,\n         2.4873e-03, -2.3222e-02, -4.4354e-03,  4.3655e-03, -1.7922e-02,\n         7.1855e-02,  1.4560e-02, -7.8529e-02, -7.9027e-02, -1.7677e-02,\n         4.1625e-02,  5.8449e-02, -1.6991e-02,  1.6376e-02,  2.1746e-02,\n        -1.0373e-03, -2.7459e-02, -7.5050e-02, -2.3569e-02,  1.5776e-02,\n        -2.3194e-03, -4.5207e-02, -4.7494e-02, -1.1734e-03, -4.8874e-03,\n        -1.5740e-03, -2.2495e-01, -5.9987e-02,  3.6710e-02, -2.0347e-02,\n         6.5534e-03,  4.0868e-02, -5.1758e-02, -7.4455e-02, -4.0618e-02,\n         4.8911e-03,  1.3344e-02, -1.3366e-01,  3.7696e-02, -8.4637e-02,\n         7.8612e-03, -4.3043e-03, -5.8001e-02, -2.0213e-01,  5.7936e-03,\n        -3.6219e-02,  1.9084e-02,  1.2807e-02, -1.0162e-01, -1.3247e-01,\n        -1.1911e-02, -2.2586e-01, -1.4757e-01,  3.9221e-02, -2.8038e-02,\n        -9.1034e-02, -4.3826e-02,  4.8492e-03, -1.9928e-02,  1.2048e-02,\n        -6.8796e-02, -4.7659e-02,  5.3157e-02,  2.9983e-02, -2.1112e-01,\n        -1.5204e-01, -2.1964e-01, -3.1895e-02, -1.3881e-01, -6.8215e-03,\n        -7.9684e-02,  5.1243e-02,  6.8785e-02, -3.4916e-02, -4.8016e-02,\n        -3.5940e-02, -6.3466e-02, -1.2011e-02, -4.2613e-02,  4.2584e-03,\n         5.1136e-02, -1.8545e-03, -1.1297e-01, -4.4978e-02, -2.9800e-02,\n        -1.1624e-01,  4.1026e-05,  1.6769e-03,  1.5415e-02, -1.0845e-01,\n        -6.2645e-02,  7.8771e-02, -1.1379e-01,  1.0626e-01,  1.1746e-01,\n        -1.6912e-03, -2.6942e-02,  1.1431e-02,  1.9547e-02, -2.0514e-02,\n        -1.0834e-01, -1.2458e-01,  1.1545e-02, -4.3064e-02, -1.0030e-02,\n         1.8004e-02,  6.7254e-02, -7.6608e-02,  1.3816e-02, -9.4544e-02,\n        -1.8811e-02, -6.3729e-02, -1.3190e-01,  8.6543e-03, -1.0072e-02,\n         9.8633e-02, -1.0473e-01, -3.0692e-02,  6.3956e-03, -3.7653e-02,\n        -7.9369e-02,  8.8415e-03,  1.2024e-02, -5.9945e-02, -1.9900e-02,\n        -4.2639e-02,  2.8175e-02,  1.9229e-02,  1.4658e-03,  1.8305e-02,\n        -1.6745e-02, -5.2276e-02,  2.7727e-02,  2.1788e-02, -5.1007e-03,\n        -1.6311e-02, -1.7414e-02, -1.0071e-01, -8.6823e-03,  1.4742e-02,\n        -3.2199e-02, -1.0221e-01, -3.7566e-02, -2.7086e-02, -1.5009e-02,\n        -4.5398e-02, -1.6370e-01, -6.3808e-02, -1.1735e-01, -3.2840e-01,\n        -2.1360e-01,  2.2701e-02, -1.1186e-03,  3.0121e-02, -5.8815e-02,\n        -5.1384e-02, -1.1315e-01,  5.0047e-02,  5.8961e-02, -2.8797e-01,\n         2.4115e-02, -6.1966e-02, -1.9940e-02,  2.2397e-02, -4.3039e-02,\n        -4.1686e-02, -1.8877e-01, -5.0067e-02, -8.2233e-03, -4.9077e-03,\n         3.1763e-02, -4.2263e-02, -2.7041e-02,  5.6800e-02, -4.1121e-02,\n        -8.7963e-02, -2.6075e-01, -4.5471e-02, -1.3477e-01, -3.0008e-02,\n        -1.2744e-01, -1.2536e-02,  2.3327e-02, -3.0500e-02, -1.9355e-02,\n         7.8899e-03, -1.3227e-02, -9.7468e-03, -9.4424e-03, -3.9509e-02,\n        -2.4584e-01,  6.0538e-02,  2.8726e-03,  4.2925e-02, -1.2237e-02,\n        -3.3323e-02, -2.2288e-01, -2.8869e-02, -1.0219e-01, -1.4286e-03,\n         9.3140e-02, -3.5493e-02, -6.1311e-02,  1.3519e-01, -1.5956e-02,\n         1.9448e-03, -7.7090e-02, -2.2282e-02,  2.0689e-02, -9.5254e-02,\n         3.3063e-02, -6.0423e-02,  4.9028e-02, -2.3577e-01,  1.8244e-02,\n         2.9586e-02, -2.7721e-02,  4.7155e-02, -1.8999e-02,  2.9624e-06,\n        -6.9145e-02, -5.7038e-02, -2.4466e-02, -7.3180e-02, -1.5564e-02,\n        -1.1377e-01, -7.9811e-03,  2.4378e-02, -1.2085e-01,  1.7233e-02,\n         7.5590e-03,  1.1963e-02,  1.5651e-02, -6.2775e-03,  1.1785e-01,\n        -8.3776e-02, -5.4163e-02,  7.1721e-03,  1.1758e-02,  1.7843e-02,\n        -2.7940e-01, -5.1140e-03, -5.0495e-03, -1.4303e-01, -6.8661e-03,\n        -5.0605e-02, -1.4809e-02, -2.1530e-02, -2.6329e-02, -9.5911e-02,\n        -1.4414e-01, -3.1782e-02,  4.8910e-02, -3.2928e-02, -1.8627e-02,\n         7.6625e-02, -9.7085e-03, -3.7559e-02,  2.4670e-02,  1.4199e-01,\n        -6.8952e-02, -5.4273e-02], requires_grad=True)\nlayers.1.weight\nParameter containing:\ntensor([[-0.0874, -0.0371, -0.0256,  ...,  0.0554, -0.0252, -0.0774],\n        [ 0.0425, -0.1103, -0.0323,  ...,  0.1190,  0.1555,  0.0357],\n        [-0.0857, -0.0248,  0.0412,  ..., -0.0150, -0.0415, -0.0093],\n        ...,\n        [-0.0754, -0.0533,  0.0477,  ...,  0.1281,  0.0499, -0.0761],\n        [-0.1245,  0.0510,  0.0436,  ..., -0.0211,  0.0245, -0.0484],\n        [ 0.0167,  0.0136, -0.0603,  ...,  0.1475, -0.0516,  0.0957]],\n       requires_grad=True)\nlayers.1.bias\nParameter containing:\ntensor([-1.1656e-02, -7.8881e-02, -1.6796e-02,  1.4739e-02,  2.7130e-04,\n        -4.4321e-02, -3.6571e-02, -1.4769e-02, -5.6753e-02, -1.0497e-01,\n         1.2703e-02, -5.5395e-02,  1.8311e-02, -3.1623e-02, -1.1168e-01,\n         2.7370e-02, -6.3311e-02, -2.9809e-02, -5.5490e-02, -3.2793e-02,\n        -2.0614e-02, -6.7264e-03, -9.4669e-02,  2.1386e-02, -5.7758e-02,\n        -5.2045e-02, -3.2438e-02, -6.5869e-02, -2.6769e-02,  1.0873e-01,\n        -2.9416e-02,  3.3048e-02, -3.5733e-02, -1.5146e-02, -6.8485e-04,\n        -1.2649e-02, -2.6116e-02,  3.5453e-04, -6.8944e-02, -1.2899e-02,\n         1.7226e-02,  7.7284e-02, -8.8453e-03, -1.5189e-01, -1.9968e-02,\n        -1.2741e-01, -6.7156e-02, -1.1191e-01, -2.4591e-02,  8.3636e-03,\n        -3.7409e-02, -3.5582e-02, -3.1573e-03, -4.1433e-02, -1.2987e-02,\n        -2.9006e-03, -3.3904e-02, -1.1877e-02,  2.7988e-02, -7.5086e-02,\n        -2.0908e-02, -4.0054e-02, -8.1890e-02, -8.6035e-02, -8.7662e-02,\n        -2.4993e-02, -9.0150e-02, -4.3544e-02, -1.6000e-02, -5.6091e-02,\n        -1.1372e-01, -2.3612e-02, -3.9141e-02, -7.5383e-03, -6.2086e-02,\n         7.1529e-03,  6.4194e-03, -4.2739e-02, -2.9471e-02, -6.3935e-03,\n         5.1967e-02, -1.5016e-02, -7.1085e-03, -6.2118e-02,  5.7958e-02,\n        -4.3205e-02, -2.3574e-02, -7.5852e-03, -2.5554e-02, -3.5177e-02,\n         2.7569e-02, -4.8473e-02, -5.1454e-03, -4.6803e-02, -7.3303e-02,\n        -7.2189e-03,  4.4090e-03, -1.2420e-01,  1.3596e-02, -7.4608e-02,\n        -2.4801e-02, -5.1133e-03, -3.6271e-02, -8.5957e-02,  2.1907e-02,\n         1.6648e-03, -8.3137e-02, -6.3641e-02, -2.9626e-02, -2.6079e-02,\n         7.8786e-03, -6.1100e-03,  3.9816e-03,  3.4330e-03,  5.1674e-02,\n        -7.4918e-02, -3.0392e-02, -4.5488e-02, -3.1736e-02,  4.0115e-02,\n        -5.9606e-02,  5.3223e-02,  3.0372e-02, -6.1482e-02, -2.3222e-02,\n        -3.3901e-02, -4.8754e-04, -4.3621e-02, -3.7482e-02,  3.0265e-03,\n        -2.1248e-02,  3.0577e-02, -4.1977e-02, -1.9406e-02, -9.9932e-03,\n        -1.3039e-03, -6.5709e-02, -5.2836e-03, -5.0153e-02,  6.8022e-02,\n        -2.6796e-02, -2.7293e-02, -5.2970e-02, -9.2929e-02, -3.4156e-02,\n        -9.7800e-02, -4.0548e-02, -2.2526e-02, -6.6727e-02,  2.2520e-02,\n         1.1381e-02, -2.8818e-02, -4.2198e-02, -3.0733e-02,  8.6367e-03,\n        -2.1726e-02, -2.6924e-02, -9.0016e-02, -1.4618e-02, -3.2094e-02,\n         4.7775e-02, -9.8367e-03, -1.4320e-02, -6.5571e-03, -1.6927e-02,\n        -4.1116e-02, -2.4782e-02, -7.1850e-02,  1.5173e-02, -1.0494e-01,\n        -3.5800e-02, -1.9203e-02, -4.0382e-02, -4.6074e-02, -3.0932e-02,\n        -3.4901e-02,  2.9938e-02, -3.7786e-02, -2.3649e-02, -4.5941e-03,\n        -8.0732e-02,  2.7692e-02, -5.3503e-02, -1.8047e-02, -3.4937e-02,\n        -9.3240e-02,  3.6747e-02, -8.8033e-02, -2.7246e-02, -2.1628e-02,\n         3.7960e-03, -1.9516e-02, -3.9107e-02,  1.1009e-02,  4.7369e-05,\n        -6.7147e-02,  3.7004e-02,  2.7707e-02,  1.2785e-02, -3.4095e-02,\n         2.9274e-02, -1.1500e-01, -4.9623e-02, -2.7543e-02, -3.2737e-02,\n         5.6155e-02, -3.3053e-02, -8.3444e-03,  6.2556e-02, -6.2315e-03,\n        -1.2396e-01,  6.1719e-02, -4.0214e-02, -5.9774e-02, -7.8146e-02,\n        -2.5309e-02,  5.4238e-03, -2.2835e-02, -2.5153e-02, -1.1287e-01,\n        -3.3478e-02,  2.8738e-03, -1.1937e-01, -7.0906e-02, -4.0252e-02,\n        -7.7082e-02, -2.7376e-02, -8.3747e-02, -2.5282e-02, -4.0541e-02,\n        -4.6833e-02,  3.8858e-02,  2.7560e-02, -1.4941e-02, -2.4571e-02,\n        -5.8059e-02, -2.9100e-02, -1.8905e-02, -1.9006e-02, -3.9229e-02,\n        -1.4742e-02, -5.6469e-03, -4.0630e-02, -1.9498e-02, -4.0714e-02,\n         2.0711e-02, -1.6885e-02, -7.4511e-02,  6.5749e-03, -8.3187e-03,\n        -1.5554e-02, -6.0618e-02, -2.3210e-02, -1.4522e-02, -1.1909e-01,\n        -6.6773e-02, -5.5303e-02,  1.5268e-02, -5.5484e-02, -1.4070e-02,\n        -6.5823e-03, -2.3190e-02, -1.4556e-02, -1.6916e-03, -6.5549e-02,\n         2.7605e-02, -4.1009e-02, -1.9966e-02,  3.3095e-03, -2.6868e-02,\n        -2.1795e-02, -7.0454e-02,  1.2649e-02, -4.9417e-03, -1.4527e-02,\n         1.3407e-02, -2.6475e-02, -7.9786e-03, -7.8957e-02,  1.2320e-02,\n         1.7450e-02, -2.2791e-02, -4.4733e-02, -2.3688e-02, -4.3946e-02,\n        -4.6846e-02, -8.6492e-02, -1.2584e-02, -1.1563e-02, -4.4576e-02,\n        -3.6462e-02, -3.9583e-02, -3.9825e-02, -6.7829e-03, -8.8910e-02,\n        -2.4623e-02, -2.6069e-02, -5.7603e-02, -2.1978e-02, -1.7244e-02,\n        -3.9499e-02, -8.6440e-03, -1.3826e-02,  9.1622e-03, -2.6005e-02,\n         2.9108e-02,  1.8789e-02, -4.7901e-03, -4.2235e-02, -2.9934e-02,\n         2.5715e-02, -2.4395e-02, -2.8580e-02, -1.1719e-01, -3.6412e-02,\n        -7.4101e-03, -3.0972e-02, -3.6864e-02, -2.9637e-02, -2.6952e-02,\n        -2.5204e-03, -3.4979e-02, -5.2986e-02, -3.3898e-02, -3.0058e-02,\n         1.7774e-02, -1.1629e-02,  4.0972e-02, -2.1281e-02, -8.2517e-03,\n        -3.7794e-02, -2.6688e-02, -1.6831e-01,  1.3783e-03, -6.0891e-03,\n        -7.4763e-02, -4.6234e-02, -4.5059e-02, -5.2703e-02, -7.4866e-03,\n        -1.8245e-02, -5.7457e-03, -8.3088e-02,  5.1606e-02,  6.6013e-03,\n        -2.0897e-02,  7.7131e-03, -9.9760e-02, -2.5458e-02, -2.3730e-02,\n        -2.7102e-02, -2.4756e-02, -8.3383e-03, -8.2826e-02, -4.5507e-02,\n        -6.4249e-02, -1.5440e-02, -1.2754e-01, -1.5944e-03, -1.5714e-02,\n        -6.4176e-02, -1.7548e-02, -9.9496e-03, -4.6046e-02, -3.2688e-02,\n        -2.8735e-02, -2.5178e-02, -9.9911e-02, -3.1866e-02, -1.0919e-01,\n        -1.8756e-02,  2.6028e-02,  6.9896e-03, -4.9614e-02, -3.7667e-02,\n        -2.0275e-02, -6.2903e-02, -6.9062e-03, -2.0380e-02, -2.7048e-02,\n        -2.1558e-02, -5.9511e-02, -4.7602e-02, -6.6926e-02, -6.0140e-02,\n        -1.5809e-02, -7.1942e-02, -2.1639e-02, -6.3492e-02, -3.6448e-02,\n        -3.7546e-02, -4.2178e-02, -4.0144e-02, -6.8733e-02, -5.5325e-02,\n         2.2391e-02,  1.2441e-02, -2.0922e-02, -7.7211e-02, -2.3285e-02,\n         1.9334e-02, -4.8998e-03,  3.7059e-03, -1.8522e-02, -8.4334e-02,\n         1.0642e-01, -5.8298e-02, -7.0504e-03, -5.4305e-02, -5.5592e-02,\n        -2.5502e-02, -3.9430e-02, -7.3971e-02, -5.5690e-05, -7.5740e-02,\n        -2.9525e-02, -5.6033e-02, -5.7204e-02, -2.4063e-02,  7.1725e-02,\n        -3.1387e-02, -2.7072e-02, -1.3153e-01,  2.0391e-03,  1.4520e-02,\n        -1.0625e-03, -9.6767e-02, -2.2868e-02, -2.8156e-02,  3.3088e-02,\n        -2.6966e-02, -6.9867e-02, -3.3126e-02, -5.9885e-03, -1.0265e-02,\n        -5.6630e-02, -4.5759e-04,  3.1627e-02,  2.6288e-02, -1.2642e-03,\n        -2.2507e-02, -1.2503e-02, -1.3062e-02,  9.6700e-03, -2.4003e-02,\n        -2.0974e-02,  4.2986e-02, -2.3519e-02, -6.4087e-03,  7.1256e-02,\n         5.8117e-02,  2.3235e-02, -3.5151e-02,  3.6348e-02,  4.2754e-03,\n        -5.5187e-02, -7.0037e-02, -6.7039e-03, -7.2431e-02, -3.3539e-02,\n        -3.1735e-02, -1.0009e-01, -3.1190e-02, -9.6952e-03, -6.1894e-04,\n        -9.6628e-03, -3.7643e-02, -7.6668e-02, -1.7174e-02, -1.4805e-02,\n        -3.6098e-02, -8.0668e-02, -1.7089e-02, -6.9967e-02,  5.9400e-02,\n        -4.3530e-02,  3.8900e-02, -4.1923e-03, -8.9462e-02, -3.8623e-02,\n         4.9639e-03, -2.6033e-02,  1.3188e-02, -9.3202e-02, -4.7561e-02,\n        -2.3808e-02, -4.0765e-02, -3.9694e-02, -9.3765e-02,  3.9986e-02,\n        -6.8962e-03, -5.7063e-02, -4.9603e-02, -1.0491e-02, -1.1659e-02,\n        -9.5441e-02, -5.1461e-02, -2.7382e-02, -1.2717e-02,  4.9109e-02,\n        -1.0926e-01, -5.7250e-02, -3.5621e-02, -8.4102e-03, -4.9192e-02,\n        -2.1019e-02, -6.2706e-02, -1.3100e-03, -2.7876e-02, -3.2167e-02,\n        -4.4947e-02, -6.3159e-03], requires_grad=True)\nlayers.2.weight\nParameter containing:\ntensor([[ 0.0511, -0.3444, -0.0517,  ..., -0.0300,  0.0711, -0.0345],\n        [-0.0223, -0.1594,  0.1274,  ..., -0.0691,  0.0764,  0.0065],\n        [ 0.0430, -0.1317,  0.1408,  ..., -0.1247, -0.0559, -0.1374],\n        ...,\n        [ 0.1045, -0.0980, -0.0170,  ..., -0.1047,  0.0458,  0.0293],\n        [-0.0203, -0.0833,  0.0351,  ..., -0.0284,  0.0125,  0.2015],\n        [ 0.1091,  0.1767, -0.0027,  ...,  0.0547, -0.0238, -0.0629]],\n       requires_grad=True)\nlayers.2.bias\nParameter containing:\ntensor([-0.0455, -0.0318,  0.1457, -0.0227, -0.1118, -0.0510, -0.0020, -0.0250,\n         0.0611, -0.0528,  0.1319, -0.0034, -0.0049, -0.0078, -0.0062,  0.0327,\n        -0.0145,  0.0054, -0.0275,  0.0016, -0.0043, -0.0392, -0.0698, -0.0250,\n        -0.0070, -0.0579,  0.0701, -0.0618, -0.0075, -0.1154,  0.0138,  0.0040,\n        -0.0984, -0.0334, -0.0765, -0.0350, -0.0436,  0.0124, -0.0198,  0.0256,\n        -0.0078, -0.0478, -0.0301,  0.0738,  0.0370, -0.0916,  0.0829,  0.0043,\n        -0.0077, -0.0039, -0.0253, -0.0796, -0.0026, -0.0613,  0.0002, -0.0331,\n        -0.0447,  0.0155, -0.0182, -0.1143, -0.0076, -0.0128,  0.0485,  0.0009,\n         0.0127,  0.0162, -0.0399,  0.0735, -0.0454, -0.0169, -0.0501, -0.0374,\n         0.0638, -0.0674, -0.0626, -0.0151, -0.0379, -0.0167,  0.0155, -0.1116,\n        -0.0455, -0.0590, -0.0413, -0.0108, -0.0082, -0.0305, -0.0090, -0.0740,\n        -0.0094, -0.0333, -0.0375, -0.0438, -0.0580, -0.0576,  0.0293,  0.0632,\n        -0.0506,  0.0277,  0.0190, -0.0499, -0.0771, -0.0078, -0.0091, -0.0631,\n        -0.0338, -0.0089,  0.0392, -0.0104, -0.0070,  0.0002, -0.0666, -0.0615,\n        -0.0145,  0.0164,  0.0808, -0.0875, -0.0186, -0.0018, -0.0634, -0.0080,\n        -0.0348, -0.0483, -0.0343, -0.0456,  0.0396, -0.0335, -0.0043, -0.0461,\n        -0.0448, -0.0639, -0.0159, -0.0069, -0.0179, -0.0274, -0.0072, -0.0239,\n         0.0603, -0.0101, -0.0842, -0.0187, -0.0817, -0.0339,  0.1663, -0.1386,\n        -0.0497, -0.0184, -0.0584, -0.0099, -0.0014, -0.0266, -0.0054, -0.0028,\n         0.0159, -0.1124,  0.0295, -0.1617, -0.0083,  0.0588, -0.0339, -0.0249,\n         0.0378, -0.0017, -0.0215,  0.0170, -0.0173, -0.0145, -0.0749, -0.0923,\n         0.0135, -0.0280,  0.0915, -0.0309, -0.0473, -0.0242, -0.0113, -0.0447,\n        -0.0012, -0.0227,  0.0879, -0.0191, -0.0295,  0.0057, -0.0438, -0.0240,\n        -0.0112, -0.0740, -0.0156,  0.0117,  0.0178,  0.0811, -0.0369, -0.0865,\n        -0.0231,  0.0069,  0.0544,  0.0806, -0.0295, -0.0378,  0.0039, -0.0007,\n        -0.0037, -0.0320, -0.0676, -0.0494,  0.0182,  0.0535, -0.0283, -0.0839,\n        -0.0969, -0.0802, -0.0284, -0.0348, -0.0242, -0.0019, -0.0440, -0.0030,\n        -0.0216, -0.0477, -0.0917,  0.0036, -0.0546,  0.0082,  0.0058, -0.0642,\n        -0.0097, -0.0347, -0.0189, -0.0338,  0.0093, -0.0053, -0.0037, -0.0161,\n        -0.0710, -0.0275, -0.0445, -0.0079,  0.0136,  0.0119, -0.0232, -0.0562,\n        -0.0942, -0.0370, -0.0361, -0.0042, -0.0414, -0.0469, -0.1341, -0.0379,\n        -0.0310,  0.0348, -0.0203, -0.0898, -0.0361,  0.0610, -0.0039, -0.0346],\n       requires_grad=True)\nlayers.3.weight\nParameter containing:\ntensor([[ 0.1022,  0.0950, -0.0200,  ...,  0.1093, -0.0572,  0.1176],\n        [ 0.0925,  0.0328,  0.1833,  ...,  0.1658,  0.0289,  0.1058],\n        [ 0.1852, -0.0007, -0.1388,  ..., -0.1596,  0.2128,  0.0745],\n        ...,\n        [ 0.1026,  0.0100, -0.0292,  ..., -0.1517, -0.0477, -0.0866],\n        [-0.0281,  0.0757, -0.1179,  ...,  0.1466, -0.0991,  0.1789],\n        [ 0.0191, -0.1582, -0.0368,  ..., -0.0891,  0.1383, -0.0979]],\n       requires_grad=True)\nlayers.3.bias\nParameter containing:\ntensor([-0.0567,  0.1323, -0.0587, -0.0691,  0.0361,  0.0179, -0.0111,  0.0716,\n        -0.0298, -0.0418, -0.0742, -0.0092, -0.0093, -0.0421, -0.0045,  0.0051,\n        -0.0269,  0.0095, -0.0224, -0.0574, -0.0674, -0.0127, -0.0444, -0.0213,\n        -0.0070,  0.0557, -0.0088, -0.0163, -0.0145, -0.0336, -0.0133, -0.0435,\n        -0.0725,  0.0486,  0.0492,  0.0198, -0.0972, -0.0119, -0.0310,  0.0416,\n        -0.0304, -0.0187,  0.0074, -0.0708, -0.0486,  0.0733,  0.0067, -0.0447,\n         0.0609, -0.0555, -0.0316, -0.0073, -0.0226,  0.0516, -0.0051,  0.0522,\n        -0.0666,  0.0046,  0.0638, -0.0376,  0.0273,  0.0015,  0.0132, -0.0207,\n        -0.0081, -0.0350, -0.0069, -0.0316, -0.0382,  0.0070, -0.0150, -0.0251,\n         0.0574, -0.0156, -0.0682, -0.0243,  0.0436,  0.0092, -0.0633,  0.0068,\n        -0.0068,  0.0008,  0.0015,  0.0317, -0.1401, -0.0227,  0.0176, -0.0204,\n         0.0010,  0.0301,  0.0311,  0.0013, -0.0427, -0.0626, -0.0042, -0.0654,\n        -0.0295, -0.0181, -0.0590, -0.0262, -0.0290, -0.0183, -0.0004,  0.0194,\n        -0.0084,  0.0196, -0.0103,  0.0006,  0.0482, -0.0180, -0.0611, -0.0065,\n        -0.0072, -0.0769, -0.0203,  0.0012, -0.0037,  0.0032, -0.0282, -0.0093,\n        -0.0370, -0.0268, -0.0137,  0.0137,  0.0006,  0.0083, -0.0105, -0.0286,\n        -0.0471, -0.0215, -0.0210,  0.0162, -0.0852,  0.0115, -0.0063,  0.0940,\n        -0.0073, -0.0576, -0.0151, -0.0802, -0.0063, -0.0085,  0.0152,  0.0735,\n        -0.0394, -0.0364,  0.0203, -0.0088, -0.0023,  0.0969, -0.0107,  0.0306,\n        -0.0132, -0.0500,  0.0244,  0.0319, -0.0336, -0.0020, -0.0321, -0.0161,\n         0.0210, -0.0080, -0.0112, -0.0621, -0.0521,  0.0002,  0.0715, -0.0399,\n        -0.0130, -0.0208,  0.0083, -0.0311, -0.0087, -0.1016, -0.0369, -0.0790,\n        -0.0100,  0.0381,  0.0317, -0.0456, -0.0048, -0.0412, -0.0188,  0.0326,\n        -0.0396, -0.0166,  0.0221, -0.0025, -0.0483,  0.0536, -0.0316, -0.0128,\n        -0.0558, -0.0223, -0.0259,  0.0443, -0.0120, -0.0010, -0.0536, -0.0182,\n        -0.0297,  0.0391, -0.0086,  0.0038,  0.0151, -0.0409,  0.0047, -0.0476,\n        -0.0055, -0.0503, -0.0337, -0.0469, -0.0090, -0.0190, -0.0881,  0.0634,\n        -0.0337, -0.0435, -0.0062, -0.0161, -0.1226,  0.0500,  0.0249, -0.0086,\n        -0.0372, -0.0043, -0.0272, -0.0317,  0.0302, -0.0200, -0.0495, -0.0537,\n        -0.0524,  0.0161, -0.0088, -0.0742, -0.0017,  0.0059,  0.0124,  0.0073,\n        -0.0140, -0.0198, -0.0017, -0.0024,  0.1261,  0.0105, -0.0409,  0.0133,\n        -0.0121,  0.0338, -0.0200, -0.0312, -0.0465, -0.0199, -0.0016, -0.0015],\n       requires_grad=True)\nlayers.4.weight\nParameter containing:\ntensor([[ 0.0122, -0.1184, -0.1141,  ...,  0.1189, -0.2547, -0.0318],\n        [-0.0936, -0.2431, -0.1109,  ..., -0.0507,  0.0135,  0.2048],\n        [ 0.1623, -0.1213,  0.0686,  ..., -0.1222,  0.0694,  0.0966],\n        ...,\n        [-0.3393,  0.1569, -0.0565,  ...,  0.0332, -0.0549, -0.0631],\n        [-0.0823, -0.3437, -0.0902,  ..., -0.1307,  0.0303,  0.0221],\n        [ 0.2725, -0.0963,  0.0260,  ...,  0.2175,  0.0584, -0.0236]],\n       requires_grad=True)\nlayers.4.bias\nParameter containing:\ntensor([ 0.0426, -0.0723, -0.0725, -0.0881, -0.0564, -0.0091, -0.0002, -0.0010,\n        -0.0355, -0.0403, -0.0707,  0.0027, -0.0275, -0.0157, -0.0504,  0.1037,\n        -0.0234,  0.0129, -0.0102, -0.0601, -0.0348, -0.0189, -0.0335, -0.0116,\n        -0.0551,  0.0005, -0.0138,  0.0023, -0.0236, -0.0128, -0.0005, -0.0623,\n        -0.0019, -0.0644, -0.0172, -0.0245, -0.0027, -0.0133, -0.0295, -0.0611,\n         0.0445, -0.0085, -0.0269, -0.0080,  0.0039, -0.0552, -0.0369, -0.0138,\n        -0.0143,  0.0706, -0.0032, -0.0017, -0.0393, -0.0306, -0.0901, -0.0318,\n         0.0152, -0.0003,  0.0759, -0.0303,  0.0011, -0.0096, -0.0037, -0.0592,\n        -0.0199,  0.0622,  0.0434, -0.0005, -0.0942,  0.0266,  0.0474, -0.0612,\n        -0.0214, -0.0009,  0.0089,  0.0167,  0.0073,  0.0087, -0.0085, -0.0078,\n         0.0429,  0.0037, -0.0038, -0.0073, -0.0044, -0.0195, -0.0324, -0.0164,\n        -0.0185, -0.0441, -0.0195, -0.0046,  0.0222, -0.0324, -0.0090,  0.0317,\n         0.0415, -0.0403,  0.0173, -0.0289,  0.0515,  0.1270,  0.0293,  0.0363,\n        -0.0014,  0.0282, -0.0133,  0.0601, -0.0392,  0.0344, -0.0012, -0.0642,\n        -0.0044,  0.0028,  0.0518, -0.0175, -0.0590, -0.0009,  0.0083, -0.0265,\n        -0.0278,  0.0051, -0.0079, -0.0235, -0.0043,  0.0667, -0.0243, -0.0216],\n       requires_grad=True)\nlayers.5.weight\nParameter containing:\ntensor([[ 0.0586, -0.1069, -0.2929,  ..., -0.1423, -0.0448,  0.0062],\n        [ 0.0485, -0.1505, -0.0913,  ..., -0.0413, -0.1587,  0.1794],\n        [-0.0946, -0.0864, -0.1717,  ...,  0.1915, -0.0083, -0.0517],\n        ...,\n        [-0.0964,  0.0973,  0.0742,  ..., -0.0730,  0.0819,  0.1351],\n        [ 0.0058,  0.1323,  0.0814,  ..., -0.0761, -0.0166,  0.1178],\n        [-0.0136,  0.0039, -0.0457,  ..., -0.1532, -0.0634, -0.3048]],\n       requires_grad=True)\nlayers.5.bias\nParameter containing:\ntensor([-7.6963e-03, -2.8884e-02,  1.0930e-02,  2.3216e-03, -1.3256e-02,\n        -2.2817e-02, -3.9400e-03, -3.0401e-02, -1.5452e-02, -2.0482e-02,\n        -3.0440e-02, -2.1184e-02,  2.2078e-02,  4.1589e-02, -1.3356e-02,\n        -6.7799e-02,  8.5047e-03, -3.2248e-02, -1.2898e-02,  2.0879e-02,\n         3.9760e-02, -1.1856e-02, -4.2163e-03,  1.4938e-02, -2.7168e-02,\n        -1.6182e-02,  7.2331e-05, -5.8499e-02, -4.7407e-03,  1.2369e-02,\n        -1.0826e-02, -2.4806e-02, -6.3338e-03,  2.9406e-02,  1.1840e-02,\n        -4.2845e-02,  1.0814e-02, -5.8909e-03,  2.1381e-02, -3.9999e-02,\n        -1.9159e-02,  2.4215e-02,  3.4502e-04, -4.4043e-02,  2.5051e-02,\n        -3.8151e-03,  5.8977e-02,  3.6523e-03,  4.7450e-03, -6.5462e-02,\n        -2.3089e-02, -1.7299e-03,  3.3700e-02, -1.3029e-02, -6.0813e-02,\n        -5.1457e-02, -5.0727e-02, -1.4613e-02, -1.7279e-04,  1.0163e-02,\n         3.6038e-02, -5.0850e-02,  3.0409e-02, -1.4574e-02, -2.4882e-02,\n         2.5816e-02, -3.4578e-02, -2.6238e-02, -2.5688e-02, -8.9868e-03,\n        -9.9544e-03, -5.4681e-04, -2.2060e-02, -2.5440e-03, -2.7208e-02,\n         3.9523e-02, -9.9956e-03, -2.3614e-03, -6.6416e-03, -4.2302e-03,\n        -1.0382e-02,  5.4526e-02,  1.2179e-02,  2.6663e-02,  4.4771e-02,\n        -7.5669e-03, -1.5294e-02,  4.0521e-02, -2.2341e-02, -3.3404e-02,\n         3.6223e-02,  1.2828e-03,  1.7307e-02, -9.9014e-03, -3.6386e-02,\n         4.3255e-02, -2.0522e-02, -2.4426e-02,  1.1382e-02,  2.7782e-03,\n        -2.1163e-02, -8.6346e-03, -2.0967e-02, -6.3001e-03,  9.5824e-02,\n        -1.0595e-02,  2.6443e-02, -1.9253e-02, -5.0586e-03, -3.0030e-02,\n        -7.7620e-03, -4.3327e-02,  8.5085e-03,  1.7705e-02, -4.5859e-03,\n         4.6504e-03, -2.6127e-03, -6.8907e-02, -6.3221e-03,  7.1178e-02,\n        -3.6719e-02, -1.4595e-02, -5.5373e-03, -3.4683e-02,  5.5018e-02,\n        -1.2717e-02, -2.3354e-02,  4.1670e-02], requires_grad=True)\nlayers.6.weight\nParameter containing:\ntensor([[ 0.2374,  0.3018,  0.2089,  ...,  0.0085, -0.1758,  0.3058],\n        [-0.0908,  0.1259,  0.0211,  ...,  0.2065, -0.0818, -0.1132],\n        [-0.0290,  0.0057, -0.0487,  ...,  0.0141, -0.1427, -0.0950],\n        ...,\n        [ 0.2852, -0.0539, -0.2384,  ..., -0.0644,  0.0216,  0.0504],\n        [ 0.0716,  0.1432, -0.0381,  ..., -0.0210, -0.0337, -0.0768],\n        [ 0.1055,  0.3935,  0.2585,  ...,  0.0981, -0.2913, -0.0435]],\n       requires_grad=True)\nlayers.6.bias\nParameter containing:\ntensor([-2.0103e-02,  3.1680e-02, -1.3974e-02, -4.7320e-03,  9.5241e-04,\n         3.1508e-02, -2.5214e-02, -3.8696e-03,  4.0969e-03, -8.2590e-04,\n         4.6954e-02,  4.0153e-02, -2.0631e-02, -3.5438e-03, -2.4322e-02,\n         2.9885e-02, -2.4222e-03,  3.5370e-02,  1.9958e-02, -1.0641e-02,\n         1.4904e-02, -8.6549e-03, -2.1531e-02,  3.9155e-02, -6.2491e-03,\n        -2.8065e-02,  1.3798e-02,  3.7939e-02,  5.9137e-03, -1.5549e-02,\n        -1.6296e-02, -4.6080e-03, -6.1889e-03, -4.7262e-02, -3.1118e-03,\n        -4.5026e-02,  3.3084e-05, -6.7530e-03, -7.1640e-04, -2.6243e-03,\n         2.7580e-02, -3.0793e-02, -3.2502e-02, -3.5742e-03, -2.8005e-03,\n         1.7876e-03, -4.6944e-02, -9.0172e-03, -1.4267e-02, -1.6520e-02,\n        -1.1986e-02, -4.1311e-02,  2.7250e-02,  9.0715e-02,  4.3062e-02,\n        -3.5724e-02, -3.1963e-02, -1.0953e-02, -1.6944e-03, -9.4898e-03,\n        -2.3658e-02, -1.7102e-02, -4.3961e-02,  2.5524e-02],\n       requires_grad=True)\nlayers.7.weight\nParameter containing:\ntensor([[-0.1289,  0.1565, -0.1536,  ..., -0.0350,  0.0495, -0.1144],\n        [-0.3525, -0.0754, -0.2928,  ...,  0.2059, -0.0556,  0.1070],\n        [-0.2596, -0.2337, -0.1527,  ...,  0.4030, -0.0068, -0.0227],\n        ...,\n        [-0.4468, -0.2614,  0.1644,  ...,  0.1303,  0.0304, -0.2904],\n        [-0.1452,  0.2760, -0.3252,  ..., -0.1691, -0.3838,  0.0760],\n        [-0.1650,  0.0611,  0.0912,  ..., -0.0554,  0.4804,  0.0627]],\n       requires_grad=True)\nlayers.7.bias\nParameter containing:\ntensor([ 0.0402, -0.0056, -0.0074, -0.0125, -0.0051, -0.0049, -0.0417, -0.0030,\n         0.0008,  0.0850,  0.0404, -0.0061, -0.0084,  0.0149, -0.0158, -0.0115,\n        -0.0048, -0.0347, -0.0012, -0.0174, -0.0240, -0.0069, -0.0046, -0.0059,\n        -0.0163, -0.0370, -0.0033,  0.0169, -0.0019, -0.0033, -0.0290, -0.0149,\n        -0.0309, -0.0511, -0.0366, -0.0069, -0.0086,  0.0284, -0.0160, -0.0078,\n         0.0580, -0.0008,  0.0496, -0.0053, -0.0539,  0.0029, -0.0143,  0.0041,\n        -0.0111,  0.0327, -0.0010, -0.0015,  0.0007, -0.0255, -0.0239,  0.0002,\n        -0.0175, -0.0197,  0.0203, -0.0246, -0.0020,  0.0295,  0.1265,  0.0044],\n       requires_grad=True)\nlayers.8.weight\nParameter containing:\ntensor([[ 0.3064,  0.0186, -0.2509,  ...,  0.1627,  0.7724, -0.5207],\n        [-0.2178, -0.1085,  0.0962,  ..., -0.1760, -0.3786, -0.0191],\n        [-0.4270, -0.2233, -0.0295,  ..., -0.5024,  0.2098,  0.1139],\n        ...,\n        [ 0.0594, -0.0542, -0.1411,  ...,  0.1626, -0.3212, -0.0479],\n        [-0.2101,  0.5022, -0.0497,  ...,  0.2858, -0.2219, -0.1853],\n        [-0.1755, -0.8413,  0.0058,  ..., -0.0817, -0.1062, -0.3446]],\n       requires_grad=True)\nlayers.8.bias\nParameter containing:\ntensor([ 0.0887, -0.0313, -0.0296,  0.0247,  0.0137, -0.0168,  0.0579,  0.0005,\n         0.0056,  0.0202, -0.0344, -0.0219, -0.0114,  0.0215,  0.0476,  0.0007,\n        -0.0313, -0.0106, -0.0037, -0.0191, -0.0148,  0.0450, -0.0117, -0.0120,\n        -0.0531, -0.0058,  0.0379,  0.0007,  0.0137, -0.0242, -0.0176, -0.0144],\n       requires_grad=True)\nlayers.9.weight\nParameter containing:\ntensor([[-7.0518e-01, -8.4674e-01, -1.3721e-01, -2.6709e-01, -5.7108e-02,\n          2.7969e-01, -4.9685e-01, -2.9314e-01, -3.1372e-02,  5.0353e-02,\n          1.2201e-01,  9.8142e-01, -5.0276e-01, -2.3347e-01, -3.1733e-02,\n         -3.1304e-01, -3.1434e-01,  1.0463e-02, -1.9507e-01,  1.9524e-02,\n          9.3348e-01,  6.4656e-01, -3.6276e-01, -4.0109e-01,  9.0772e-01,\n          3.0703e-01,  1.8397e-01,  3.1911e-01,  7.0344e-02, -2.6628e-01,\n         -3.9445e-01,  2.3046e-01],\n        [-3.6055e-01,  2.0735e-01, -2.2335e-01,  1.3121e-01,  5.8755e-01,\n         -2.8400e-01, -3.1627e-02,  1.3422e-01, -6.3675e-01,  3.7832e-01,\n          2.1556e-01,  9.8299e-02, -5.2621e-01, -1.9891e-01, -5.3341e-02,\n          5.0043e-01,  7.8079e-02,  4.1090e-02, -5.6832e-01,  6.1698e-02,\n         -1.0078e-01, -2.1939e-01,  2.1130e-01,  2.5060e-01,  4.8999e-01,\n         -6.9337e-01, -6.0292e-01, -1.6252e-01,  7.1552e-01,  1.1021e+00,\n          1.6609e-02,  4.9244e-01],\n        [-1.7093e-01, -2.1128e-01,  1.7947e-01,  4.5872e-01,  2.5208e-01,\n         -1.6871e-02,  2.7810e-01, -5.1237e-01, -1.5599e-02, -3.6496e-01,\n         -2.6743e-01, -8.9424e-02,  6.0879e-01, -8.4882e-01,  3.0517e-01,\n         -1.6724e-03,  9.7814e-01,  5.4214e-01, -7.1423e-01,  9.6919e-01,\n         -9.9978e-01, -4.6597e-01,  2.9036e-01, -1.4616e-01,  5.9616e-01,\n          2.1019e-02,  9.5749e-02, -4.4299e-01, -2.8547e-01, -1.0467e-01,\n          1.1186e+00, -5.1082e-01],\n        [ 1.8999e-01,  1.0179e+00,  1.1398e+00, -1.8196e-01, -1.8784e-02,\n          3.6117e-01, -5.9800e-01,  2.5717e-01,  2.7317e-01, -7.6783e-01,\n         -4.7591e-01,  6.9704e-01,  2.4654e-01, -6.5323e-01,  5.9424e-02,\n          1.0028e+00,  5.3338e-02, -8.4385e-01,  6.1651e-01, -2.0984e-01,\n         -1.1911e-01, -2.0444e-01,  3.6121e-01,  9.8541e-01, -2.0572e-01,\n         -5.8159e-01, -7.7157e-02, -3.8347e-01, -3.3637e-01,  4.9532e-01,\n         -3.6138e-02,  5.0848e-02],\n        [ 1.9187e-01,  6.9784e-01, -6.7305e-01, -3.8356e-01, -3.5440e-01,\n          5.1434e-01, -2.8250e-01, -4.7323e-02,  5.9460e-02, -2.8694e-01,\n          4.0479e-01, -3.8647e-01, -4.5173e-01,  4.9406e-01, -4.9303e-02,\n         -1.0532e-01, -2.9781e-01, -8.0239e-01, -3.7557e-02,  4.6973e-01,\n          2.0524e-01, -4.7460e-01,  8.2546e-01,  5.0205e-01,  1.7539e-01,\n          2.7840e-01,  3.5060e-01, -1.8795e-01,  7.2718e-02, -7.5323e-01,\n          9.0601e-01,  5.8841e-01],\n        [-3.2184e-01, -4.0720e-01,  7.1563e-01,  1.5622e-02, -3.1670e-01,\n         -3.7047e-01, -5.5618e-01, -5.9582e-01,  2.1206e-01,  1.4735e+00,\n          5.6258e-01,  2.3696e-01,  1.9161e-01, -2.2674e-01, -1.9007e-01,\n          1.6939e-01, -5.6952e-01,  5.3065e-01,  3.7425e-01,  5.3428e-01,\n         -3.5340e-01,  2.1187e-01,  6.7776e-01,  9.9363e-01,  6.1800e-01,\n         -8.6728e-02, -2.0343e-01,  7.4270e-01,  1.0532e-01,  4.5608e-01,\n          4.2121e-01,  3.4264e-01],\n        [-2.4968e-01, -7.0062e-02,  2.4427e-01, -3.7110e-01,  2.9253e-01,\n         -3.8473e-01, -1.5478e-01,  3.4596e-01,  1.1883e+00,  2.1542e-01,\n          9.4223e-01, -2.0582e-01,  5.8963e-01, -5.4831e-01,  3.8119e-01,\n          3.1418e-01, -8.2553e-02, -1.0436e-01,  1.7763e-01, -2.1503e-01,\n          1.7377e-01, -3.5627e-01,  4.9128e-01,  8.0269e-01,  7.6068e-01,\n          7.1925e-01, -9.7197e-01, -9.3249e-01, -4.9448e-01, -2.9255e-02,\n         -4.1105e-01,  1.0569e-01],\n        [-5.8108e-03, -9.0887e-01,  6.0475e-01, -5.4540e-01, -6.4991e-01,\n          2.9323e-02, -4.1143e-01,  2.4579e-01, -6.2365e-02, -6.9517e-01,\n          5.0441e-01,  7.2998e-02, -4.4919e-05, -2.3188e-01,  1.1034e-01,\n         -1.1193e+00, -2.8344e-01, -5.2485e-01,  4.7574e-01,  6.3846e-01,\n         -8.6614e-01,  4.5847e-01,  6.0553e-01, -4.9227e-01,  8.8706e-02,\n         -1.3734e-01, -2.6086e-01, -1.2334e-01, -1.1437e-01,  5.4701e-01,\n         -4.3264e-01,  3.6299e-01],\n        [-2.6860e-01, -3.8695e-02, -3.5422e-01,  7.9569e-01,  4.3192e-01,\n          6.3628e-01,  8.1199e-01, -8.5605e-01,  6.9982e-01,  8.4198e-03,\n          1.6367e-01, -1.2497e-01,  4.7257e-01,  7.1638e-03,  5.8102e-01,\n          1.4936e-01, -1.7217e-01,  3.8544e-01,  2.8886e-01,  5.0243e-02,\n         -2.5548e-01,  9.6883e-01, -2.3678e-01, -1.0691e-01, -5.3232e-01,\n         -1.7546e-01, -2.1772e-01,  6.0266e-01,  4.5225e-01,  1.6956e-01,\n          3.4479e-02, -1.8708e-01],\n        [ 1.2463e+00, -6.3717e-01, -6.6856e-01,  4.3267e-01, -1.7070e-01,\n         -1.0048e+00,  5.0044e-01,  4.3882e-01, -5.1325e-01,  3.3911e-01,\n         -3.1509e-01, -5.3926e-01, -8.7299e-01,  3.7018e-01,  6.4159e-01,\n          3.4942e-01, -3.6760e-01, -1.0038e+00,  7.5280e-01, -2.0526e-01,\n          5.2736e-01, -4.2296e-01, -2.9083e-01, -4.4777e-01,  1.0400e-01,\n          1.9996e-01,  2.0339e-01,  3.7595e-01, -4.1836e-01, -4.1987e-01,\n         -3.6898e-01, -3.1734e-01]], requires_grad=True)\nlayers.9.bias\nParameter containing:\ntensor([-0.0046, -0.0226, -0.0097,  0.0060, -0.0201, -0.0379, -0.0358,  0.0101,\n         0.0472,  0.0675], requires_grad=True)\n\n\n\n  Cell In[145], line 12\n    return layer_grad_norms\n    ^\nSyntaxError: 'return' outside function\n\n\n\n\n\nparam.grad\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\n\na = net.named_parameters()\na\n\n&lt;generator object Module.named_parameters at 0x75664c195040&gt;\n\n\n\n\n# Initialize network\nnet = Network([784, 30, 30, 30,30,30, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 47.79%\nEpoch 1: Avg loss: 2.1635 | Test Accuracy: 47.79% | Grad Norm: 0.1322\nTest Accuracy: 48.32%\nEpoch 2: Avg loss: 1.9640 | Test Accuracy: 48.32% | Grad Norm: 0.2317\nTest Accuracy: 57.46%\nEpoch 3: Avg loss: 1.9302 | Test Accuracy: 57.46% | Grad Norm: 0.2336\nTest Accuracy: 58.27%\nEpoch 4: Avg loss: 1.8705 | Test Accuracy: 58.27% | Grad Norm: 0.2470\nTest Accuracy: 68.31%\nEpoch 5: Avg loss: 1.8217 | Test Accuracy: 68.31% | Grad Norm: 0.2289\nTest Accuracy: 76.36%\nEpoch 6: Avg loss: 1.7212 | Test Accuracy: 76.36% | Grad Norm: 0.2658\nTest Accuracy: 83.52%\nEpoch 7: Avg loss: 1.6670 | Test Accuracy: 83.52% | Grad Norm: 0.3059\nTest Accuracy: 84.94%\nEpoch 8: Avg loss: 1.6197 | Test Accuracy: 84.94% | Grad Norm: 0.2911\nTest Accuracy: 84.63%\nEpoch 9: Avg loss: 1.6113 | Test Accuracy: 84.63% | Grad Norm: 0.2488\nTest Accuracy: 85.33%\nEpoch 10: Avg loss: 1.6070 | Test Accuracy: 85.33% | Grad Norm: 0.2324\nTest Accuracy: 85.92%\nEpoch 11: Avg loss: 1.6041 | Test Accuracy: 85.92% | Grad Norm: 0.2264\nTest Accuracy: 85.55%\nEpoch 12: Avg loss: 1.6022 | Test Accuracy: 85.55% | Grad Norm: 0.2141\nTest Accuracy: 86.11%\nEpoch 13: Avg loss: 1.6009 | Test Accuracy: 86.11% | Grad Norm: 0.2255\nTest Accuracy: 86.12%\nEpoch 14: Avg loss: 1.5981 | Test Accuracy: 86.12% | Grad Norm: 0.2018\nTest Accuracy: 86.03%\nEpoch 15: Avg loss: 1.5980 | Test Accuracy: 86.03% | Grad Norm: 0.1845\nTest Accuracy: 86.24%\nEpoch 16: Avg loss: 1.5961 | Test Accuracy: 86.24% | Grad Norm: 0.1862\nTest Accuracy: 86.31%\nEpoch 17: Avg loss: 1.5948 | Test Accuracy: 86.31% | Grad Norm: 0.1796\nTest Accuracy: 86.64%\nEpoch 18: Avg loss: 1.5945 | Test Accuracy: 86.64% | Grad Norm: 0.1815\nTest Accuracy: 86.47%\nEpoch 19: Avg loss: 1.5938 | Test Accuracy: 86.47% | Grad Norm: 0.1660\nTest Accuracy: 86.61%\nEpoch 20: Avg loss: 1.5936 | Test Accuracy: 86.61% | Grad Norm: 0.1643\nTest Accuracy: 86.40%\nEpoch 21: Avg loss: 1.5931 | Test Accuracy: 86.40% | Grad Norm: 0.1504\nTest Accuracy: 86.31%\nEpoch 22: Avg loss: 1.5918 | Test Accuracy: 86.31% | Grad Norm: 0.1649\nTest Accuracy: 86.03%\nEpoch 23: Avg loss: 1.5914 | Test Accuracy: 86.03% | Grad Norm: 0.1508\nTest Accuracy: 86.47%\nEpoch 24: Avg loss: 1.5909 | Test Accuracy: 86.47% | Grad Norm: 0.1658\nTest Accuracy: 86.51%\nEpoch 25: Avg loss: 1.5900 | Test Accuracy: 86.51% | Grad Norm: 0.1498\nTest Accuracy: 86.52%\nEpoch 26: Avg loss: 1.5903 | Test Accuracy: 86.52% | Grad Norm: 0.1456\nTest Accuracy: 86.66%\nEpoch 27: Avg loss: 1.5893 | Test Accuracy: 86.66% | Grad Norm: 0.1448\nTest Accuracy: 86.76%\nEpoch 28: Avg loss: 1.5897 | Test Accuracy: 86.76% | Grad Norm: 0.1537\nTest Accuracy: 86.80%\nEpoch 29: Avg loss: 1.5892 | Test Accuracy: 86.80% | Grad Norm: 0.1450\nTest Accuracy: 86.98%\nEpoch 30: Avg loss: 1.5887 | Test Accuracy: 86.98% | Grad Norm: 0.1464\n\n\n([2.1635325968464216,\n  1.9639694020350773,\n  1.930236688196659,\n  1.8705140953063966,\n  1.8216849041382472,\n  1.7211663377483686,\n  1.6670003309647243,\n  1.6197138274709384,\n  1.6112653411030768,\n  1.6070367030700048,\n  1.6041051485737166,\n  1.602215163707733,\n  1.6009200358390807,\n  1.5981467183828353,\n  1.5980393146475156,\n  1.5960813767711322,\n  1.5947924272418021,\n  1.594452277759711,\n  1.5938180121382077,\n  1.5935831698179246,\n  1.5930660124818483,\n  1.5918122189442316,\n  1.5913974216779072,\n  1.5909487432638805,\n  1.5900184524655343,\n  1.5902962886889775,\n  1.5893317769964537,\n  1.5897210414409637,\n  1.5891797138055166,\n  1.5887350454131761],\n [47.79,\n  48.32,\n  57.46,\n  58.27,\n  68.31,\n  76.36,\n  83.52,\n  84.94,\n  84.63,\n  85.33,\n  85.92,\n  85.55,\n  86.11,\n  86.12,\n  86.03,\n  86.24,\n  86.31,\n  86.64,\n  86.47,\n  86.61,\n  86.4,\n  86.31,\n  86.03,\n  86.47,\n  86.51,\n  86.52,\n  86.66,\n  86.76,\n  86.8,\n  86.98],\n [0.13219451629541193,\n  0.23167567761579994,\n  0.23364859538727129,\n  0.24697093838391204,\n  0.2289444550202849,\n  0.26577912395540626,\n  0.30587334593512544,\n  0.29106171892368005,\n  0.24875650966949373,\n  0.2323957467955285,\n  0.226352352639214,\n  0.21414800924973074,\n  0.22548922616963196,\n  0.201752421304991,\n  0.18447010494991628,\n  0.18617558985285965,\n  0.17956458004325457,\n  0.18147292234794501,\n  0.16598685060630122,\n  0.16427449332463492,\n  0.1503524859182171,\n  0.16491795721898447,\n  0.1507572576033238,\n  0.16576083033297617,\n  0.1498191339385964,\n  0.14562260107260075,\n  0.14482543913282098,\n  0.15368519634113514,\n  0.14500450822757846,\n  0.14637346995560954])\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 0.0009341098484583199),\n ('layers.0.bias', 0.00017792556900531054),\n ('layers.1.weight', 0.0007935031317174435),\n ('layers.1.bias', 0.000190752514754422),\n ('layers.2.weight', 0.0005657682777382433),\n ('layers.2.bias', 0.00015252410958055407),\n ('layers.3.weight', 0.0004857115854974836),\n ('layers.3.bias', 0.00013925063831266016),\n ('layers.4.weight', 0.0003901872260030359),\n ('layers.4.bias', 0.00010040731285698712),\n ('layers.5.weight', 0.0003839585406240076),\n ('layers.5.bias', 9.007887274492532e-05),\n ('layers.6.weight', 0.0003126118390355259),\n ('layers.6.bias', 6.15888275206089e-05),\n ('layers.7.weight', 0.0003803927975241095),\n ('layers.7.bias', 5.366472396417521e-05),\n ('layers.8.weight', 0.0003394497325643897),\n ('layers.8.bias', 3.8529717130586505e-05),\n ('layers.9.weight', 0.00031092570861801505),\n ('layers.9.bias', 2.5548297344357707e-05)]\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 1.4873625332256779e-05),\n ('layers.0.bias', 1.4130242789178737e-06),\n ('layers.1.weight', 1.051520030159736e-05),\n ('layers.1.bias', 9.971977306122426e-07),\n ('layers.2.weight', 7.035768703644862e-06),\n ('layers.2.bias', 6.079347940612934e-07),\n ('layers.3.weight', 6.393755484168651e-06),\n ('layers.3.bias', 4.279268637219502e-07),\n ('layers.4.weight', 4.831199021282373e-06),\n ('layers.4.bias', 2.65985761416232e-07),\n ('layers.5.weight', 5.014670932723675e-06),\n ('layers.5.bias', 2.1609190525850863e-07),\n ('layers.6.weight', 3.7912045627308544e-06),\n ('layers.6.bias', 1.3680858046427602e-07),\n ('layers.7.weight', 3.3192300179507583e-06),\n ('layers.7.bias', 1.2003184224340657e-07),\n ('layers.8.weight', 2.092480144710862e-06),\n ('layers.8.bias', 8.711202070799118e-08),\n ('layers.9.weight', 1.6500765696036979e-06),\n ('layers.9.bias', 6.317808498579325e-08)]\n\n\n\n\n# Initialize network\nnet = Network([784, 30, 30, 30,30,30, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 49.17%\nEpoch 1: Avg loss: 2.1066 | Test Accuracy: 49.17% | Grad Norm: 0.1545\nTest Accuracy: 76.22%\nEpoch 2: Avg loss: 1.8757 | Test Accuracy: 76.22% | Grad Norm: 0.2665\nTest Accuracy: 82.14%\nEpoch 3: Avg loss: 1.6619 | Test Accuracy: 82.14% | Grad Norm: 0.4257\nTest Accuracy: 83.09%\nEpoch 4: Avg loss: 1.6347 | Test Accuracy: 83.09% | Grad Norm: 0.3923\nTest Accuracy: 82.23%\nEpoch 5: Avg loss: 1.6267 | Test Accuracy: 82.23% | Grad Norm: 0.3576\nTest Accuracy: 83.28%\nEpoch 6: Avg loss: 1.6234 | Test Accuracy: 83.28% | Grad Norm: 0.3438\nTest Accuracy: 90.66%\nEpoch 7: Avg loss: 1.5791 | Test Accuracy: 90.66% | Grad Norm: 0.5007\nTest Accuracy: 90.69%\nEpoch 8: Avg loss: 1.5519 | Test Accuracy: 90.69% | Grad Norm: 0.4530\nTest Accuracy: 91.90%\nEpoch 9: Avg loss: 1.5448 | Test Accuracy: 91.90% | Grad Norm: 0.3948\nTest Accuracy: 91.57%\nEpoch 10: Avg loss: 1.5409 | Test Accuracy: 91.57% | Grad Norm: 0.3736\nTest Accuracy: 91.62%\nEpoch 11: Avg loss: 1.5375 | Test Accuracy: 91.62% | Grad Norm: 0.3566\nTest Accuracy: 92.43%\nEpoch 12: Avg loss: 1.5346 | Test Accuracy: 92.43% | Grad Norm: 0.3450\nTest Accuracy: 92.06%\nEpoch 13: Avg loss: 1.5316 | Test Accuracy: 92.06% | Grad Norm: 0.3280\nTest Accuracy: 92.81%\nEpoch 14: Avg loss: 1.5301 | Test Accuracy: 92.81% | Grad Norm: 0.3218\nTest Accuracy: 92.70%\nEpoch 15: Avg loss: 1.5294 | Test Accuracy: 92.70% | Grad Norm: 0.3111\nTest Accuracy: 93.04%\nEpoch 16: Avg loss: 1.5256 | Test Accuracy: 93.04% | Grad Norm: 0.2890\nTest Accuracy: 92.72%\nEpoch 17: Avg loss: 1.5263 | Test Accuracy: 92.72% | Grad Norm: 0.2699\nTest Accuracy: 91.79%\nEpoch 18: Avg loss: 1.5240 | Test Accuracy: 91.79% | Grad Norm: 0.2641\nTest Accuracy: 92.99%\nEpoch 19: Avg loss: 1.5234 | Test Accuracy: 92.99% | Grad Norm: 0.2644\nTest Accuracy: 93.35%\nEpoch 20: Avg loss: 1.5215 | Test Accuracy: 93.35% | Grad Norm: 0.2663\nTest Accuracy: 93.21%\nEpoch 21: Avg loss: 1.5198 | Test Accuracy: 93.21% | Grad Norm: 0.2443\nTest Accuracy: 93.46%\nEpoch 22: Avg loss: 1.5203 | Test Accuracy: 93.46% | Grad Norm: 0.2640\nTest Accuracy: 93.24%\nEpoch 23: Avg loss: 1.5185 | Test Accuracy: 93.24% | Grad Norm: 0.2449\nTest Accuracy: 93.71%\nEpoch 24: Avg loss: 1.5182 | Test Accuracy: 93.71% | Grad Norm: 0.2318\nTest Accuracy: 93.58%\nEpoch 25: Avg loss: 1.5166 | Test Accuracy: 93.58% | Grad Norm: 0.2341\nTest Accuracy: 93.55%\nEpoch 26: Avg loss: 1.5141 | Test Accuracy: 93.55% | Grad Norm: 0.2143\nTest Accuracy: 93.83%\nEpoch 27: Avg loss: 1.5148 | Test Accuracy: 93.83% | Grad Norm: 0.2336\nTest Accuracy: 94.05%\nEpoch 28: Avg loss: 1.5141 | Test Accuracy: 94.05% | Grad Norm: 0.2333\nTest Accuracy: 94.07%\nEpoch 29: Avg loss: 1.5122 | Test Accuracy: 94.07% | Grad Norm: 0.2159\nTest Accuracy: 93.86%\nEpoch 30: Avg loss: 1.5113 | Test Accuracy: 93.86% | Grad Norm: 0.2133\n\n\n([2.106609010855357,\n  1.8757023508548736,\n  1.6619241003990173,\n  1.6347059144179026,\n  1.6267493901252748,\n  1.6233574739098549,\n  1.5790614494681359,\n  1.5519095999995867,\n  1.544799600203832,\n  1.5408548574646315,\n  1.5375095768968263,\n  1.5346180646220844,\n  1.5316300688385964,\n  1.530076232691606,\n  1.5293561617334683,\n  1.5256369227170945,\n  1.5263271143436432,\n  1.5240207088192304,\n  1.5233585479458174,\n  1.5214697729150455,\n  1.5198356384634972,\n  1.5202613272070884,\n  1.5184814278682073,\n  1.5181590902805329,\n  1.5165831561485927,\n  1.5140590472221374,\n  1.5147702373464902,\n  1.5141058943669001,\n  1.5122183973987897,\n  1.511292640129725],\n [49.17,\n  76.22,\n  82.14,\n  83.09,\n  82.23,\n  83.28,\n  90.66,\n  90.69,\n  91.9,\n  91.57,\n  91.62,\n  92.43,\n  92.06,\n  92.81,\n  92.7,\n  93.04,\n  92.72,\n  91.79,\n  92.99,\n  93.35,\n  93.21,\n  93.46,\n  93.24,\n  93.71,\n  93.58,\n  93.55,\n  93.83,\n  94.05,\n  94.07,\n  93.86],\n [0.1545203353925608,\n  0.26648143015344006,\n  0.4256724650240503,\n  0.39230799699549485,\n  0.3575932351596421,\n  0.3438080019064364,\n  0.5007331380928711,\n  0.45298709018271377,\n  0.3948249351742221,\n  0.3736201974570067,\n  0.3565940285629549,\n  0.34503243407932555,\n  0.3279822464524295,\n  0.321849082351473,\n  0.31105692698469406,\n  0.2889891259993431,\n  0.2698816749692742,\n  0.26406108346089663,\n  0.26441012255252766,\n  0.2662807202279061,\n  0.244252311765028,\n  0.26402584721809275,\n  0.24487796221406294,\n  0.23184617290994053,\n  0.23406889448768592,\n  0.21428815829061432,\n  0.23363848274583932,\n  0.23327912711403648,\n  0.21594139513708782,\n  0.2133217869038012])\n\n\n\ntrain(net, train_loader, epochs=3, mini_batch_size=10, eta=.0001, test_data=test_loader)\n\nTest Accuracy: 94.18%\nEpoch 1: Avg loss: 1.5050 | Test Accuracy: 94.18% | Grad Norm: 0.1529\nTest Accuracy: 94.13%\nEpoch 2: Avg loss: 1.5032 | Test Accuracy: 94.13% | Grad Norm: 0.1523\nTest Accuracy: 94.31%\nEpoch 3: Avg loss: 1.5021 | Test Accuracy: 94.31% | Grad Norm: 0.1398\n\n\n([1.5050347511370976, 1.5031935001413028, 1.5020705436468125],\n [94.18, 94.13, 94.31],\n [0.15294206734796406, 0.15231098094013806, 0.13976386858030038])\n\n\n\ntrain(net, train_loader, epochs=, mini_batch_size=10, eta=.00005, test_data=test_loader)\n\n\n# Initialize network\nnet = Network([784, 16, 16, 16,16,16, 16,16,16,16, 10])\n\n# Train\ntrain(net, train_loader,epochs=10, mini_batch_size=10, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 65.97%\nEpoch 1: Avg loss: 1.9103 | Test Accuracy: 65.97% | Grad Norm: 2.1618\nTest Accuracy: 82.17%\nEpoch 2: Avg loss: 1.7605 | Test Accuracy: 82.17% | Grad Norm: 1.9723\nTest Accuracy: 72.04%\nEpoch 3: Avg loss: 1.6924 | Test Accuracy: 72.04% | Grad Norm: 1.4801\nTest Accuracy: 73.19%\nEpoch 4: Avg loss: 1.7242 | Test Accuracy: 73.19% | Grad Norm: 1.1159\nTest Accuracy: 65.95%\nEpoch 5: Avg loss: 1.7383 | Test Accuracy: 65.95% | Grad Norm: 0.9115\nTest Accuracy: 64.79%\nEpoch 6: Avg loss: 1.7245 | Test Accuracy: 64.79% | Grad Norm: 0.7612\nTest Accuracy: 74.75%\nEpoch 7: Avg loss: 1.7472 | Test Accuracy: 74.75% | Grad Norm: 0.6778\nTest Accuracy: 74.83%\nEpoch 8: Avg loss: 1.7212 | Test Accuracy: 74.83% | Grad Norm: 0.6663\nTest Accuracy: 84.29%\nEpoch 9: Avg loss: 1.6780 | Test Accuracy: 84.29% | Grad Norm: 0.8242\nTest Accuracy: 73.18%\nEpoch 10: Avg loss: 1.6328 | Test Accuracy: 73.18% | Grad Norm: 0.7515\n\n\n([1.910252003490925,\n  1.7605421397884686,\n  1.6924408531387647,\n  1.7241778714855511,\n  1.7382639647126197,\n  1.7244922712047894,\n  1.747195117255052,\n  1.7212321440180143,\n  1.6779569004774093,\n  1.6327920169035595],\n [65.97, 82.17, 72.04, 73.19, 65.95, 64.79, 74.75, 74.83, 84.29, 73.18],\n [2.1617651940725433,\n  1.9723268485029335,\n  1.4801335709182744,\n  1.1159431979126584,\n  0.9115443883962566,\n  0.761165994037466,\n  0.6777709146138219,\n  0.666324961432839,\n  0.8241914687594972,\n  0.7515233792834197])\n\n\n\nF.relu\n\n&lt;function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -&gt; torch.Tensor&gt;\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=10, eta=0.0001, test_data=test_loader)\n\nTest Accuracy: 85.02%\nEpoch 1: Avg loss: 1.6808 | Test Accuracy: 85.02% | Grad Norm: 0.5848\nTest Accuracy: 85.27%\nEpoch 2: Avg loss: 1.6110 | Test Accuracy: 85.27% | Grad Norm: 0.5258\nTest Accuracy: 85.48%\nEpoch 3: Avg loss: 1.6093 | Test Accuracy: 85.48% | Grad Norm: 0.5570\nTest Accuracy: 88.00%\nEpoch 4: Avg loss: 1.5914 | Test Accuracy: 88.00% | Grad Norm: 1.5017\nTest Accuracy: 88.52%\nEpoch 5: Avg loss: 1.5793 | Test Accuracy: 88.52% | Grad Norm: 1.2540\nTest Accuracy: 89.07%\nEpoch 6: Avg loss: 1.5742 | Test Accuracy: 89.07% | Grad Norm: 1.1259\nTest Accuracy: 89.11%\nEpoch 7: Avg loss: 1.5696 | Test Accuracy: 89.11% | Grad Norm: 1.1550\nTest Accuracy: 89.61%\nEpoch 8: Avg loss: 1.5660 | Test Accuracy: 89.61% | Grad Norm: 1.1759\nTest Accuracy: 89.94%\nEpoch 9: Avg loss: 1.5631 | Test Accuracy: 89.94% | Grad Norm: 1.0710\nTest Accuracy: 90.16%\nEpoch 10: Avg loss: 1.5595 | Test Accuracy: 90.16% | Grad Norm: 1.0280\n\n\n([1.6808435128529866,\n  1.6109866456190745,\n  1.6093079151511192,\n  1.5913694785237313,\n  1.5792830767432848,\n  1.5742242393096288,\n  1.5695668534239133,\n  1.5659935484925906,\n  1.5630642044941585,\n  1.559465788523356],\n [85.02, 85.27, 85.48, 88.0, 88.52, 89.07, 89.11, 89.61, 89.94, 90.16],\n [0.5847676788903353,\n  0.5258062318293325,\n  0.5569941287694299,\n  1.501659070342197,\n  1.2540426928454615,\n  1.1258879237975783,\n  1.155011085806159,\n  1.1759249319730467,\n  1.0709939167230804,\n  1.0279926092156686])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=10, eta=0.00005, test_data=test_loader)\n\nTest Accuracy: 90.29%\nEpoch 1: Avg loss: 1.5576 | Test Accuracy: 90.29% | Grad Norm: 1.1653\nTest Accuracy: 90.37%\nEpoch 2: Avg loss: 1.5557 | Test Accuracy: 90.37% | Grad Norm: 1.0235\nTest Accuracy: 90.46%\nEpoch 3: Avg loss: 1.5549 | Test Accuracy: 90.46% | Grad Norm: 0.8970\nTest Accuracy: 90.42%\nEpoch 4: Avg loss: 1.5545 | Test Accuracy: 90.42% | Grad Norm: 1.0247\nTest Accuracy: 90.72%\nEpoch 5: Avg loss: 1.5531 | Test Accuracy: 90.72% | Grad Norm: 1.0628\nTest Accuracy: 90.88%\nEpoch 6: Avg loss: 1.5511 | Test Accuracy: 90.88% | Grad Norm: 0.9794\nTest Accuracy: 90.90%\nEpoch 7: Avg loss: 1.5503 | Test Accuracy: 90.90% | Grad Norm: 0.9258\nTest Accuracy: 90.93%\nEpoch 8: Avg loss: 1.5495 | Test Accuracy: 90.93% | Grad Norm: 1.0355\nTest Accuracy: 91.01%\nEpoch 9: Avg loss: 1.5490 | Test Accuracy: 91.01% | Grad Norm: 0.9989\nTest Accuracy: 91.06%\nEpoch 10: Avg loss: 1.5484 | Test Accuracy: 91.06% | Grad Norm: 0.8950\n\n\n([1.5576463149785995,\n  1.5557187402844428,\n  1.5549181991020837,\n  1.5544501681526501,\n  1.5530901376008988,\n  1.5511410237153371,\n  1.5503380763729413,\n  1.5494966670076051,\n  1.5490306969483694,\n  1.5483644265333811],\n [90.29, 90.37, 90.46, 90.42, 90.72, 90.88, 90.9, 90.93, 91.01, 91.06],\n [1.1653370030086032,\n  1.0234840901001925,\n  0.8970488051590173,\n  1.0247076750978872,\n  1.0628498398400164,\n  0.9794491940609777,\n  0.9257728102990177,\n  1.0355127711586682,\n  0.9988974580161624,\n  0.8950070320304887])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=30, eta=0.00005, test_data=test_loader)\n\nTest Accuracy: 91.10%\nEpoch 1: Avg loss: 1.5476 | Test Accuracy: 91.10% | Grad Norm: 0.8429\nTest Accuracy: 91.20%\nEpoch 2: Avg loss: 1.5472 | Test Accuracy: 91.20% | Grad Norm: 0.7848\nTest Accuracy: 91.24%\nEpoch 3: Avg loss: 1.5463 | Test Accuracy: 91.24% | Grad Norm: 0.9697\nTest Accuracy: 91.33%\nEpoch 4: Avg loss: 1.5453 | Test Accuracy: 91.33% | Grad Norm: 0.8874\nTest Accuracy: 91.39%\nEpoch 5: Avg loss: 1.5446 | Test Accuracy: 91.39% | Grad Norm: 1.0315\nTest Accuracy: 91.49%\nEpoch 6: Avg loss: 1.5441 | Test Accuracy: 91.49% | Grad Norm: 0.8475\nTest Accuracy: 91.49%\nEpoch 7: Avg loss: 1.5437 | Test Accuracy: 91.49% | Grad Norm: 0.8058\nTest Accuracy: 91.52%\nEpoch 8: Avg loss: 1.5442 | Test Accuracy: 91.52% | Grad Norm: 0.8884\nTest Accuracy: 91.49%\nEpoch 9: Avg loss: 1.5430 | Test Accuracy: 91.49% | Grad Norm: 0.8100\nTest Accuracy: 91.58%\nEpoch 10: Avg loss: 1.5420 | Test Accuracy: 91.58% | Grad Norm: 0.7816\n\n\n([1.5476057616670926,\n  1.5471696167389553,\n  1.5463465830485026,\n  1.5452923675576846,\n  1.5446079933842023,\n  1.54406529823939,\n  1.5436700760324795,\n  1.5442081407904624,\n  1.543009335021178,\n  1.542019239405791],\n [91.1, 91.2, 91.24, 91.33, 91.39, 91.49, 91.49, 91.52, 91.49, 91.58],\n [0.8428987947043185,\n  0.7847833687839821,\n  0.9697343087016209,\n  0.8873604827569046,\n  1.0314763956363955,\n  0.8475186883702619,\n  0.8057847412439724,\n  0.8884064122035553,\n  0.8100050882316191,\n  0.7816337433262337])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=30, eta=0.00002, test_data=test_loader)\n\nTest Accuracy: 91.65%\nEpoch 1: Avg loss: 1.5409 | Test Accuracy: 91.65% | Grad Norm: 0.8363\nTest Accuracy: 91.67%\nEpoch 2: Avg loss: 1.5406 | Test Accuracy: 91.67% | Grad Norm: 0.6521\nTest Accuracy: 91.60%\nEpoch 3: Avg loss: 1.5404 | Test Accuracy: 91.60% | Grad Norm: 0.7502\nTest Accuracy: 91.67%\nEpoch 4: Avg loss: 1.5399 | Test Accuracy: 91.67% | Grad Norm: 0.7862\nTest Accuracy: 91.72%\nEpoch 5: Avg loss: 1.5399 | Test Accuracy: 91.72% | Grad Norm: 0.5888\nTest Accuracy: 91.76%\nEpoch 6: Avg loss: 1.5393 | Test Accuracy: 91.76% | Grad Norm: 0.8160\nTest Accuracy: 91.75%\nEpoch 7: Avg loss: 1.5393 | Test Accuracy: 91.75% | Grad Norm: 0.6232\nTest Accuracy: 91.75%\nEpoch 8: Avg loss: 1.5391 | Test Accuracy: 91.75% | Grad Norm: 0.7121\nTest Accuracy: 91.72%\nEpoch 9: Avg loss: 1.5389 | Test Accuracy: 91.72% | Grad Norm: 0.6047\nTest Accuracy: 91.77%\nEpoch 10: Avg loss: 1.5387 | Test Accuracy: 91.77% | Grad Norm: 0.6256\n\n\n([1.5408721971710524,\n  1.5405561513702075,\n  1.5404061170220376,\n  1.539928627272447,\n  1.5399407841563224,\n  1.539325503985087,\n  1.539281131406625,\n  1.5390899473627409,\n  1.5388786518971125,\n  1.5387439164320629],\n [91.65, 91.67, 91.6, 91.67, 91.72, 91.76, 91.75, 91.75, 91.72, 91.77],\n [0.8363055294336015,\n  0.6521481247455782,\n  0.7501852165585575,\n  0.7862274248023394,\n  0.588820192058458,\n  0.8159551650568143,\n  0.6232069042345509,\n  0.7121345371288179,\n  0.6046759387407957,\n  0.6256342942342423])\n\n\n\nF.StepLR\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[46], line 1\n----&gt; 1 F.StepLR\n\nAttributeError: module 'torch.nn.functional' has no attribute 'StepLR'"
  },
  {
    "objectID": "meetups/Meetup-13/neural-networks-and-deep-learning-master/GradientDescentExperimentation.html",
    "href": "meetups/Meetup-13/neural-networks-and-deep-learning-master/GradientDescentExperimentation.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "#This cell sets up basic plotting functions awe\n#we will use to visualize the gradient descent routines.\n\n#Make plots interactive\n#%matplotlib notebook\n\n#Make plots static\n%matplotlib inline\n\n#Make 3D plots\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n#from matplotlib import animation\nfrom IPython.display import HTML\nfrom matplotlib.colors import LogNorm\n#from itertools import zip_longest\n\n#Import Numpy\nimport numpy as np\n\n#Define function for plotting \n\ndef plot_surface(x, y, z, azim=-60, elev=40, dist=10, cmap=\"RdYlBu_r\"):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    plot_args = {'rstride': 1, 'cstride': 1, 'cmap':cmap,\n             'linewidth': 20, 'antialiased': True,\n             'vmin': -2, 'vmax': 2}\n    ax.plot_surface(x, y, z, **plot_args)\n    ax.view_init(azim=azim, elev=elev)\n    ax.dist=dist\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-2, 2)\n    \n    plt.xticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n    plt.yticks([-1, -0.5, 0, 0.5, 1], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n    ax.set_zticks([-2, -1, 0, 1, 2])\n    ax.set_zticklabels([\"-2\", \"-1\", \"0\", \"1\", \"2\"])\n    \n    ax.set_xlabel(\"x\", fontsize=18)\n    ax.set_ylabel(\"y\", fontsize=18)\n    ax.set_zlabel(\"z\", fontsize=18)\n    return fig, ax;\n\ndef overlay_trajectory_quiver(ax,obj_func,trajectory, color='k'):\n    xs=trajectory[:,0]\n    ys=trajectory[:,1]\n    zs=obj_func(xs,ys)\n    ax.quiver(xs[:-1], ys[:-1], zs[:-1], xs[1:]-xs[:-1], ys[1:]-ys[:-1],zs[1:]-zs[:-1],color=color,arrow_length_ratio=0.3)\n    \n    return ax;\n\ndef overlay_trajectory(ax,obj_func,trajectory,label,color='k'):\n    xs=trajectory[:,0]\n    ys=trajectory[:,1]\n    zs=obj_func(xs,ys)\n    ax.plot(xs,ys,zs, color, label=label)\n    \n    return ax;\n\n    \ndef overlay_trajectory_contour_M(ax,trajectory, label,color='k',lw=2):\n    xs=trajectory[:,0]\n    ys=trajectory[:,1]\n    ax.plot(xs,ys, color, label=label,lw=lw)\n    ax.plot(xs[-1],ys[-1],color+'&gt;', markersize=14)\n    return ax;\n\ndef overlay_trajectory_contour(ax,trajectory, label,color='k',lw=2):\n    xs=trajectory[:,0]\n    ys=trajectory[:,1]\n    ax.plot(xs,ys, color, label=label,lw=lw)\n    return ax;\n\n\n#DEFINE SURFACES WE WILL WORK WITH\n\n#Define monkey saddle and gradient\n\ndef saddle(x,y):\n    return x**2 - y**2\n\ndef grad_saddle(params):\n    x=params[0]\n    y=params[1]\n    grad_x= 2*x\n    grad_y= -2*y\n    return [grad_x,grad_y]\n\ndef monkey_saddle(x,y):\n    return x**3 - 3*x*y**2\n\ndef grad_monkey_saddle(params):\n    x=params[0]\n    y=params[1]\n    grad_x= 3*x**2-3*y**2\n    grad_y= -6*x*y\n    return [grad_x,grad_y]\n\n#Define saddle surface\n\ndef saddle_surface(x,y,a=1,b=1):\n    return a*x**2-b*y**2\n\ndef grad_saddle_surface(params,a=1,b=1):\n    x=params[0]\n    y=params[1]\n    grad_x= a*x\n    grad_y= -1*b*y\n    return [grad_x,grad_y]\n\n\n# Define minima_surface\n\ndef minima_surface(x,y,a=1,b=1):\n    return a*x**2+b*y**2-1\n\ndef grad_minima_surface(params,a=1,b=1):\n    x=params[0]\n    y=params[1]\n    grad_x= 2*a*x\n    grad_y= 2*b*y\n    return [grad_x,grad_y]\n\n\ndef beales_function(x,y):\n    return np.square(1.5-x+x*y)+np.square(2.25-x+x*y*y)+np.square(2.625-x+x*y**3)\n    return f\n\ndef grad_beales_function(params):\n    x=params[0]\n    y=params[1]\n    grad_x=2*(1.5-x+x*y)*(-1+y)+2*(2.25-x+x*y**2)*(-1+y**2)+2*(2.625-x+x*y**3)*(-1+y**3)\n    grad_y=2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*x*y+6*(2.625-x+x*y**3)*x*y**2\n    return [grad_x,grad_y]\n\n\n\n\ndef contour_beales_function():\n    #plot beales function\n    x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n    fig, ax = plt.subplots(figsize=(10, 6))\n    z=beales_function(x,y)\n    cax = ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n    ax.plot(3,0.5, 'r*', markersize=18)\n\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n\n    ax.set_xlim((-4.5, 4.5))\n    ax.set_ylim((-4.5, 4.5))\n    \n    return fig,ax\n\ndef himmel(x,y):\n    return( (x**2+y-11)**2 + (x+y**2-7)**2)\n\n\ndef grad_himmel(params):\n    x = params[0]\n    y = params[1]\n\n    grad_x = 4*x*(x**2+y-11) + 2*(x+y**2-7)\n    grad_y = 2*(x**2+y-11) + 4*y*(x+y**2-7)\n    return([grad_x,grad_y])\n\n\n\n\n\ndef rosenbrock(x,y):\n    a = 1\n    b = 100\n    return((a-x)**2 + b*(y-x**2)**2)\n\ndef grad_rosenbrock(params):\n    a = 1\n    b = 100\n    x=params[0]\n    y=params[1]\n    grad_x= -2*(a-x) - 4*b*(y-x**2)*x\n    grad_y= 2*b*(y-x**2)\n    return [grad_x,grad_y]\n    \ndef contour_rosenbrock():\n    x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n    fig, ax = plt.subplots(figsize=(10, 6))\n    z=rosenbrock(x,y)\n    cax = ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n    ax.plot(1,1, 'r*', markersize=18)\n\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n\n    ax.set_xlim((-4.5, 4.5))\n    ax.set_ylim((-4.5, 4.5))\n    \n    return fig,ax\n\ndef contour_himmel():\n    x, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\n    fig, ax = plt.subplots(figsize=(10, 6))\n    z=himmel(x,y)\n    cax = ax.contour(x, y, z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n\n    ax.set_xlim((-4.5, 4.5))\n    ax.set_ylim((-4.5, 4.5))\n    \n    return fig,ax\n\n#Make plots of surfaces\nplt.close() # closes previous plots\nx, y = np.mgrid[-1:1:31j, -1:1:31j]\nfig1,ax1=plot_surface(x,y,monkey_saddle(x,y))\nfig2,ax2=plot_surface(x,y,saddle_surface(x,y))\nfig3,ax3=plot_surface(x,y,minima_surface(x,y,5),0)\n\n#Contour plot of Beale's Function\n\nfig4,ax4 =contour_beales_function()\nplt.show()\n\ndef contour_saddle():\n    x, y = np.meshgrid(np.arange(-2.5, 2.5, 0.2), np.arange(-2.5, 2.5, 0.2))\n    fig, ax = plt.subplots(figsize=(10, 6))\n    z= saddle(x,y)\n    cax = ax.contour(x, y, z,levels=np.linspace(-10,10,35), norm=LogNorm(), cmap=\"RdYlBu_r\")\n\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n\n    ax.set_xlim((-2.5, 2.5))\n    ax.set_ylim((-2.5, 2.5))\n    \n    return fig,ax\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#This writes a simple gradient descent, gradient descent+ momentum,\n#nesterov. \n\n#Mean-gradient based methods\ndef gd(grad, init, n_epochs=1000, eta=10**-4, noise_strength=0):\n    #This is a simple optimizer\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init\n    v=0;\n    for j in range(n_epochs):\n        noise=noise_strength*np.random.randn(params.size)\n        v=eta*(np.array(grad(params))+noise)\n        params=params-v\n        param_traj[j+1,]=params\n    return param_traj\n\n\ndef gd_with_mom(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9,noise_strength=0):\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init\n    v=0\n    for j in range(n_epochs):\n        noise=noise_strength*np.random.randn(params.size)\n        v=gamma*v+eta*(np.array(grad(params))+noise)\n        params=params-v\n        param_traj[j+1,]=params\n    return param_traj\n\ndef NAG(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9,noise_strength=0):\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init\n    v=0\n    for j in range(n_epochs):\n        noise=noise_strength*np.random.randn(params.size)\n        params_nesterov=params-gamma*v\n        v=gamma*v+eta*(np.array(grad(params_nesterov))+noise)\n        params=params-v\n        param_traj[j+1,]=params\n    return param_traj\n\n\n# Investigate effect of learning rate in GD\nplt.close()\na,b = 1.0,1.0\nx, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\nfig, ax = plt.subplots(figsize=(10, 6))\nz=np.abs(minima_surface(x,y,a,b))\nax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\nax.plot(0,0, 'r*', markersize=18)\n\n#initial point\ninit1=[-2,4]\ninit2=[-1.7,4]\ninit3=[-1.5,4]\ninit4=[-3,4.5]\neta1=0.1\neta2=0.5\neta3=1\neta4=1.01\ngd_1=gd(grad_minima_surface,init1, n_epochs=100, eta=eta1)\ngd_2=gd(grad_minima_surface,init2, n_epochs=100, eta=eta2)\ngd_3=gd(grad_minima_surface,init3, n_epochs=100, eta=eta3)\ngd_4=gd(grad_minima_surface,init4, n_epochs=10, eta=eta4)\n#print(gd_1)\noverlay_trajectory_contour(ax,gd_1,'$\\eta=$%s'% eta1,'g--*', lw=0.5)\noverlay_trajectory_contour(ax,gd_2,'$\\eta=$%s'% eta2,'b-&lt;', lw=0.5)\noverlay_trajectory_contour(ax,gd_3,'$\\eta=$%s'% eta3,'-&gt;', lw=0.5)\noverlay_trajectory_contour(ax,gd_4,'$\\eta=$%s'% eta4,'c-o', lw=0.5)\nplt.legend(loc=2)\nplt.show()\nfig.savefig(\"GD3regimes.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Investigate effect of learning rate in GD\nplt.close()\na,b = 1.0,1.0\nx, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\nfig, ax = plt.subplots(figsize=(10, 6))\nz=np.abs(minima_surface(x,y,a,b))\nax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\nax.plot(0,0, 'r*', markersize=18)\n\n#initial point\ninit1=[-2,4]\ninit2=[-1.7,4]\ninit3=[-1.5,4]\ninit4=[-3,4.5]\neta1=0.1\neta2=0.5\neta3=1\neta4=1.01\ngd_1=gd_with_mom(grad_minima_surface,init1, n_epochs=100, eta=eta1)\ngd_2=gd_with_mom(grad_minima_surface,init2, n_epochs=100, eta=eta2)\ngd_3=gd_with_mom(grad_minima_surface,init3, n_epochs=100, eta=eta3)\ngd_4=gd_with_mom(grad_minima_surface,init4, n_epochs=10, eta=eta4)\n#print(gd_1)\noverlay_trajectory_contour(ax,gd_1,'$\\eta=$%s'% eta1,'g--*', lw=0.5)\noverlay_trajectory_contour(ax,gd_2,'$\\eta=$%s'% eta2,'b-&lt;', lw=0.5)\noverlay_trajectory_contour(ax,gd_3,'$\\eta=$%s'% eta3,'-&gt;', lw=0.5)\noverlay_trajectory_contour(ax,gd_4,'$\\eta=$%s'% eta4,'c-o', lw=0.5)\nplt.legend(loc=2)\nplt.show()\n#fig.savefig(\"GD3regimes.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Investigate effect of learning rate in GD\nplt.close()\na,b = 1.0,1.0\nx, y = np.meshgrid(np.arange(-4.5, 4.5, 0.2), np.arange(-4.5, 4.5, 0.2))\nfig, ax = plt.subplots(figsize=(10, 6))\nz=np.abs(minima_surface(x,y,a,b))\nax.contour(x, y, z, levels=np.logspace(0.0, 5, 35), norm=LogNorm(), cmap=\"RdYlBu_r\")\nax.plot(0,0, 'r*', markersize=18)\n\n#initial point\ninit1=[-2,4]\ninit2=[-1.7,4]\ninit3=[-1.5,4]\ninit4=[-3,4.5]\neta1=0.1\neta2=0.5\neta3=1\neta4=1.01\ngd_1=NAG(grad_minima_surface,init1, n_epochs=100, eta=eta1)\ngd_2=NAG(grad_minima_surface,init2, n_epochs=100, eta=eta2)\n#gd_3=NAG(grad_minima_surface,init3, n_epochs=100, eta=eta3)\n#gd_4=NAG(grad_minima_surface,init4, n_epochs=10, eta=eta4)\n#print(gd_1)\noverlay_trajectory_contour(ax,gd_1,'$\\eta=$%s'% eta1,'g--*', lw=0.5)\noverlay_trajectory_contour(ax,gd_2,'$\\eta=$%s'% eta2,'b-&lt;', lw=0.5)\n#overlay_trajectory_contour(ax,gd_3,'$\\eta=$%s'% eta3,'-&gt;', lw=0.5)\n#overlay_trajectory_contour(ax,gd_4,'$\\eta=$%s'% eta4,'c-o', lw=0.5)\nplt.legend(loc=2)\nplt.show()\n#fig.savefig(\"GD3regimes.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n################################################################################\n# Methods that exploit first and second moments of gradient: RMS-PROP and ADAMS\n################################################################################\n\ndef rms_prop(grad, init, n_epochs=5000, eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0):\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init#Import relevant packages\n    grad_sq=0;\n    for j in range(n_epochs):\n        noise=noise_strength*np.random.randn(params.size)\n        g=np.array(grad(params))+noise\n        grad_sq=beta*grad_sq+(1-beta)*g*g\n        v=eta*np.divide(g,np.sqrt(grad_sq+epsilon))\n        params= params-v\n        param_traj[j+1,]=params\n    return param_traj\n                        \n                        \ndef adams(grad, init, n_epochs=5000, eta=10**-4, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0):\n    params=np.array(init)\n    param_traj=np.zeros([n_epochs+1,2])\n    param_traj[0,]=init\n    v=0;\n    grad_sq=0;\n    for j in range(n_epochs):\n        noise=noise_strength*np.random.randn(params.size)\n        g=np.array(grad(params))+noise\n        v=gamma*v+(1-gamma)*g\n        grad_sq=beta*grad_sq+(1-beta)*g*g\n        v_hat=v/(1-gamma**(j+1))\n        grad_sq_hat=grad_sq/(1-beta**(j+1))\n        params=params-eta*np.divide(v_hat,np.sqrt(grad_sq_hat+epsilon))\n        param_traj[j+1,]=params\n    return param_traj\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**4\nlr_l=10**-3\nlr_s=10**-6\n\ninit1=np.array([4,3])\nfig1, ax1=contour_beales_function()\n\ngd_trajectory1=gd(grad_beales_function,init1,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory1=gd_with_mom(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory1=NAG(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_beales_function,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory1=adams(grad_beales_function,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_beales_function,init3,10**5, eta=lr_s, noise_strength=0)\ngdm_trajectory3=gd_with_mom(grad_beales_function,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory3=NAG(grad_beales_function,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory3=rms_prop(grad_beales_function,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory3=adams(grad_beales_function,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_beales_function,init4,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory4=gd_with_mom(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory4=NAG(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory4=rms_prop(grad_beales_function,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory4=adams(grad_beales_function,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**4\nlr_l=10**-6\nlr_s=10**-6\n\ninit1=np.array([4,3])\nfig1, ax1=contour_beales_function()\n\ngd_trajectory1=gd(grad_beales_function,init1,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory1=gd_with_mom(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory1=NAG(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_beales_function,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory1=adams(grad_beales_function,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_beales_function,init3,10**5, eta=lr_s, noise_strength=0)\ngdm_trajectory3=gd_with_mom(grad_beales_function,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory3=NAG(grad_beales_function,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory3=rms_prop(grad_beales_function,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory3=adams(grad_beales_function,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_beales_function,init4,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory4=gd_with_mom(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory4=NAG(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory4=rms_prop(grad_beales_function,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory4=adams(grad_beales_function,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**4\nlr_l=10**-3\nlr_s=10**-3\n\ninit1=np.array([4,3])\nfig1, ax1=contour_beales_function()\n\ngd_trajectory1=gd(grad_beales_function,init1,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory1=gd_with_mom(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory1=NAG(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_beales_function,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory1=adams(grad_beales_function,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_beales_function,init3,10**5, eta=lr_s, noise_strength=0)\ngdm_trajectory3=gd_with_mom(grad_beales_function,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory3=NAG(grad_beales_function,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory3=rms_prop(grad_beales_function,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory3=adams(grad_beales_function,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_beales_function,init4,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory4=gd_with_mom(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory4=NAG(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory4=rms_prop(grad_beales_function,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=0)\nadam_trajectory4=adams(grad_beales_function,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n/tmp/ipykernel_739963/3449801150.py:47: RuntimeWarning: overflow encountered in scalar multiply\n  grad_x=2*(1.5-x+x*y)*(-1+y)+2*(2.25-x+x*y**2)*(-1+y**2)+2*(2.625-x+x*y**3)*(-1+y**3)\n/tmp/ipykernel_739963/3449801150.py:47: RuntimeWarning: overflow encountered in scalar power\n  grad_x=2*(1.5-x+x*y)*(-1+y)+2*(2.25-x+x*y**2)*(-1+y**2)+2*(2.625-x+x*y**3)*(-1+y**3)\n/tmp/ipykernel_739963/3449801150.py:48: RuntimeWarning: overflow encountered in scalar multiply\n  grad_y=2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*x*y+6*(2.625-x+x*y**3)*x*y**2\n/tmp/ipykernel_739963/3449801150.py:48: RuntimeWarning: overflow encountered in scalar power\n  grad_y=2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*x*y+6*(2.625-x+x*y**3)*x*y**2\n/tmp/ipykernel_739963/3449801150.py:47: RuntimeWarning: invalid value encountered in scalar add\n  grad_x=2*(1.5-x+x*y)*(-1+y)+2*(2.25-x+x*y**2)*(-1+y**2)+2*(2.625-x+x*y**3)*(-1+y**3)\n/tmp/ipykernel_739963/3449801150.py:48: RuntimeWarning: invalid value encountered in scalar add\n  grad_y=2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*x*y+6*(2.625-x+x*y**3)*x*y**2\n\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**5\nlr_l=10**-3\nlr_s=10**-4\nnoise = 400.0\ninit1=np.array([4,3])\nfig1, ax1=contour_beales_function()\n\ngd_trajectory1=gd(grad_beales_function,init1,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory1=gd_with_mom(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\n#NAG_trajectory1=NAG(grad_beales_function,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory1=rms_prop(grad_beales_function,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\n#adam_trajectory1=adams(grad_beales_function,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_beales_function,init3,10**5, eta=lr_s, noise_strength=noise)\ngdm_trajectory3=gd_with_mom(grad_beales_function,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=noise)\n#NAG_trajectory3=NAG(grad_beales_function,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory3=rms_prop(grad_beales_function,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\n#adam_trajectory3=adams(grad_beales_function,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_beales_function,init4,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory4=gd_with_mom(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\n#NAG_trajectory4=NAG(grad_beales_function,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory4=rms_prop(grad_beales_function,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\n#adam_trajectory4=adams(grad_beales_function,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n#Make plots of surfaces\nplt.close() # closes previous plots\nx, y = np.mgrid[-1:1:31j, -1:1:31j]\nfig1,ax1=contour_rosenbrock()\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**5\nlr_l=10**-3\nlr_s=10**-3\nnoise = 0.0\ninit1=np.array([4,3])\nfig1, ax1=contour_rosenbrock()\n\ngd_trajectory1=gd(grad_rosenbrock,init1,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory1=gd_with_mom(grad_rosenbrock,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory1=NAG(grad_rosenbrock,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_rosenbrock,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory1=adams(grad_rosenbrock,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_rosenbrock,init3,10**5, eta=lr_s, noise_strength=noise)\ngdm_trajectory3=gd_with_mom(grad_rosenbrock,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory3=NAG(grad_rosenbrock,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory3=rms_prop(grad_rosenbrock,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory3=adams(grad_rosenbrock,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_rosenbrock,init4,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory4=gd_with_mom(grad_rosenbrock,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory4=NAG(grad_rosenbrock,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory4=rms_prop(grad_rosenbrock,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory4=adams(grad_rosenbrock,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n/tmp/ipykernel_739963/3753588982.py:96: RuntimeWarning: overflow encountered in scalar power\n  grad_x= -2*(a-x) - 4*b*(y-x**2)*x\n/tmp/ipykernel_739963/3753588982.py:97: RuntimeWarning: overflow encountered in scalar power\n  grad_y= 2*b*(y-x**2)\n/tmp/ipykernel_739963/3753588982.py:96: RuntimeWarning: invalid value encountered in scalar subtract\n  grad_x= -2*(a-x) - 4*b*(y-x**2)*x\n/tmp/ipykernel_739963/3753588982.py:97: RuntimeWarning: invalid value encountered in scalar subtract\n  grad_y= 2*b*(y-x**2)\n/tmp/ipykernel_739963/3753588982.py:96: RuntimeWarning: overflow encountered in scalar multiply\n  grad_x= -2*(a-x) - 4*b*(y-x**2)*x\n/tmp/ipykernel_739963/4156107574.py:39: RuntimeWarning: invalid value encountered in add\n  v=gamma*v+eta*(np.array(grad(params_nesterov))+noise)\n/tmp/ipykernel_739963/4156107574.py:14: RuntimeWarning: invalid value encountered in subtract\n  params=params-v\n\n\n\n\n\n\n\n\n\n\nplt.plot(np.log10(np.linalg.norm(gdm_trajectory4 - [1,1],axis=1)))\nplt.plot(np.log10(np.linalg.norm(gd_trajectory4 - [1,1],axis=1)))\nplt.plot(np.log10(np.linalg.norm(adam_trajectory4 - [1,1],axis=1)))\n\n\n\n\n\n\n\n\n\ncontour_himmel()\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**2\nlr_l=10**-2\nlr_s=10**-4\nnoise = 0.0\ninit1=np.array([-3,0])\nfig1, ax1=contour_himmel()\n\ngd_trajectory1=gd(grad_himmel,init1,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory1=gd_with_mom(grad_himmel,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory1=NAG(grad_himmel,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_himmel,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory1=adams(grad_himmel,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\ninit3=np.array([-1,4])\n\ngd_trajectory3=gd(grad_himmel,init3,10**5, eta=lr_s, noise_strength=noise)\ngdm_trajectory3=gd_with_mom(grad_himmel,init3,10**5,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory3=NAG(grad_himmel,init3,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory3=rms_prop(grad_himmel,init3,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory3=adams(grad_himmel,init3,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory3, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory3, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory3, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory3,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory3,'ADAMS', 'r')\n\ninit4=np.array([-2,-4])\n\ngd_trajectory4=gd(grad_himmel,init4,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectory4=gd_with_mom(grad_himmel,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\nNAG_trajectory4=NAG(grad_himmel,init4,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory4=rms_prop(grad_himmel,init4,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory4=adams(grad_himmel,init4,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory4, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory4, 'GDM','m')\noverlay_trajectory_contour_M(ax1,NAG_trajectory4, 'NAG','c--')\noverlay_trajectory_contour_M(ax1,rms_prop_trajectory4,'RMS', 'b-.')\noverlay_trajectory_contour_M(ax1,adam_trajectory4,'ADAMS', 'r')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.plot(np.log10(np.linalg.norm(gdm_trajectory1 - [3,2],axis=1)))\nplt.plot(np.log10(np.linalg.norm(gd_trajectory1 - [3,2],axis=1)))\nplt.plot(np.log10(np.linalg.norm(adam_trajectory1 - [3,2],axis=1)))\n\n/tmp/ipykernel_739963/3398975203.py:3: RuntimeWarning: divide by zero encountered in log10\n  plt.plot(np.log10(np.linalg.norm(adam_trajectory1 - [3,2],axis=1)))\n\n\n\n\n\n\n\n\n\n\nplt.plot(gdm_trajectory1-[gdm_trajectory1[-1,-1],gdm_trajectory1[-1,-1]])\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**3\nlr_l=10**-2\nlr_s=10**-4\nnoise = 10.0\ninit1=np.array([-3,0])\nfig1, ax1=contour_himmel()\n\ngd_trajectory1=gd(grad_himmel,init1,Nsteps, eta=lr_s, noise_strength=0)\ngdm_trajectory1=gd_with_mom(grad_himmel,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nNAG_trajectory1=NAG(grad_himmel,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\nrms_prop_trajectory1=rms_prop(grad_himmel,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\nadam_trajectory1=adams(grad_himmel,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\n\ngd_trajectoryNoise=gd(grad_himmel,init1,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectoryNoise=gd_with_mom(grad_himmel,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\n#overlay_trajectory_contour_M(ax1,gdm_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,gd_trajectoryNoise, 'GD_N','c-')\n#overlay_trajectory_contour_M(ax1,gdm_trajectoryNoise, 'GDM_N','r-')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.close()\n#Make static plot of the results\nNsteps=10**5\nlr_l=10**-2\nlr_s=10**-4\nnoise = 3.0\ninit1=np.array([-2,0])\nfig1, ax1=contour_saddle()\n\ngd_trajectory1=gd(grad_saddle,init1,Nsteps, eta=lr_s, noise_strength=0.1)\ngdm_trajectory1=gd_with_mom(grad_saddle,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\n#NAG_trajectory1=NAG(grad_saddle,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory1=rms_prop(grad_saddle,init1,Nsteps,eta=lr_l, beta=0.9,epsilon=10**-8,noise_strength=noise)\n#adam_trajectory1=adams(grad_saddle,init1,Nsteps,eta=lr_l, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=noise)\n\n\ngd_trajectoryNoise=gd(grad_saddle,init1,Nsteps, eta=lr_s, noise_strength=noise)\ngdm_trajectoryNoise=gd_with_mom(grad_saddle,init1,Nsteps,eta=lr_s, gamma=0.9,noise_strength=noise)\n\noverlay_trajectory_contour_M(ax1,gd_trajectory1, 'GD','k')\noverlay_trajectory_contour_M(ax1,gdm_trajectory1, 'GD-M','r')\n\n#overlay_trajectory_contour_M(ax1,gdm_trajectory1, 'GDM','m')\noverlay_trajectory_contour_M(ax1,gd_trajectoryNoise, 'GD_N','c-')\n#overlay_trajectory_contour_M(ax1,gdm_trajectoryNoise, 'GDM_N','r-')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory1, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory1,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory1,'ADAMS', 'r')\n\nplt.legend(loc=2)\n\n#init2=np.array([1.5,1.5])\n#gd_trajectory2=gd(grad_beales_function,init2,Nsteps, eta=10**-6, noise_strength=0)\n#gdm_trajectory2=gd_with_mom(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#NAG_trajectory2=NAG(grad_beales_function,init2,Nsteps,eta=10**-6, gamma=0.9,noise_strength=0)\n#rms_prop_trajectory2=rms_prop(grad_beales_function,init2,Nsteps,eta=10**-3, beta=0.9,epsilon=10**-8,noise_strength=0)\n#adam_trajectory2=adams(grad_beales_function,init2,Nsteps,eta=10**-3, gamma=0.9, beta=0.99,epsilon=10**-8,noise_strength=0)\n#overlay_trajectory_contour_M(ax1,gdm_trajectory2, 'GDM','m')\n#overlay_trajectory_contour_M(ax1,NAG_trajectory2, 'NAG','c--')\n#overlay_trajectory_contour_M(ax1,rms_prop_trajectory2,'RMS', 'b-.')\n#overlay_trajectory_contour_M(ax1,adam_trajectory2,'ADAMS', 'r')\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef contour_saddle():\n    x, y = np.meshgrid(np.arange(-2.5, 2.5, 0.2), np.arange(-2.5, 2.5, 0.2))\n    fig, ax = plt.subplots(figsize=(10, 6))\n    z= saddle(x,y)\n    cax = ax.contourf(x, y, z,levels=np.linspace(-10,10,35), cmap=\"bwr\")\n\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n\n    ax.set_xlim((-2.5, 2.5))\n    ax.set_ylim((-2.5, 2.5))\n    \n    return fig,ax\n\n\ncontour_saddle()\n\n\n\n\n\n\n\n\n\nsaddle(2,0)\n\n4\n\n\n\n\nx, y = np.mgrid[-1:1:31j, -1:1:31j]\nfig1,ax1=plot_surface(x,y,saddle(x,y))\n\n\n\n\n\n\n\n\n\ndef plot_surface(x, y, z, azim=0, elev=60, dist=30, cmap=\"RdYlBu_r\"):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    plot_args = {'rstride': 1, 'cstride': 1, 'cmap':cmap,\n             'linewidth': 20, 'antialiased': True,\n             'vmin': 0, 'vmax': 200}\n    ax.plot_surface(x, y, z, **plot_args)\n    ax.view_init(azim=azim, elev=elev)\n    ax.dist=dist\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-2, 2)\n    \n    plt.xticks([-5, -2, 0, 2, 5], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n    plt.yticks([-5, -2, 0, 2, 5], [\"-1\", \"-1/2\", \"0\", \"1/2\", \"1\"])\n    ax.set_zticks([0, 50, 100, 150,200])\n    ax.set_zticklabels([\"0\", \"50\", \"100\", \"150\",\"200\"])\n    \n    ax.set_xlabel(\"x\", fontsize=18)\n    ax.set_ylabel(\"y\", fontsize=18)\n    ax.set_zlabel(\"z\", fontsize=18)\n    return fig, ax;\n\nhimmel(x,y)\n\narray([[2.50000000e+02, 1.82716049e+02, 1.39382716e+02, 1.16000000e+02,\n        1.08864198e+02, 1.14567901e+02, 1.30000000e+02, 1.52345679e+02,\n        1.79086420e+02, 2.08000000e+02, 2.37160494e+02, 2.64938272e+02,\n        2.90000000e+02, 3.11308642e+02, 3.28123457e+02, 3.40000000e+02,\n        3.46790123e+02, 3.48641975e+02, 3.46000000e+02, 3.39604938e+02,\n        3.30493827e+02, 3.20000000e+02, 3.09753086e+02, 3.01679012e+02,\n        2.98000000e+02, 3.01234568e+02, 3.14197531e+02, 3.40000000e+02,\n        3.82049383e+02, 4.44049383e+02, 5.30000000e+02],\n       [2.11160494e+02, 1.39580247e+02, 9.20987654e+01, 6.47160494e+01,\n        5.37283951e+01, 5.57283951e+01, 6.76049383e+01, 8.65432099e+01,\n        1.10024691e+02, 1.35827160e+02, 1.62024691e+02, 1.86987654e+02,\n        2.09382716e+02, 2.28172840e+02, 2.42617284e+02, 2.52271605e+02,\n        2.56987654e+02, 2.56913580e+02, 2.52493827e+02, 2.44469136e+02,\n        2.33876543e+02, 2.22049383e+02, 2.10617284e+02, 2.01506173e+02,\n        1.96938272e+02, 1.99432099e+02, 2.11802469e+02, 2.37160494e+02,\n        2.78913580e+02, 3.40765432e+02, 4.26716049e+02],\n       [1.94493827e+02, 1.18765432e+02, 6.72839506e+01, 3.60493827e+01,\n        2.13580247e+01, 1.98024691e+01, 2.82716049e+01, 4.39506173e+01,\n        6.43209877e+01, 8.71604938e+01, 1.10543210e+02, 1.32839506e+02,\n        1.52716049e+02, 1.69135802e+02, 1.81358025e+02, 1.88938272e+02,\n        1.91728395e+02, 1.89876543e+02, 1.83827160e+02, 1.74320988e+02,\n        1.62395062e+02, 1.49382716e+02, 1.36913580e+02, 1.26913580e+02,\n        1.21604938e+02, 1.23506173e+02, 1.35432099e+02, 1.60493827e+02,\n        2.02098765e+02, 2.63950617e+02, 3.50049383e+02],\n       [1.96000000e+02, 1.16271605e+02, 6.09382716e+01, 2.60000000e+01,\n        7.75308642e+00, 2.79012346e+00, 8.00000000e+00, 2.05679012e+01,\n        3.79753086e+01, 5.80000000e+01, 7.87160494e+01, 9.84938272e+01,\n        1.16000000e+02, 1.30197531e+02, 1.40345679e+02, 1.46000000e+02,\n        1.47012346e+02, 1.43530864e+02, 1.36000000e+02, 1.25160494e+02,\n        1.12049383e+02, 9.80000000e+01, 8.46419753e+01, 7.39012346e+01,\n        6.80000000e+01, 6.94567901e+01, 8.10864198e+01, 1.06000000e+02,\n        1.47604938e+02, 2.09604938e+02, 2.96000000e+02],\n       [2.11975309e+02, 1.28395062e+02, 6.93580247e+01, 3.08641975e+01,\n        9.20987654e+00, 9.87654321e-01, 3.08641975e+00, 1.26913580e+01,\n        2.72839506e+01, 4.46419753e+01, 6.28395062e+01, 8.02469136e+01,\n        9.55308642e+01, 1.07654321e+02, 1.15876543e+02, 1.19753086e+02,\n        1.19135802e+02, 1.14172840e+02, 1.05308642e+02, 9.32839506e+01,\n        7.91358025e+01, 6.41975309e+01, 5.00987654e+01, 3.87654321e+01,\n        3.24197531e+01, 3.35802469e+01, 4.50617284e+01, 6.99753086e+01,\n        1.11728395e+02, 1.74024691e+02, 2.60864198e+02],\n       [2.39012346e+02, 1.51728395e+02, 8.91358025e+01, 4.72345679e+01,\n        2.23209877e+01, 1.09876543e+01, 1.01234568e+01, 1.69135802e+01,\n        2.88395062e+01, 4.36790123e+01, 5.95061728e+01, 7.46913580e+01,\n        8.79012346e+01, 9.80987654e+01, 1.04543210e+02, 1.06790123e+02,\n        1.04691358e+02, 9.83950617e+01, 8.83456790e+01, 7.52839506e+01,\n        6.02469136e+01, 4.45679012e+01, 2.98765432e+01, 1.80987654e+01,\n        1.14567901e+01, 1.24691358e+01, 2.39506173e+01, 4.90123457e+01,\n        9.10617284e+01, 1.53802469e+02, 2.41234568e+02],\n       [2.74000000e+02, 1.83160494e+02, 1.17160494e+02, 7.20000000e+01,\n        4.39753086e+01, 2.96790123e+01, 2.60000000e+01, 3.01234568e+01,\n        3.95308642e+01, 5.20000000e+01, 6.56049383e+01, 7.87160494e+01,\n        9.00000000e+01, 9.84197531e+01, 1.03234568e+02, 1.04000000e+02,\n        1.00567901e+02, 9.30864198e+01, 8.20000000e+01, 6.80493827e+01,\n        5.22716049e+01, 3.60000000e+01, 2.08641975e+01, 8.79012346e+00,\n        2.00000000e+00, 3.01234568e+00, 1.46419753e+01, 4.00000000e+01,\n        8.24938272e+01, 1.45827160e+02, 2.34000000e+02],\n       [3.14123457e+02, 2.19876543e+02, 1.50617284e+02, 1.02345679e+02,\n        7.13580247e+01, 5.42469136e+01, 4.79012346e+01, 4.95061728e+01,\n        5.65432099e+01, 6.67901235e+01, 7.83209877e+01, 8.95061728e+01,\n        9.90123457e+01, 1.05802469e+02, 1.09135802e+02, 1.08567901e+02,\n        1.03950617e+02, 9.54320988e+01, 8.34567901e+01, 6.87654321e+01,\n        5.23950617e+01, 3.56790123e+01, 2.02469136e+01, 8.02469136e+00,\n        1.23456790e+00, 2.39506173e+00, 1.43209877e+01, 4.01234568e+01,\n        8.32098765e+01, 1.47283951e+02, 2.36345679e+02],\n       [3.56864198e+02, 2.59358025e+02, 1.86987654e+02, 1.35753086e+02,\n        1.01950617e+02, 8.21728395e+01, 7.33086420e+01, 7.25432099e+01,\n        7.73580247e+01, 8.55308642e+01, 9.51358025e+01, 1.04543210e+02,\n        1.12419753e+02, 1.17728395e+02, 1.19728395e+02, 1.17975309e+02,\n        1.12320988e+02, 1.02913580e+02, 9.01975309e+01, 7.49135802e+01,\n        5.80987654e+01, 4.10864198e+01, 2.55061728e+01, 1.32839506e+01,\n        6.64197531e+00, 8.09876543e+00, 2.04691358e+01, 4.68641975e+01,\n        9.06913580e+01, 1.55654321e+02, 2.45753086e+02],\n       [4.00000000e+02, 2.99382716e+02, 2.24049383e+02, 1.70000000e+02,\n        1.33530864e+02, 1.11234568e+02, 1.00000000e+02, 9.70123457e+01,\n        9.97530864e+01, 1.06000000e+02, 1.13827160e+02, 1.21604938e+02,\n        1.28000000e+02, 1.31975309e+02, 1.32790123e+02, 1.30000000e+02,\n        1.23456790e+02, 1.13308642e+02, 1.00000000e+02, 8.42716049e+01,\n        6.71604938e+01, 5.00000000e+01, 3.44197531e+01, 2.23456790e+01,\n        1.60000000e+01, 1.79012346e+01, 3.08641975e+01, 5.80000000e+01,\n        1.02716049e+02, 1.68716049e+02, 2.60000000e+02],\n       [4.41604938e+02, 3.38024691e+02, 2.59876543e+02, 2.03160494e+02,\n        1.64172840e+02, 1.39506173e+02, 1.26049383e+02, 1.20987654e+02,\n        1.21802469e+02, 1.26271605e+02, 1.32469136e+02, 1.38765432e+02,\n        1.43827160e+02, 1.46617284e+02, 1.46395062e+02, 1.42716049e+02,\n        1.35432099e+02, 1.24691358e+02, 1.10938272e+02, 9.49135802e+01,\n        7.76543210e+01, 6.04938272e+01, 4.50617284e+01, 3.32839506e+01,\n        2.73827160e+01, 2.98765432e+01, 4.35802469e+01, 7.16049383e+01,\n        1.17358025e+02, 1.84543210e+02, 2.77160494e+02],\n       [4.80049383e+02, 3.73654321e+02, 2.92839506e+02, 2.33604938e+02,\n        1.92246914e+02, 1.65358025e+02, 1.49827160e+02, 1.42839506e+02,\n        1.41876543e+02, 1.44716049e+02, 1.49432099e+02, 1.54395062e+02,\n        1.58271605e+02, 1.60024691e+02, 1.58913580e+02, 1.54493827e+02,\n        1.46617284e+02, 1.35432099e+02, 1.21382716e+02, 1.05209877e+02,\n        8.79506173e+01, 7.09382716e+01, 5.58024691e+01, 4.44691358e+01,\n        3.91604938e+01, 4.23950617e+01, 5.69876543e+01, 8.60493827e+01,\n        1.32987654e+02, 2.01506173e+02, 2.95604938e+02],\n       [5.14000000e+02, 4.04938272e+02, 3.21604938e+02, 2.60000000e+02,\n        2.16419753e+02, 1.87456790e+02, 1.70000000e+02, 1.61234568e+02,\n        1.58641975e+02, 1.60000000e+02, 1.63382716e+02, 1.67160494e+02,\n        1.70000000e+02, 1.70864198e+02, 1.69012346e+02, 1.64000000e+02,\n        1.55679012e+02, 1.44197531e+02, 1.30000000e+02, 1.13827160e+02,\n        9.67160494e+01, 8.00000000e+01, 6.53086420e+01, 5.45679012e+01,\n        5.00000000e+01, 5.41234568e+01, 6.97530864e+01, 1.00000000e+02,\n        1.48271605e+02, 2.18271605e+02, 3.14000000e+02],\n       [5.42419753e+02, 4.30839506e+02, 3.45135802e+02, 2.81308642e+02,\n        2.35654321e+02, 2.04765432e+02, 1.85530864e+02, 1.75135802e+02,\n        1.71061728e+02, 1.71086420e+02, 1.73283951e+02, 1.76024691e+02,\n        1.77975309e+02, 1.78098765e+02, 1.75654321e+02, 1.70197531e+02,\n        1.61580247e+02, 1.49950617e+02, 1.35753086e+02, 1.19728395e+02,\n        1.02913580e+02, 8.66419753e+01, 7.25432099e+01, 6.25432099e+01,\n        5.88641975e+01, 6.40246914e+01, 8.08395062e+01, 1.12419753e+02,\n        1.62172840e+02, 2.33802469e+02, 3.31308642e+02],\n       [5.64567901e+02, 4.50617284e+02, 3.62691358e+02, 2.96790123e+02,\n        2.49209877e+02, 2.16543210e+02, 1.95679012e+02, 1.83802469e+02,\n        1.78395062e+02, 1.77234568e+02, 1.78395062e+02, 1.80246914e+02,\n        1.81456790e+02, 1.80987654e+02, 1.78098765e+02, 1.72345679e+02,\n        1.63580247e+02, 1.51950617e+02, 1.37901235e+02, 1.22172840e+02,\n        1.05802469e+02, 9.01234568e+01, 7.67654321e+01, 6.76543210e+01,\n        6.50123457e+01, 7.13580247e+01, 8.95061728e+01, 1.22567901e+02,\n        1.73950617e+02, 2.47358025e+02, 3.46790123e+02],\n       [5.80000000e+02, 4.63827160e+02, 3.73827160e+02, 3.06000000e+02,\n        2.56641975e+02, 2.22345679e+02, 2.00000000e+02, 1.86790123e+02,\n        1.80197531e+02, 1.78000000e+02, 1.78271605e+02, 1.79382716e+02,\n        1.80000000e+02, 1.79086420e+02, 1.75901235e+02, 1.70000000e+02,\n        1.61234568e+02, 1.49753086e+02, 1.36000000e+02, 1.20716049e+02,\n        1.04938272e+02, 9.00000000e+01, 7.75308642e+01, 6.94567901e+01,\n        6.80000000e+01, 7.56790123e+01, 9.53086420e+01, 1.30000000e+02,\n        1.83160494e+02, 2.58493827e+02, 3.60000000e+02],\n       [5.88567901e+02, 4.70320988e+02, 3.78395062e+02, 3.08790123e+02,\n        2.57802469e+02, 2.22024691e+02, 1.98345679e+02, 1.83950617e+02,\n        1.76320988e+02, 1.73234568e+02, 1.72765432e+02, 1.73283951e+02,\n        1.73456790e+02, 1.72246914e+02, 1.68913580e+02, 1.63012346e+02,\n        1.54395062e+02, 1.43209877e+02, 1.29901235e+02, 1.15209877e+02,\n        1.00172840e+02, 8.61234568e+01, 7.46913580e+01, 6.78024691e+01,\n        6.76790123e+01, 7.68395062e+01, 9.80987654e+01, 1.34567901e+02,\n        1.89654321e+02, 2.67061728e+02, 3.70790123e+02],\n       [5.90419753e+02, 4.70246914e+02, 3.76543210e+02, 3.05308642e+02,\n        2.52839506e+02, 2.15728395e+02, 1.90864198e+02, 1.75432099e+02,\n        1.66913580e+02, 1.63086420e+02, 1.62024691e+02, 1.62098765e+02,\n        1.61975309e+02, 1.60617284e+02, 1.57283951e+02, 1.51530864e+02,\n        1.43209877e+02, 1.32469136e+02, 1.19753086e+02, 1.05802469e+02,\n        9.16543210e+01, 7.86419753e+01, 6.83950617e+01, 6.28395062e+01,\n        6.41975309e+01, 7.49876543e+01, 9.80246914e+01, 1.36419753e+02,\n        1.93580247e+02, 2.73209877e+02, 3.79308642e+02],\n       [5.86000000e+02, 4.64049383e+02, 3.68716049e+02, 2.96000000e+02,\n        2.42197531e+02, 2.03901235e+02, 1.78000000e+02, 1.61679012e+02,\n        1.52419753e+02, 1.48000000e+02, 1.46493827e+02, 1.46271605e+02,\n        1.46000000e+02, 1.44641975e+02, 1.41456790e+02, 1.36000000e+02,\n        1.28123457e+02, 1.17975309e+02, 1.06000000e+02, 9.29382716e+01,\n        7.98271605e+01, 6.80000000e+01, 5.90864198e+01, 5.50123457e+01,\n        5.80000000e+01, 7.05679012e+01, 9.55308642e+01, 1.36000000e+02,\n        1.95382716e+02, 2.77382716e+02, 3.86000000e+02],\n       [5.76049383e+02, 4.52469136e+02, 3.55654321e+02, 2.81604938e+02,\n        2.26617284e+02, 1.87283951e+02, 1.60493827e+02, 1.43432099e+02,\n        1.33580247e+02, 1.28716049e+02, 1.26913580e+02, 1.26543210e+02,\n        1.26271605e+02, 1.25061728e+02, 1.22172840e+02, 1.17160494e+02,\n        1.09876543e+02, 1.00469136e+02, 8.93827160e+01, 7.73580247e+01,\n        6.54320988e+01, 5.49382716e+01, 4.75061728e+01, 4.50617284e+01,\n        4.98271605e+01, 6.43209877e+01, 9.13580247e+01, 1.34049383e+02,\n        1.95802469e+02, 2.80320988e+02, 3.91604938e+02],\n       [5.61604938e+02, 4.36543210e+02, 3.38395062e+02, 2.63160494e+02,\n        2.07135802e+02, 1.66913580e+02, 1.39382716e+02, 1.21728395e+02,\n        1.11432099e+02, 1.06271605e+02, 1.04320988e+02, 1.03950617e+02,\n        1.03827160e+02, 1.02913580e+02, 1.00469136e+02, 9.60493827e+01,\n        8.95061728e+01, 8.09876543e+01, 7.09382716e+01, 6.00987654e+01,\n        4.95061728e+01, 4.04938272e+01, 3.46913580e+01, 3.40246914e+01,\n        4.07160494e+01, 5.72839506e+01, 8.65432099e+01, 1.31604938e+02,\n        1.95876543e+02, 2.83061728e+02, 3.97160494e+02],\n       [5.44000000e+02, 4.17604938e+02, 3.18271605e+02, 2.42000000e+02,\n        1.85086420e+02, 1.44123457e+02, 1.16000000e+02, 9.79012346e+01,\n        8.73086420e+01, 8.20000000e+01, 8.00493827e+01, 7.98271605e+01,\n        8.00000000e+01, 7.95308642e+01, 7.76790123e+01, 7.40000000e+01,\n        6.83456790e+01, 6.08641975e+01, 5.20000000e+01, 4.24938272e+01,\n        3.33827160e+01, 2.60000000e+01, 2.19753086e+01, 2.32345679e+01,\n        3.20000000e+01, 5.07901235e+01, 8.24197531e+01, 1.30000000e+02,\n        1.96938272e+02, 2.86938272e+02, 4.04000000e+02],\n       [5.24864198e+02, 3.97283951e+02, 2.96913580e+02, 2.19753086e+02,\n        1.62098765e+02, 1.20543210e+02, 9.19753086e+01, 7.35802469e+01,\n        6.28395062e+01, 5.75308642e+01, 5.57283951e+01, 5.58024691e+01,\n        5.64197531e+01, 5.65432099e+01, 5.54320988e+01, 5.26419753e+01,\n        4.80246914e+01, 4.17283951e+01, 3.41975309e+01, 2.61728395e+01,\n        1.86913580e+01, 1.30864198e+01, 1.09876543e+01, 1.43209877e+01,\n        2.53086420e+01, 4.64691358e+01, 8.06172840e+01, 1.30864198e+02,\n        2.00617284e+02, 2.93580247e+02, 4.13753086e+02],\n       [5.06123457e+02, 3.77506173e+02, 2.76246914e+02, 1.98345679e+02,\n        1.40098765e+02, 9.80987654e+01, 6.92345679e+01, 5.06913580e+01,\n        3.99506173e+01, 3.47901235e+01, 3.32839506e+01, 3.38024691e+01,\n        3.50123457e+01, 3.58765432e+01, 3.56543210e+01, 3.39012346e+01,\n        3.04691358e+01, 2.55061728e+01, 1.94567901e+01, 1.30617284e+01,\n        7.35802469e+00, 3.67901235e+00, 3.65432099e+00, 9.20987654e+00,\n        2.25679012e+01, 4.62469136e+01, 8.30617284e+01, 1.36123457e+02,\n        2.08839506e+02, 3.04913580e+02, 4.28345679e+02],\n       [4.90000000e+02, 3.60493827e+02, 2.58493827e+02, 1.80000000e+02,\n        1.21308642e+02, 7.90123457e+01, 5.00000000e+01, 3.14567901e+01,\n        2.08641975e+01, 1.60000000e+01, 1.49382716e+01, 1.60493827e+01,\n        1.80000000e+01, 1.97530864e+01, 2.05679012e+01, 2.00000000e+01,\n        1.79012346e+01, 1.44197531e+01, 1.00000000e+01, 5.38271605e+00,\n        1.60493827e+00, 0.00000000e+00, 2.19753086e+00, 1.01234568e+01,\n        2.60000000e+01, 5.23456790e+01, 9.19753086e+01, 1.48000000e+02,\n        2.23827160e+02, 3.23160494e+02, 4.50000000e+02],\n       [4.79012346e+02, 3.48765432e+02, 2.46172840e+02, 1.67234568e+02,\n        1.08246914e+02, 6.58024691e+01, 3.67901235e+01, 1.83950617e+01,\n        8.09876543e+00, 3.67901235e+00, 3.20987654e+00, 5.06172840e+00,\n        7.90123457e+00, 1.06913580e+01, 1.26913580e+01, 1.34567901e+01,\n        1.28395062e+01, 1.09876543e+01, 8.34567901e+00, 5.65432099e+00,\n        3.95061728e+00, 4.56790123e+00, 9.13580247e+00, 1.95802469e+01,\n        3.81234568e+01, 6.72839506e+01, 1.09876543e+02, 1.69012346e+02,\n        2.48098765e+02, 3.50839506e+02, 4.81234568e+02],\n       [4.75975309e+02, 3.45135802e+02, 2.42098765e+02, 1.62864198e+02,\n        1.03728395e+02, 6.12839506e+01, 3.24197531e+01, 1.43209877e+01,\n        4.46913580e+00, 6.41975309e-01, 9.13580247e-01, 3.65432099e+00,\n        7.53086420e+00, 1.15061728e+01, 1.48395062e+01, 1.70864198e+01,\n        1.80987654e+01, 1.80246914e+01, 1.73086420e+01, 1.66913580e+01,\n        1.72098765e+01, 2.01975309e+01, 2.72839506e+01, 4.03950617e+01,\n        6.17530864e+01, 9.38765432e+01, 1.39580247e+02, 2.01975309e+02,\n        2.84469136e+02, 3.90765432e+02, 5.24864198e+02],\n       [4.84000000e+02, 3.52716049e+02, 2.49382716e+02, 1.70000000e+02,\n        1.10864198e+02, 6.85679012e+01, 4.00000000e+01, 2.23456790e+01,\n        1.30864198e+01, 1.00000000e+01, 1.11604938e+01, 1.49382716e+01,\n        2.00000000e+01, 2.53086420e+01, 3.01234568e+01, 3.40000000e+01,\n        3.67901235e+01, 3.86419753e+01, 4.00000000e+01, 4.16049383e+01,\n        4.44938272e+01, 5.00000000e+01, 5.97530864e+01, 7.56790123e+01,\n        1.00000000e+02, 1.35234568e+02, 1.84197531e+02, 2.50000000e+02,\n        3.36049383e+02, 4.46049383e+02, 5.84000000e+02],\n       [5.06493827e+02, 3.74913580e+02, 2.71432099e+02, 1.92049383e+02,\n        1.33061728e+02, 9.10617284e+01, 6.29382716e+01, 4.58765432e+01,\n        3.73580247e+01, 3.51604938e+01, 3.73580247e+01, 4.23209877e+01,\n        4.87160494e+01, 5.55061728e+01, 6.19506173e+01, 6.76049383e+01,\n        7.23209877e+01, 7.62469136e+01, 7.98271605e+01, 8.38024691e+01,\n        8.92098765e+01, 9.73827160e+01, 1.09950617e+02, 1.28839506e+02,\n        1.56271605e+02, 1.94765432e+02, 2.47135802e+02, 3.16493827e+02,\n        4.06246914e+02, 5.20098765e+02, 6.62049383e+02],\n       [5.47160494e+02, 4.15432099e+02, 3.11950617e+02, 2.32716049e+02,\n        1.74024691e+02, 1.32469136e+02, 1.04938272e+02, 8.86172840e+01,\n        8.09876543e+01, 7.98271605e+01, 8.32098765e+01, 8.95061728e+01,\n        9.73827160e+01, 1.05802469e+02, 1.14024691e+02, 1.21604938e+02,\n        1.28395062e+02, 1.34543210e+02, 1.40493827e+02, 1.46987654e+02,\n        1.55061728e+02, 1.66049383e+02, 1.81580247e+02, 2.03580247e+02,\n        2.34271605e+02, 2.76172840e+02, 3.32098765e+02, 4.05160494e+02,\n        4.98765432e+02, 6.16617284e+02, 7.62716049e+02],\n       [6.10000000e+02, 4.78271605e+02, 3.74938272e+02, 2.96000000e+02,\n        2.37753086e+02, 1.96790123e+02, 1.70000000e+02, 1.54567901e+02,\n        1.47975309e+02, 1.48000000e+02, 1.52716049e+02, 1.60493827e+02,\n        1.70000000e+02, 1.80197531e+02, 1.90345679e+02, 2.00000000e+02,\n        2.09012346e+02, 2.17530864e+02, 2.26000000e+02, 2.35160494e+02,\n        2.46049383e+02, 2.60000000e+02, 2.78641975e+02, 3.03901235e+02,\n        3.38000000e+02, 3.83456790e+02, 4.43086420e+02, 5.20000000e+02,\n        6.17604938e+02, 7.39604938e+02, 8.90000000e+02]])\n\n\n\n\nx, y = np.mgrid[-5:5:100j, -5:5:100j]\nfig1,ax1=plot_surface(x,y,himmel(x,y))\n\n\n\n\n\n\n\n\n\nnp.min(np.min(himmel(x,y)))\n\n0.0\n\n\n\n(2.0/(1+np.sqrt(10)))**2\n\n0.23088615702040688\n\n\n\n(2.0/(1+np.sqrt(10)))**2\n\n\n((1-np.sqrt(0.1))/(1+np.sqrt(0.1)))**2\n\n0.26987386361223825\n\n\n\n(2/(1+np.sqrt(0.1)))**2\n\n2.3088615702040696\n\n\n\n((1-np.sqrt(0.1))/(1+np.sqrt(0.1)))**2\n\n0.26987386361223825\n\n\n\n((np.sqrt(10)-1)/(np.sqrt(10)+1))**2\n\n0.26987386361223836"
  },
  {
    "objectID": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Python3-Copy1.html",
    "href": "meetups/Meetup-13/neural-networks-and-deep-learning-master/MNIST_Python3-Copy1.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "import random\nimport numpy as np\n\nclass Network:\n    def __init__(self, sizes):\n        # sizes = list of number of neurons in each layer\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        # Biases: one (n,1) vector per layer (except input)\n        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n        # Weights: one (next_layer, this_layer) matrix\n        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n\n    def feedforward(self, a):\n        # Pass input through network\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a) + b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n        # training_data = list of (x, y) tuples\n        if test_data:\n            n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)\n            ]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n            else:\n                print(f\"Epoch {j} complete\")\n\n    def update_mini_batch(self, mini_batch, eta):\n        # Apply a single step of gradient descent\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            x = x.reshape((-1, 1))  # Ensure x is (784, 1)\n            y = y.reshape((-1, 1)) if y.ndim == 1 else y  # optional for y\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w - (eta / len(mini_batch)) * nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b - (eta / len(mini_batch)) * nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        # Return (nabla_b, nabla_w) representing gradient\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        activation = x\n        activations = [x]  # list to store all activations, layer by layer\n        zs = []  # list to store all z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation) + b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # Backward pass\n        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        # Return number of correct classifications\n        test_results = [(np.argmax(self.feedforward(x.reshape((-1, 1)))), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        return (output_activations - y)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\n\nimport gzip\n\n\ndef load_mnist_images(filename):\n    with gzip.open(filename, 'rb') as f:\n        # Skip the header: first 16 bytes\n        f.read(16)\n        # Read the rest and reshape\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        images = data.reshape(-1, 28*28).astype(np.float32)\n        # Normalize pixel values from [0, 255] to [0.0, 1.0]\n        images /= 255.0\n        return images\n\ndef load_mnist_labels(filename):\n    with gzip.open(filename, 'rb') as f:\n        # Skip the header: first 8 bytes\n        f.read(8)\n        labels = np.frombuffer(f.read(), dtype=np.uint8)\n        return labels\n\ndef load_data():\n    # Path to your MNIST data files (downloaded from the site)\n    train_images = load_mnist_images('../data/MNIST/raw/train-images-idx3-ubyte.gz')\n    train_labels = load_mnist_labels('../data/MNIST/raw/train-labels-idx1-ubyte.gz')\n    test_images = load_mnist_images('../data/MNIST/raw/t10k-images-idx3-ubyte.gz')\n    test_labels = load_mnist_labels('../data/MNIST/raw/t10k-labels-idx1-ubyte.gz')\n\n    # Package into (image, label) tuples\n    training_data = list(zip(train_images, train_labels))\n    test_data = list(zip(test_images, test_labels))\n    return training_data, test_data\n\n\ntraining_data, test_data = load_data()\n\n# Initialize the network\nnet = Network([784, 30, 10])\n\n# Train!\nnet.SGD(training_data, epochs=30, mini_batch_size=10, eta=3.0, test_data=test_data)\n\nEpoch 0: 1489 / 10000\nEpoch 1: 881 / 10000\nEpoch 2: 722 / 10000\nEpoch 3: 813 / 10000\nEpoch 4: 706 / 10000\nEpoch 5: 836 / 10000\nEpoch 6: 976 / 10000\nEpoch 7: 992 / 10000\nEpoch 8: 855 / 10000\nEpoch 9: 893 / 10000\nEpoch 10: 965 / 10000\nEpoch 11: 971 / 10000\nEpoch 12: 699 / 10000\nEpoch 13: 978 / 10000\nEpoch 14: 976 / 10000\nEpoch 15: 974 / 10000\nEpoch 16: 978 / 10000\nEpoch 17: 962 / 10000\nEpoch 18: 942 / 10000\nEpoch 19: 898 / 10000\nEpoch 20: 695 / 10000\nEpoch 21: 791 / 10000\nEpoch 22: 866 / 10000\nEpoch 23: 953 / 10000\nEpoch 24: 832 / 10000\nEpoch 25: 1044 / 10000\nEpoch 26: 718 / 10000\nEpoch 27: 1042 / 10000\nEpoch 28: 283 / 10000\nEpoch 29: 766 / 10000\n\n\n\nnet\n\n&lt;__main__.Network at 0x7566664eb790&gt;\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass Network(nn.Module):\n    def __init__(self, sizes):\n        super(Network, self).__init__()\n        self.sizes = sizes\n        self.num_layers = len(sizes)\n        \n        self.layers = nn.ModuleList()\n        for i in range(self.num_layers - 1):\n            layer = nn.Linear(sizes[i], sizes[i+1])\n            nn.init.xavier_normal_(layer.weight)   # Xavier initialization\n            #nn.init.xavier_uniform_(layer.weight)   # Xavier initialization\n\n            #nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n\n            nn.init.zeros_(layer.bias)               # Bias to zero\n            self.layers.append(layer)\n    \n\n\n    \n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = F.sigmoid(layer(x))   # hidden layers \n        x = self.layers[-1](x) \n        # x = F.softmax(self.layers[-1](x), dim=1)  # softmax output \n        return x\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            #nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            #nn.init.xavier_uniform_(m.weight)   # Xavier initialization\n\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n\n\n\n\ndef train(network, train_data, epochs, mini_batch_size, eta, test_data=None):\n    \n    step_size = 1\n    gamma = 0.7\n    #optimizer = optim.SGD(network.parameters(), lr=eta,weight_decay=1e-5,momentum=0.7)\n    optimizer = optim.Adam(network.parameters(),betas = (0.9, 0.999),lr=eta,weight_decay=1e-5)\n\n    step_size = 1\n    gamma = 0.7\n    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n\n    loss_fn = nn.CrossEntropyLoss()\n    \n    loss_history = []      # to store average loss per epoch\n    accuracy_history = []  # to store test accuracy per epoch\n    grad_norm_history = [] # to store gradient norms per epoch\n    train_accuracy_history = []\n    for epoch in range(epochs):\n        network.train()\n        running_loss = 0.0\n        batch_count = 0\n        total_grad_norm = 0.0\n        \n        for data, target in train_data:\n            optimizer.zero_grad()\n            output = network(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            \n            # Compute gradient norm (L2 norm)\n            grad_norm = sum(p.grad.norm()**2 for p in network.parameters() if p.grad is not None).sqrt()\n            total_grad_norm += grad_norm.item()\n\n            optimizer.step()\n            \n            running_loss += loss.item()\n            batch_count += 1\n        \n        avg_loss = running_loss / batch_count\n        loss_history.append(avg_loss)\n        \n        avg_grad_norm = total_grad_norm / batch_count\n        grad_norm_history.append(avg_grad_norm)\n\n        train_acc = evaluate(network,train_data)\n        train_accuracy_history.append(train_acc)\n        \n        # Evaluate test accuracy if test data is provided\n        if test_data:\n            acc = evaluate(network, test_data)\n            accuracy_history.append(acc)\n            print(f\"Epoch {epoch+1}: Avg loss: {avg_loss:.4f} | Train Accuracy: {train_acc:.2f}% | Test Accuracy: {acc:.2f}% | Grad Norm: {avg_grad_norm:.4f}\")\n        else:\n            print(f\"Epoch {epoch+1}: Avg loss: {avg_loss:.4f} | Grad Norm: {avg_grad_norm:.4f}\")\n\n    return loss_history, train_accuracy_history, accuracy_history, grad_norm_history\n\n\n\ndef evaluate(network, test_data):\n    network.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_data:\n            output = network(data)\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            total += data.size(0)\n    acc = 100. * correct / total\n    print(f\"Test Accuracy: {acc:.2f}%\")\n    return acc\n\n\ndef compute_layer_gradient_norms(model):\n    \"\"\"\n    Compute the 2-norm of the gradients for each layer in a PyTorch model.\n    \n    Returns:\n        A list of (layer_name, gradient_norm) tuples.\n    \"\"\"\n    layer_grad_norms = []\n\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.data.norm(2).item()\n            layer_grad_norms.append((name, grad_norm))\n        else:\n            layer_grad_norms.append((name, 0.0))  # No gradient yet\n\n    return layer_grad_norms\n\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n\n# Load MNIST\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), transforms.Lambda(lambda x: x.view(-1))])\n\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n\n\n\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n\n# Initialize network\nnet = Network([784, 30, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 82.01%\nEpoch 1: Avg loss: 1.7479 | Test Accuracy: 82.01% | Grad Norm: 0.6613\nTest Accuracy: 83.88%\nEpoch 2: Avg loss: 1.6297 | Test Accuracy: 83.88% | Grad Norm: 0.4825\nTest Accuracy: 84.48%\nEpoch 3: Avg loss: 1.6162 | Test Accuracy: 84.48% | Grad Norm: 0.4258\nTest Accuracy: 85.00%\nEpoch 4: Avg loss: 1.6093 | Test Accuracy: 85.00% | Grad Norm: 0.3679\nTest Accuracy: 84.79%\nEpoch 5: Avg loss: 1.6056 | Test Accuracy: 84.79% | Grad Norm: 0.3424\nTest Accuracy: 85.91%\nEpoch 6: Avg loss: 1.6015 | Test Accuracy: 85.91% | Grad Norm: 0.3386\nTest Accuracy: 86.16%\nEpoch 7: Avg loss: 1.5985 | Test Accuracy: 86.16% | Grad Norm: 0.3195\nTest Accuracy: 86.25%\nEpoch 8: Avg loss: 1.5961 | Test Accuracy: 86.25% | Grad Norm: 0.3121\nTest Accuracy: 86.30%\nEpoch 9: Avg loss: 1.5944 | Test Accuracy: 86.30% | Grad Norm: 0.2743\nTest Accuracy: 86.33%\nEpoch 10: Avg loss: 1.5938 | Test Accuracy: 86.33% | Grad Norm: 0.2720\nTest Accuracy: 86.26%\nEpoch 11: Avg loss: 1.5933 | Test Accuracy: 86.26% | Grad Norm: 0.2675\nTest Accuracy: 86.68%\nEpoch 12: Avg loss: 1.5917 | Test Accuracy: 86.68% | Grad Norm: 0.2532\nTest Accuracy: 86.80%\nEpoch 13: Avg loss: 1.5899 | Test Accuracy: 86.80% | Grad Norm: 0.2526\nTest Accuracy: 86.45%\nEpoch 14: Avg loss: 1.5898 | Test Accuracy: 86.45% | Grad Norm: 0.2424\nTest Accuracy: 86.32%\nEpoch 15: Avg loss: 1.5899 | Test Accuracy: 86.32% | Grad Norm: 0.2519\nTest Accuracy: 86.64%\nEpoch 16: Avg loss: 1.5889 | Test Accuracy: 86.64% | Grad Norm: 0.2341\nTest Accuracy: 86.70%\nEpoch 17: Avg loss: 1.5883 | Test Accuracy: 86.70% | Grad Norm: 0.2400\nTest Accuracy: 86.69%\nEpoch 18: Avg loss: 1.5876 | Test Accuracy: 86.69% | Grad Norm: 0.2285\nTest Accuracy: 86.59%\nEpoch 19: Avg loss: 1.5875 | Test Accuracy: 86.59% | Grad Norm: 0.2331\nTest Accuracy: 86.53%\nEpoch 20: Avg loss: 1.5870 | Test Accuracy: 86.53% | Grad Norm: 0.2336\nTest Accuracy: 86.82%\nEpoch 21: Avg loss: 1.5860 | Test Accuracy: 86.82% | Grad Norm: 0.2103\nTest Accuracy: 86.75%\nEpoch 22: Avg loss: 1.5853 | Test Accuracy: 86.75% | Grad Norm: 0.2098\nTest Accuracy: 86.85%\nEpoch 23: Avg loss: 1.5857 | Test Accuracy: 86.85% | Grad Norm: 0.1958\nTest Accuracy: 86.92%\nEpoch 24: Avg loss: 1.5853 | Test Accuracy: 86.92% | Grad Norm: 0.2163\nTest Accuracy: 86.59%\nEpoch 25: Avg loss: 1.5852 | Test Accuracy: 86.59% | Grad Norm: 0.2085\nTest Accuracy: 86.60%\nEpoch 26: Avg loss: 1.5847 | Test Accuracy: 86.60% | Grad Norm: 0.1931\nTest Accuracy: 86.76%\nEpoch 27: Avg loss: 1.5840 | Test Accuracy: 86.76% | Grad Norm: 0.1931\nTest Accuracy: 86.75%\nEpoch 28: Avg loss: 1.5834 | Test Accuracy: 86.75% | Grad Norm: 0.1838\nTest Accuracy: 86.89%\nEpoch 29: Avg loss: 1.5842 | Test Accuracy: 86.89% | Grad Norm: 0.2004\nTest Accuracy: 86.93%\nEpoch 30: Avg loss: 1.5835 | Test Accuracy: 86.93% | Grad Norm: 0.1779\n\n\n([1.74790550806125,\n  1.6297487737933796,\n  1.6161798699498175,\n  1.6092668992082277,\n  1.6055524744987488,\n  1.601471389790376,\n  1.598520632982254,\n  1.596056900382042,\n  1.5944379499753316,\n  1.5938490320444108,\n  1.5932815373937288,\n  1.5917234587868054,\n  1.5898525688052176,\n  1.589750134408474,\n  1.5898958249489465,\n  1.5888650045394896,\n  1.5883209694822629,\n  1.587556079308192,\n  1.5874870212276777,\n  1.5869530142943065,\n  1.586045099357764,\n  1.5852716310620307,\n  1.5856834475000698,\n  1.5852874374787014,\n  1.5852042710582415,\n  1.584652586877346,\n  1.583988606552283,\n  1.5834005736112595,\n  1.5842443289160728,\n  1.5834948438803356],\n [82.01,\n  83.88,\n  84.48,\n  85.0,\n  84.79,\n  85.91,\n  86.16,\n  86.25,\n  86.3,\n  86.33,\n  86.26,\n  86.68,\n  86.8,\n  86.45,\n  86.32,\n  86.64,\n  86.7,\n  86.69,\n  86.59,\n  86.53,\n  86.82,\n  86.75,\n  86.85,\n  86.92,\n  86.59,\n  86.6,\n  86.76,\n  86.75,\n  86.89,\n  86.93],\n [0.6612975722271155,\n  0.4825186188622237,\n  0.4257589255382276,\n  0.3679266643657933,\n  0.3423927102114697,\n  0.33856970327806374,\n  0.319497850643979,\n  0.3121458507727982,\n  0.27430253130167115,\n  0.2719803816841343,\n  0.26749333685991783,\n  0.253242492902964,\n  0.25257572787322347,\n  0.24242354910524155,\n  0.25188813617398464,\n  0.23406134189437208,\n  0.24003111226325127,\n  0.22851929347434635,\n  0.2330922658906635,\n  0.23364725782050796,\n  0.21033703011161048,\n  0.20976593265373936,\n  0.19579574871172842,\n  0.21631469806073828,\n  0.20846173795101944,\n  0.19312117529631392,\n  0.19305682116398828,\n  0.18378307557012935,\n  0.2003692640993782,\n  0.17794695179657605])\n\n\n\ntrain_dataset\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: ../data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.1307,), std=(0.3081,))\n               Lambda()\n           )\n\n\n\n\n\n# Randomly sample 1000 indices\nsubset_indices = random.sample(range(len(train_dataset)), 30)\n\n# Create a subset\nsampled_dataset = torch.utils.data.Subset(train_dataset, subset_indices)\n\n# Optional: create a DataLoader\nsampled_loader = DataLoader(sampled_dataset, batch_size=64, shuffle=True)\n\n\nnet = Network([784,30, 10])\n\nsol = train(net, train_loader, epochs=30, mini_batch_size=30, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 87.64%\nTest Accuracy: 88.61%\nEpoch 1: Avg loss: 1.0087 | Train Accuracy: 87.64% | Test Accuracy: 88.61% | Grad Norm: 1.5247\nTest Accuracy: 89.81%\nTest Accuracy: 90.33%\nEpoch 2: Avg loss: 0.5053 | Train Accuracy: 89.81% | Test Accuracy: 90.33% | Grad Norm: 1.3321\nTest Accuracy: 90.83%\nTest Accuracy: 91.17%\nEpoch 3: Avg loss: 0.3947 | Train Accuracy: 90.83% | Test Accuracy: 91.17% | Grad Norm: 1.3069\nTest Accuracy: 91.55%\nTest Accuracy: 91.76%\nEpoch 4: Avg loss: 0.3433 | Train Accuracy: 91.55% | Test Accuracy: 91.76% | Grad Norm: 1.2931\nTest Accuracy: 92.14%\nTest Accuracy: 92.19%\nEpoch 5: Avg loss: 0.3116 | Train Accuracy: 92.14% | Test Accuracy: 92.19% | Grad Norm: 1.2774\nTest Accuracy: 92.55%\nTest Accuracy: 92.44%\nEpoch 6: Avg loss: 0.2894 | Train Accuracy: 92.55% | Test Accuracy: 92.44% | Grad Norm: 1.2640\nTest Accuracy: 92.84%\nTest Accuracy: 92.64%\nEpoch 7: Avg loss: 0.2726 | Train Accuracy: 92.84% | Test Accuracy: 92.64% | Grad Norm: 1.2514\nTest Accuracy: 93.20%\nTest Accuracy: 92.96%\nEpoch 8: Avg loss: 0.2589 | Train Accuracy: 93.20% | Test Accuracy: 92.96% | Grad Norm: 1.2383\nTest Accuracy: 93.48%\nTest Accuracy: 93.25%\nEpoch 9: Avg loss: 0.2476 | Train Accuracy: 93.48% | Test Accuracy: 93.25% | Grad Norm: 1.2286\nTest Accuracy: 93.76%\nTest Accuracy: 93.50%\nEpoch 10: Avg loss: 0.2380 | Train Accuracy: 93.76% | Test Accuracy: 93.50% | Grad Norm: 1.2188\nTest Accuracy: 93.97%\nTest Accuracy: 93.68%\nEpoch 11: Avg loss: 0.2295 | Train Accuracy: 93.97% | Test Accuracy: 93.68% | Grad Norm: 1.2042\nTest Accuracy: 94.14%\nTest Accuracy: 93.90%\nEpoch 12: Avg loss: 0.2221 | Train Accuracy: 94.14% | Test Accuracy: 93.90% | Grad Norm: 1.1993\nTest Accuracy: 94.34%\nTest Accuracy: 93.99%\nEpoch 13: Avg loss: 0.2154 | Train Accuracy: 94.34% | Test Accuracy: 93.99% | Grad Norm: 1.1874\nTest Accuracy: 94.52%\nTest Accuracy: 94.10%\nEpoch 14: Avg loss: 0.2093 | Train Accuracy: 94.52% | Test Accuracy: 94.10% | Grad Norm: 1.1842\nTest Accuracy: 94.57%\nTest Accuracy: 94.21%\nEpoch 15: Avg loss: 0.2038 | Train Accuracy: 94.57% | Test Accuracy: 94.21% | Grad Norm: 1.1716\nTest Accuracy: 94.73%\nTest Accuracy: 94.26%\nEpoch 16: Avg loss: 0.1988 | Train Accuracy: 94.73% | Test Accuracy: 94.26% | Grad Norm: 1.1638\nTest Accuracy: 94.84%\nTest Accuracy: 94.33%\nEpoch 17: Avg loss: 0.1941 | Train Accuracy: 94.84% | Test Accuracy: 94.33% | Grad Norm: 1.1590\nTest Accuracy: 94.95%\nTest Accuracy: 94.43%\nEpoch 18: Avg loss: 0.1898 | Train Accuracy: 94.95% | Test Accuracy: 94.43% | Grad Norm: 1.1520\nTest Accuracy: 95.10%\nTest Accuracy: 94.48%\nEpoch 19: Avg loss: 0.1858 | Train Accuracy: 95.10% | Test Accuracy: 94.48% | Grad Norm: 1.1488\nTest Accuracy: 95.17%\nTest Accuracy: 94.50%\nEpoch 20: Avg loss: 0.1819 | Train Accuracy: 95.17% | Test Accuracy: 94.50% | Grad Norm: 1.1362\nTest Accuracy: 95.28%\nTest Accuracy: 94.66%\nEpoch 21: Avg loss: 0.1783 | Train Accuracy: 95.28% | Test Accuracy: 94.66% | Grad Norm: 1.1323\nTest Accuracy: 95.34%\nTest Accuracy: 94.57%\nEpoch 22: Avg loss: 0.1749 | Train Accuracy: 95.34% | Test Accuracy: 94.57% | Grad Norm: 1.1270\nTest Accuracy: 95.40%\nTest Accuracy: 94.74%\nEpoch 23: Avg loss: 0.1718 | Train Accuracy: 95.40% | Test Accuracy: 94.74% | Grad Norm: 1.1248\nTest Accuracy: 95.51%\nTest Accuracy: 94.82%\nEpoch 24: Avg loss: 0.1687 | Train Accuracy: 95.51% | Test Accuracy: 94.82% | Grad Norm: 1.1160\nTest Accuracy: 95.55%\nTest Accuracy: 94.91%\nEpoch 25: Avg loss: 0.1658 | Train Accuracy: 95.55% | Test Accuracy: 94.91% | Grad Norm: 1.1107\nTest Accuracy: 95.62%\nTest Accuracy: 95.03%\nEpoch 26: Avg loss: 0.1629 | Train Accuracy: 95.62% | Test Accuracy: 95.03% | Grad Norm: 1.1076\nTest Accuracy: 95.71%\nTest Accuracy: 94.97%\nEpoch 27: Avg loss: 0.1603 | Train Accuracy: 95.71% | Test Accuracy: 94.97% | Grad Norm: 1.0958\nTest Accuracy: 95.76%\nTest Accuracy: 95.05%\nEpoch 28: Avg loss: 0.1578 | Train Accuracy: 95.76% | Test Accuracy: 95.05% | Grad Norm: 1.0917\nTest Accuracy: 95.86%\nTest Accuracy: 95.13%\nEpoch 29: Avg loss: 0.1554 | Train Accuracy: 95.86% | Test Accuracy: 95.13% | Grad Norm: 1.0925\nTest Accuracy: 95.91%\nTest Accuracy: 95.21%\nEpoch 30: Avg loss: 0.1531 | Train Accuracy: 95.91% | Test Accuracy: 95.21% | Grad Norm: 1.0858\n\n\n\na = train(net, train_loader, epochs=1, mini_batch_size=10, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 95.72%\nEpoch 1: Avg loss: 1.4899 | Test Accuracy: 95.72% | Grad Norm: 0.0397\n\n\n\n# Deep with no scheduler\n\nnet = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=.0001, test_data=test_loader)\n\nTest Accuracy: 19.14%\nEpoch 1: Avg loss: 2.2448 | Test Accuracy: 19.14% | Grad Norm: 0.1899\nTest Accuracy: 20.76%\nEpoch 2: Avg loss: 2.2204 | Test Accuracy: 20.76% | Grad Norm: 0.2420\nTest Accuracy: 20.82%\nEpoch 3: Avg loss: 2.2190 | Test Accuracy: 20.82% | Grad Norm: 0.2481\nTest Accuracy: 20.55%\nEpoch 4: Avg loss: 2.2202 | Test Accuracy: 20.55% | Grad Norm: 0.2541\nTest Accuracy: 20.71%\nEpoch 5: Avg loss: 2.2211 | Test Accuracy: 20.71% | Grad Norm: 0.2493\nTest Accuracy: 20.59%\nEpoch 6: Avg loss: 2.2194 | Test Accuracy: 20.59% | Grad Norm: 0.2303\nTest Accuracy: 20.58%\nEpoch 7: Avg loss: 2.2195 | Test Accuracy: 20.58% | Grad Norm: 0.2610\nTest Accuracy: 20.75%\nEpoch 8: Avg loss: 2.2185 | Test Accuracy: 20.75% | Grad Norm: 0.2452\nTest Accuracy: 20.85%\nEpoch 9: Avg loss: 2.2180 | Test Accuracy: 20.85% | Grad Norm: 0.2495\nTest Accuracy: 20.73%\nEpoch 10: Avg loss: 2.2177 | Test Accuracy: 20.73% | Grad Norm: 0.2601\nTest Accuracy: 20.72%\nEpoch 11: Avg loss: 2.2167 | Test Accuracy: 20.72% | Grad Norm: 0.2380\nTest Accuracy: 20.69%\nEpoch 12: Avg loss: 2.2199 | Test Accuracy: 20.69% | Grad Norm: 0.2578\nTest Accuracy: 20.85%\nEpoch 13: Avg loss: 2.2165 | Test Accuracy: 20.85% | Grad Norm: 0.2404\nTest Accuracy: 20.35%\nEpoch 14: Avg loss: 2.2180 | Test Accuracy: 20.35% | Grad Norm: 0.2589\nTest Accuracy: 20.76%\nEpoch 15: Avg loss: 2.2197 | Test Accuracy: 20.76% | Grad Norm: 0.2379\nTest Accuracy: 19.34%\nEpoch 16: Avg loss: 2.2183 | Test Accuracy: 19.34% | Grad Norm: 0.2312\nTest Accuracy: 20.56%\nEpoch 17: Avg loss: 2.2181 | Test Accuracy: 20.56% | Grad Norm: 0.2400\nTest Accuracy: 20.88%\nEpoch 18: Avg loss: 2.2184 | Test Accuracy: 20.88% | Grad Norm: 0.2429\nTest Accuracy: 20.82%\nEpoch 19: Avg loss: 2.2176 | Test Accuracy: 20.82% | Grad Norm: 0.2403\nTest Accuracy: 21.05%\nEpoch 20: Avg loss: 2.2173 | Test Accuracy: 21.05% | Grad Norm: 0.2394\nTest Accuracy: 20.94%\nEpoch 21: Avg loss: 2.2172 | Test Accuracy: 20.94% | Grad Norm: 0.2712\nTest Accuracy: 21.05%\nEpoch 22: Avg loss: 2.2172 | Test Accuracy: 21.05% | Grad Norm: 0.2667\nTest Accuracy: 20.87%\nEpoch 23: Avg loss: 2.2167 | Test Accuracy: 20.87% | Grad Norm: 0.2310\nTest Accuracy: 18.87%\nEpoch 24: Avg loss: 2.2192 | Test Accuracy: 18.87% | Grad Norm: 0.2360\nTest Accuracy: 20.83%\nEpoch 25: Avg loss: 2.2201 | Test Accuracy: 20.83% | Grad Norm: 0.2416\nTest Accuracy: 21.00%\nEpoch 26: Avg loss: 2.2179 | Test Accuracy: 21.00% | Grad Norm: 0.2335\nTest Accuracy: 20.91%\nEpoch 27: Avg loss: 2.2171 | Test Accuracy: 20.91% | Grad Norm: 0.2453\nTest Accuracy: 20.67%\nEpoch 28: Avg loss: 2.2185 | Test Accuracy: 20.67% | Grad Norm: 0.2312\nTest Accuracy: 20.76%\nEpoch 29: Avg loss: 2.2222 | Test Accuracy: 20.76% | Grad Norm: 0.2492\nTest Accuracy: 20.96%\nEpoch 30: Avg loss: 2.2202 | Test Accuracy: 20.96% | Grad Norm: 0.2301\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.scatter(sol[0],sol[1])\n\n\n\n\n\n\n\n\n\n# Deep with no scheduler\n\nnet = Network([784, 30,30 , 30,30,30,30,30,30,30, 10])\nsol = train(net, train_loader, epochs=5, mini_batch_size=100, eta=.0001, test_data=test_loader)\n\nTest Accuracy: 20.69%\nEpoch 1: Avg loss: 2.2873 | Test Accuracy: 20.69% | Grad Norm: 0.1122\nTest Accuracy: 21.27%\nEpoch 2: Avg loss: 2.2179 | Test Accuracy: 21.27% | Grad Norm: 0.1602\nTest Accuracy: 20.98%\nEpoch 3: Avg loss: 2.2077 | Test Accuracy: 20.98% | Grad Norm: 0.1356\nTest Accuracy: 21.06%\nEpoch 4: Avg loss: 2.2051 | Test Accuracy: 21.06% | Grad Norm: 0.1389\nTest Accuracy: 21.13%\nEpoch 5: Avg loss: 2.2044 | Test Accuracy: 21.13% | Grad Norm: 0.1308\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 2.9409675335045904e-05),\n ('layers.0.bias', 9.542326324663009e-07),\n ('layers.1.weight', 2.6448502467246726e-05),\n ('layers.1.bias', 1.7008404711305047e-06),\n ('layers.2.weight', 3.9246409869519994e-05),\n ('layers.2.bias', 2.855350885511143e-06),\n ('layers.3.weight', 3.987160380347632e-05),\n ('layers.3.bias', 4.101609192730393e-06),\n ('layers.4.weight', 7.801799802109599e-05),\n ('layers.4.bias', 7.812871444912162e-06),\n ('layers.5.weight', 0.00011470988829387352),\n ('layers.5.bias', 1.4928327800589614e-05),\n ('layers.6.weight', 0.00048420310486108065),\n ('layers.6.bias', 6.47182168904692e-05),\n ('layers.7.weight', 0.004019377753138542),\n ('layers.7.bias', 0.0007568754372186959),\n ('layers.8.weight', 0.04133795574307442),\n ('layers.8.bias', 0.008317744359374046),\n ('layers.9.weight', 0.14119262993335724),\n ('layers.9.bias', 0.04371275007724762)]\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 2.9409675335045904e-05),\n ('layers.0.bias', 9.542326324663009e-07),\n ('layers.1.weight', 2.6448502467246726e-05),\n ('layers.1.bias', 1.7008404711305047e-06),\n ('layers.2.weight', 3.9246409869519994e-05),\n ('layers.2.bias', 2.855350885511143e-06),\n ('layers.3.weight', 3.987160380347632e-05),\n ('layers.3.bias', 4.101609192730393e-06),\n ('layers.4.weight', 7.801799802109599e-05),\n ('layers.4.bias', 7.812871444912162e-06),\n ('layers.5.weight', 0.00011470988829387352),\n ('layers.5.bias', 1.4928327800589614e-05),\n ('layers.6.weight', 0.00048420310486108065),\n ('layers.6.bias', 6.47182168904692e-05),\n ('layers.7.weight', 0.004019377753138542),\n ('layers.7.bias', 0.0007568754372186959),\n ('layers.8.weight', 0.04133795574307442),\n ('layers.8.bias', 0.008317744359374046),\n ('layers.9.weight', 0.14119262993335724),\n ('layers.9.bias', 0.04371275007724762)]\n\n\n\nnet = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\nnet.apply(net.init_weights())\ncompute_layer_gradient_norms(net)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[75], line 2\n      1 net = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\n----&gt; 2 net.apply(net.init_weights())\n      3 compute_layer_gradient_norms(net)\n\nTypeError: Network.init_weights() missing 1 required positional argument: 'm'\n\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 4.1792472416091186e-07),\n ('layers.0.bias', 1.570747443224718e-08),\n ('layers.1.weight', 6.034351685002548e-08),\n ('layers.1.bias', 6.662162821413631e-09),\n ('layers.2.weight', 5.6365973222227694e-08),\n ('layers.2.bias', 5.440007555534976e-09),\n ('layers.3.weight', 3.217432151814137e-07),\n ('layers.3.bias', 2.961611578200518e-08),\n ('layers.4.weight', 3.928074875148013e-06),\n ('layers.4.bias', 3.470892409040971e-07),\n ('layers.5.weight', 3.216185359633528e-05),\n ('layers.5.bias', 4.11073870054679e-06),\n ('layers.6.weight', 0.0002945692394860089),\n ('layers.6.bias', 3.7662866816390306e-05),\n ('layers.7.weight', 0.0032506876159459352),\n ('layers.7.bias', 0.0005663582705892622),\n ('layers.8.weight', 0.03259878605604172),\n ('layers.8.bias', 0.006158514879643917),\n ('layers.9.weight', 0.16322623193264008),\n ('layers.9.bias', 0.044430315494537354)]\n\n\n\ndef compute_layer_gradient_norms(model):\n    \"\"\"\n    \n    \"\"\"\n    layer_grad_norms = []\n\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.data.norm(2).item()\n            layer_grad_norms.append((name, grad_norm))\n        else:\n            layer_grad_norms.append((name, 0.0))  # No gradient yet\n\n    return layer_grad_norms\n\nnet.named_parameters\n\n&lt;bound method Module.named_parameters of Network(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): Linear(in_features=512, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=256, bias=True)\n    (4): Linear(in_features=256, out_features=128, bias=True)\n    (5): Linear(in_features=128, out_features=128, bias=True)\n    (6): Linear(in_features=128, out_features=64, bias=True)\n    (7): Linear(in_features=64, out_features=64, bias=True)\n    (8): Linear(in_features=64, out_features=32, bias=True)\n    (9): Linear(in_features=32, out_features=10, bias=True)\n  )\n)&gt;\n\n\n\nnet.named_parameters\n\n\n# Deep with scheduler\n\nnet = Network([784, 512,512 , 256, 256,128,128,64,64,32, 10])\nsol = train(net, train_loader, epochs=30, mini_batch_size=100, eta=.0003, test_data=test_loader)\n\n\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 97.19%\nEpoch 1: Avg loss: 1.4700 | Test Accuracy: 97.19% | Grad Norm: 0.1536\nTest Accuracy: 97.08%\nEpoch 2: Avg loss: 1.4697 | Test Accuracy: 97.08% | Grad Norm: 0.0736\nTest Accuracy: 97.11%\nEpoch 3: Avg loss: 1.4697 | Test Accuracy: 97.11% | Grad Norm: 0.0705\nTest Accuracy: 97.12%\nEpoch 4: Avg loss: 1.4696 | Test Accuracy: 97.12% | Grad Norm: 0.0640\nTest Accuracy: 97.06%\nEpoch 5: Avg loss: 1.4695 | Test Accuracy: 97.06% | Grad Norm: 0.0607\nTest Accuracy: 97.22%\nEpoch 6: Avg loss: 1.4695 | Test Accuracy: 97.22% | Grad Norm: 0.0463\nTest Accuracy: 97.25%\nEpoch 7: Avg loss: 1.4694 | Test Accuracy: 97.25% | Grad Norm: 0.0302\nTest Accuracy: 97.14%\nEpoch 8: Avg loss: 1.4694 | Test Accuracy: 97.14% | Grad Norm: 0.0297\nTest Accuracy: 97.21%\nEpoch 9: Avg loss: 1.4694 | Test Accuracy: 97.21% | Grad Norm: 0.0377\nTest Accuracy: 97.17%\nEpoch 10: Avg loss: 1.4693 | Test Accuracy: 97.17% | Grad Norm: 0.0362\nTest Accuracy: 97.08%\nEpoch 11: Avg loss: 1.4693 | Test Accuracy: 97.08% | Grad Norm: 0.0389\nTest Accuracy: 97.09%\nEpoch 12: Avg loss: 1.4692 | Test Accuracy: 97.09% | Grad Norm: 0.0324\nTest Accuracy: 97.17%\nEpoch 13: Avg loss: 1.4692 | Test Accuracy: 97.17% | Grad Norm: 0.0317\nTest Accuracy: 97.17%\nEpoch 14: Avg loss: 1.4692 | Test Accuracy: 97.17% | Grad Norm: 0.0312\nTest Accuracy: 97.19%\nEpoch 15: Avg loss: 1.4692 | Test Accuracy: 97.19% | Grad Norm: 0.0297\nTest Accuracy: 97.18%\nEpoch 16: Avg loss: 1.4691 | Test Accuracy: 97.18% | Grad Norm: 0.0294\nTest Accuracy: 97.19%\nEpoch 17: Avg loss: 1.4691 | Test Accuracy: 97.19% | Grad Norm: 0.0314\nTest Accuracy: 97.18%\nEpoch 18: Avg loss: 1.4691 | Test Accuracy: 97.18% | Grad Norm: 0.0333\nTest Accuracy: 97.14%\nEpoch 19: Avg loss: 1.4691 | Test Accuracy: 97.14% | Grad Norm: 0.0454\nTest Accuracy: 97.13%\nEpoch 20: Avg loss: 1.4690 | Test Accuracy: 97.13% | Grad Norm: 0.0368\nTest Accuracy: 97.14%\nEpoch 21: Avg loss: 1.4690 | Test Accuracy: 97.14% | Grad Norm: 0.0356\nTest Accuracy: 97.18%\nEpoch 22: Avg loss: 1.4690 | Test Accuracy: 97.18% | Grad Norm: 0.0496\nTest Accuracy: 97.24%\nEpoch 23: Avg loss: 1.4689 | Test Accuracy: 97.24% | Grad Norm: 0.0441\nTest Accuracy: 97.20%\nEpoch 24: Avg loss: 1.4689 | Test Accuracy: 97.20% | Grad Norm: 0.0346\nTest Accuracy: 97.16%\nEpoch 25: Avg loss: 1.4688 | Test Accuracy: 97.16% | Grad Norm: 0.0304\nTest Accuracy: 97.19%\nEpoch 26: Avg loss: 1.4688 | Test Accuracy: 97.19% | Grad Norm: 0.0300\nTest Accuracy: 97.19%\nEpoch 27: Avg loss: 1.4688 | Test Accuracy: 97.19% | Grad Norm: 0.0303\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[46], line 1\n----&gt; 1 train(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nCell In[42], line 56, in train(network, train_data, epochs, mini_batch_size, eta, test_data)\n     53 total_grad_norm = 0.0\n     55 for data, target in train_data:\n---&gt; 56     optimizer.zero_grad()\n     57     output = network(data)\n     58     loss = loss_fn(output, target)\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/_compile.py:32, in _disable_dynamo.&lt;locals&gt;.inner(*args, **kwargs)\n     29     disable_fn = torch._dynamo.disable(fn, recursive)\n     30     fn.__dynamo_disable = disable_fn\n---&gt; 32 return disable_fn(*args, **kwargs)\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632, in DisableContext.__call__.&lt;locals&gt;._fn(*args, **kwargs)\n    630 prior = _maybe_set_eval_frame(callback)\n    631 try:\n--&gt; 632     return fn(*args, **kwargs)\n    633 finally:\n    634     _maybe_set_eval_frame(prior)\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/optim/optimizer.py:947, in Optimizer.zero_grad(self, set_to_none)\n    944 else:\n    945     per_device_and_dtype_grads = None\n--&gt; 947 with torch.autograd.profiler.record_function(self._zero_grad_profile_name):\n    948     for group in self.param_groups:\n    949         for p in group[\"params\"]:\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/autograd/profiler.py:733, in record_function.__enter__(self)\n    732 def __enter__(self):\n--&gt; 733     self.record = torch.ops.profiler._record_function_enter_new(\n    734         self.name, self.args\n    735     )\n    736     return self\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/_ops.py:1116, in OpOverloadPacket.__call__(self, *args, **kwargs)\n   1114 if self._has_torchbind_op_overload and _must_dispatch_in_python(args, kwargs):\n   1115     return _call_overload_packet_from_python(self, args, kwargs)\n-&gt; 1116 return self._op(*args, **(kwargs or {}))\n\nKeyboardInterrupt: \n\n\n\n\n0.7**30\n\n2.2539340290692216e-05\n\n\n\n\n# Initialize network\nnet = Network([784, 480,240 , 120, 60,40,20, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 66.49%\nEpoch 1: Avg loss: 1.8895 | Test Accuracy: 66.49% | Grad Norm: 3.5853\nTest Accuracy: 80.60%\nEpoch 2: Avg loss: 1.7731 | Test Accuracy: 80.60% | Grad Norm: 2.6630\nTest Accuracy: 83.95%\nEpoch 3: Avg loss: 1.6358 | Test Accuracy: 83.95% | Grad Norm: 2.9567\nTest Accuracy: 85.07%\nEpoch 4: Avg loss: 1.6142 | Test Accuracy: 85.07% | Grad Norm: 2.2482\nTest Accuracy: 85.09%\nEpoch 5: Avg loss: 1.6047 | Test Accuracy: 85.09% | Grad Norm: 1.9975\nTest Accuracy: 86.09%\nEpoch 6: Avg loss: 1.5981 | Test Accuracy: 86.09% | Grad Norm: 1.7313\nTest Accuracy: 85.83%\nEpoch 7: Avg loss: 1.5934 | Test Accuracy: 85.83% | Grad Norm: 1.6062\nTest Accuracy: 86.33%\nEpoch 8: Avg loss: 1.5892 | Test Accuracy: 86.33% | Grad Norm: 1.3630\nTest Accuracy: 86.52%\nEpoch 9: Avg loss: 1.5862 | Test Accuracy: 86.52% | Grad Norm: 1.3188\nTest Accuracy: 86.19%\nEpoch 10: Avg loss: 1.5840 | Test Accuracy: 86.19% | Grad Norm: 1.1467\nTest Accuracy: 86.52%\nEpoch 11: Avg loss: 1.5820 | Test Accuracy: 86.52% | Grad Norm: 1.1071\nTest Accuracy: 86.68%\nEpoch 12: Avg loss: 1.5796 | Test Accuracy: 86.68% | Grad Norm: 0.9997\nTest Accuracy: 86.89%\nEpoch 13: Avg loss: 1.5782 | Test Accuracy: 86.89% | Grad Norm: 0.9310\nTest Accuracy: 86.86%\nEpoch 14: Avg loss: 1.5767 | Test Accuracy: 86.86% | Grad Norm: 0.8280\nTest Accuracy: 86.79%\nEpoch 15: Avg loss: 1.5755 | Test Accuracy: 86.79% | Grad Norm: 0.7814\nTest Accuracy: 86.95%\nEpoch 16: Avg loss: 1.5742 | Test Accuracy: 86.95% | Grad Norm: 0.6607\nTest Accuracy: 86.94%\nEpoch 17: Avg loss: 1.5734 | Test Accuracy: 86.94% | Grad Norm: 0.6174\nTest Accuracy: 86.49%\nEpoch 18: Avg loss: 1.5727 | Test Accuracy: 86.49% | Grad Norm: 0.6193\nTest Accuracy: 87.05%\nEpoch 19: Avg loss: 1.5720 | Test Accuracy: 87.05% | Grad Norm: 0.5320\nTest Accuracy: 87.07%\nEpoch 20: Avg loss: 1.5713 | Test Accuracy: 87.07% | Grad Norm: 0.4691\nTest Accuracy: 87.03%\nEpoch 21: Avg loss: 1.5709 | Test Accuracy: 87.03% | Grad Norm: 0.4334\nTest Accuracy: 86.80%\nEpoch 22: Avg loss: 1.5704 | Test Accuracy: 86.80% | Grad Norm: 0.4164\nTest Accuracy: 87.13%\nEpoch 23: Avg loss: 1.5700 | Test Accuracy: 87.13% | Grad Norm: 0.4170\nTest Accuracy: 87.08%\nEpoch 24: Avg loss: 1.5697 | Test Accuracy: 87.08% | Grad Norm: 0.4113\nTest Accuracy: 86.88%\nEpoch 25: Avg loss: 1.5693 | Test Accuracy: 86.88% | Grad Norm: 0.3963\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[43], line 5\n      2 net = Network([784, 480,240 , 120, 60,40,20, 10])\n      4 # Train\n----&gt; 5 train(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nCell In[42], line 57, in train(network, train_data, epochs, mini_batch_size, eta, test_data)\n     55 for data, target in train_data:\n     56     optimizer.zero_grad()\n---&gt; 57     output = network(data)\n     58     loss = loss_fn(output, target)\n     59     loss.backward()\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nCell In[42], line 23, in Network.forward(self, x)\n     21 for layer in self.layers[:-1]:\n     22     x = F.relu(layer(x))   # hidden layers \n---&gt; 23 x = F.softmax(self.layers[-1](x), dim=1)  # softmax output \n     24 return x\n\nFile ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/nn/functional.py:2140, in softmax(input, dim, _stacklevel, dtype)\n   2138     dim = _get_softmax_dim(\"softmax\", input.dim(), _stacklevel)\n   2139 if dtype is None:\n-&gt; 2140     ret = input.softmax(dim)\n   2141 else:\n   2142     ret = input.softmax(dim, dtype=dtype)\n\nKeyboardInterrupt: \n\n\n\n\ncompute_layer_gradient_norms(net)\n\n[('layers.0.weight', 0.0),\n ('layers.0.bias', 0.0),\n ('layers.1.weight', 0.0),\n ('layers.1.bias', 0.0),\n ('layers.2.weight', 0.0),\n ('layers.2.bias', 0.0),\n ('layers.3.weight', 0.0),\n ('layers.3.bias', 0.0),\n ('layers.4.weight', 0.0),\n ('layers.4.bias', 0.0),\n ('layers.5.weight', 0.0),\n ('layers.5.bias', 0.0),\n ('layers.6.weight', 0.0),\n ('layers.6.bias', 0.0),\n ('layers.7.weight', 0.0),\n ('layers.7.bias', 0.0),\n ('layers.8.weight', 0.0),\n ('layers.8.bias', 0.0),\n ('layers.9.weight', 0.0),\n ('layers.9.bias', 0.0)]\n\n\n\n\n# Initialize network\nnet = Network([784, 30, 30, 30,30,30, 10])\n\n# Train\ntrain(net, train_loader, epochs=30, mini_batch_size=10, eta=.001, test_data=test_loader)\n\nTest Accuracy: 49.17%\nEpoch 1: Avg loss: 2.1066 | Test Accuracy: 49.17% | Grad Norm: 0.1545\nTest Accuracy: 76.22%\nEpoch 2: Avg loss: 1.8757 | Test Accuracy: 76.22% | Grad Norm: 0.2665\nTest Accuracy: 82.14%\nEpoch 3: Avg loss: 1.6619 | Test Accuracy: 82.14% | Grad Norm: 0.4257\nTest Accuracy: 83.09%\nEpoch 4: Avg loss: 1.6347 | Test Accuracy: 83.09% | Grad Norm: 0.3923\nTest Accuracy: 82.23%\nEpoch 5: Avg loss: 1.6267 | Test Accuracy: 82.23% | Grad Norm: 0.3576\nTest Accuracy: 83.28%\nEpoch 6: Avg loss: 1.6234 | Test Accuracy: 83.28% | Grad Norm: 0.3438\nTest Accuracy: 90.66%\nEpoch 7: Avg loss: 1.5791 | Test Accuracy: 90.66% | Grad Norm: 0.5007\nTest Accuracy: 90.69%\nEpoch 8: Avg loss: 1.5519 | Test Accuracy: 90.69% | Grad Norm: 0.4530\nTest Accuracy: 91.90%\nEpoch 9: Avg loss: 1.5448 | Test Accuracy: 91.90% | Grad Norm: 0.3948\nTest Accuracy: 91.57%\nEpoch 10: Avg loss: 1.5409 | Test Accuracy: 91.57% | Grad Norm: 0.3736\nTest Accuracy: 91.62%\nEpoch 11: Avg loss: 1.5375 | Test Accuracy: 91.62% | Grad Norm: 0.3566\nTest Accuracy: 92.43%\nEpoch 12: Avg loss: 1.5346 | Test Accuracy: 92.43% | Grad Norm: 0.3450\nTest Accuracy: 92.06%\nEpoch 13: Avg loss: 1.5316 | Test Accuracy: 92.06% | Grad Norm: 0.3280\nTest Accuracy: 92.81%\nEpoch 14: Avg loss: 1.5301 | Test Accuracy: 92.81% | Grad Norm: 0.3218\nTest Accuracy: 92.70%\nEpoch 15: Avg loss: 1.5294 | Test Accuracy: 92.70% | Grad Norm: 0.3111\nTest Accuracy: 93.04%\nEpoch 16: Avg loss: 1.5256 | Test Accuracy: 93.04% | Grad Norm: 0.2890\nTest Accuracy: 92.72%\nEpoch 17: Avg loss: 1.5263 | Test Accuracy: 92.72% | Grad Norm: 0.2699\nTest Accuracy: 91.79%\nEpoch 18: Avg loss: 1.5240 | Test Accuracy: 91.79% | Grad Norm: 0.2641\nTest Accuracy: 92.99%\nEpoch 19: Avg loss: 1.5234 | Test Accuracy: 92.99% | Grad Norm: 0.2644\nTest Accuracy: 93.35%\nEpoch 20: Avg loss: 1.5215 | Test Accuracy: 93.35% | Grad Norm: 0.2663\nTest Accuracy: 93.21%\nEpoch 21: Avg loss: 1.5198 | Test Accuracy: 93.21% | Grad Norm: 0.2443\nTest Accuracy: 93.46%\nEpoch 22: Avg loss: 1.5203 | Test Accuracy: 93.46% | Grad Norm: 0.2640\nTest Accuracy: 93.24%\nEpoch 23: Avg loss: 1.5185 | Test Accuracy: 93.24% | Grad Norm: 0.2449\nTest Accuracy: 93.71%\nEpoch 24: Avg loss: 1.5182 | Test Accuracy: 93.71% | Grad Norm: 0.2318\nTest Accuracy: 93.58%\nEpoch 25: Avg loss: 1.5166 | Test Accuracy: 93.58% | Grad Norm: 0.2341\nTest Accuracy: 93.55%\nEpoch 26: Avg loss: 1.5141 | Test Accuracy: 93.55% | Grad Norm: 0.2143\nTest Accuracy: 93.83%\nEpoch 27: Avg loss: 1.5148 | Test Accuracy: 93.83% | Grad Norm: 0.2336\nTest Accuracy: 94.05%\nEpoch 28: Avg loss: 1.5141 | Test Accuracy: 94.05% | Grad Norm: 0.2333\nTest Accuracy: 94.07%\nEpoch 29: Avg loss: 1.5122 | Test Accuracy: 94.07% | Grad Norm: 0.2159\nTest Accuracy: 93.86%\nEpoch 30: Avg loss: 1.5113 | Test Accuracy: 93.86% | Grad Norm: 0.2133\n\n\n([2.106609010855357,\n  1.8757023508548736,\n  1.6619241003990173,\n  1.6347059144179026,\n  1.6267493901252748,\n  1.6233574739098549,\n  1.5790614494681359,\n  1.5519095999995867,\n  1.544799600203832,\n  1.5408548574646315,\n  1.5375095768968263,\n  1.5346180646220844,\n  1.5316300688385964,\n  1.530076232691606,\n  1.5293561617334683,\n  1.5256369227170945,\n  1.5263271143436432,\n  1.5240207088192304,\n  1.5233585479458174,\n  1.5214697729150455,\n  1.5198356384634972,\n  1.5202613272070884,\n  1.5184814278682073,\n  1.5181590902805329,\n  1.5165831561485927,\n  1.5140590472221374,\n  1.5147702373464902,\n  1.5141058943669001,\n  1.5122183973987897,\n  1.511292640129725],\n [49.17,\n  76.22,\n  82.14,\n  83.09,\n  82.23,\n  83.28,\n  90.66,\n  90.69,\n  91.9,\n  91.57,\n  91.62,\n  92.43,\n  92.06,\n  92.81,\n  92.7,\n  93.04,\n  92.72,\n  91.79,\n  92.99,\n  93.35,\n  93.21,\n  93.46,\n  93.24,\n  93.71,\n  93.58,\n  93.55,\n  93.83,\n  94.05,\n  94.07,\n  93.86],\n [0.1545203353925608,\n  0.26648143015344006,\n  0.4256724650240503,\n  0.39230799699549485,\n  0.3575932351596421,\n  0.3438080019064364,\n  0.5007331380928711,\n  0.45298709018271377,\n  0.3948249351742221,\n  0.3736201974570067,\n  0.3565940285629549,\n  0.34503243407932555,\n  0.3279822464524295,\n  0.321849082351473,\n  0.31105692698469406,\n  0.2889891259993431,\n  0.2698816749692742,\n  0.26406108346089663,\n  0.26441012255252766,\n  0.2662807202279061,\n  0.244252311765028,\n  0.26402584721809275,\n  0.24487796221406294,\n  0.23184617290994053,\n  0.23406889448768592,\n  0.21428815829061432,\n  0.23363848274583932,\n  0.23327912711403648,\n  0.21594139513708782,\n  0.2133217869038012])\n\n\n\ntrain(net, train_loader, epochs=3, mini_batch_size=10, eta=.0001, test_data=test_loader)\n\nTest Accuracy: 94.18%\nEpoch 1: Avg loss: 1.5050 | Test Accuracy: 94.18% | Grad Norm: 0.1529\nTest Accuracy: 94.13%\nEpoch 2: Avg loss: 1.5032 | Test Accuracy: 94.13% | Grad Norm: 0.1523\nTest Accuracy: 94.31%\nEpoch 3: Avg loss: 1.5021 | Test Accuracy: 94.31% | Grad Norm: 0.1398\n\n\n([1.5050347511370976, 1.5031935001413028, 1.5020705436468125],\n [94.18, 94.13, 94.31],\n [0.15294206734796406, 0.15231098094013806, 0.13976386858030038])\n\n\n\ntrain(net, train_loader, epochs=, mini_batch_size=10, eta=.00005, test_data=test_loader)\n\n\n# Initialize network\nnet = Network([784,30, 10])\n\n# Train\ntrain(net, train_loader,epochs=30, mini_batch_size=100, eta=0.001, test_data=test_loader)\n\nTest Accuracy: 81.87%\nEpoch 1: Avg loss: 1.7124 | Test Accuracy: 81.87% | Grad Norm: 0.7243\nTest Accuracy: 84.19%\nEpoch 2: Avg loss: 1.6347 | Test Accuracy: 84.19% | Grad Norm: 0.4565\nTest Accuracy: 84.75%\nEpoch 3: Avg loss: 1.6227 | Test Accuracy: 84.75% | Grad Norm: 0.3992\nTest Accuracy: 85.23%\nEpoch 4: Avg loss: 1.6158 | Test Accuracy: 85.23% | Grad Norm: 0.3694\nTest Accuracy: 85.27%\nEpoch 5: Avg loss: 1.6114 | Test Accuracy: 85.27% | Grad Norm: 0.3146\nTest Accuracy: 85.37%\nEpoch 6: Avg loss: 1.6082 | Test Accuracy: 85.37% | Grad Norm: 0.3032\nTest Accuracy: 94.89%\nEpoch 7: Avg loss: 1.5234 | Test Accuracy: 94.89% | Grad Norm: 0.4304\nTest Accuracy: 94.95%\nEpoch 8: Avg loss: 1.5092 | Test Accuracy: 94.95% | Grad Norm: 0.3597\nTest Accuracy: 95.50%\nEpoch 9: Avg loss: 1.5068 | Test Accuracy: 95.50% | Grad Norm: 0.3649\nTest Accuracy: 95.68%\nEpoch 10: Avg loss: 1.5036 | Test Accuracy: 95.68% | Grad Norm: 0.3122\nTest Accuracy: 95.75%\nEpoch 11: Avg loss: 1.5021 | Test Accuracy: 95.75% | Grad Norm: 0.3081\nTest Accuracy: 95.81%\nEpoch 12: Avg loss: 1.5022 | Test Accuracy: 95.81% | Grad Norm: 0.2850\nTest Accuracy: 95.61%\nEpoch 13: Avg loss: 1.4999 | Test Accuracy: 95.61% | Grad Norm: 0.2800\nTest Accuracy: 96.11%\nEpoch 14: Avg loss: 1.4995 | Test Accuracy: 96.11% | Grad Norm: 0.2886\nTest Accuracy: 96.00%\nEpoch 15: Avg loss: 1.4981 | Test Accuracy: 96.00% | Grad Norm: 0.2755\nTest Accuracy: 95.70%\nEpoch 16: Avg loss: 1.4973 | Test Accuracy: 95.70% | Grad Norm: 0.2705\nTest Accuracy: 96.15%\nEpoch 17: Avg loss: 1.4964 | Test Accuracy: 96.15% | Grad Norm: 0.2651\nTest Accuracy: 96.16%\nEpoch 18: Avg loss: 1.4955 | Test Accuracy: 96.16% | Grad Norm: 0.2590\nTest Accuracy: 96.24%\nEpoch 19: Avg loss: 1.4949 | Test Accuracy: 96.24% | Grad Norm: 0.2658\nTest Accuracy: 95.53%\nEpoch 20: Avg loss: 1.4943 | Test Accuracy: 95.53% | Grad Norm: 0.2571\nTest Accuracy: 96.19%\nEpoch 21: Avg loss: 1.4942 | Test Accuracy: 96.19% | Grad Norm: 0.2515\nTest Accuracy: 96.04%\nEpoch 22: Avg loss: 1.4937 | Test Accuracy: 96.04% | Grad Norm: 0.2457\nTest Accuracy: 96.27%\nEpoch 23: Avg loss: 1.4932 | Test Accuracy: 96.27% | Grad Norm: 0.2524\nTest Accuracy: 95.57%\nEpoch 24: Avg loss: 1.4916 | Test Accuracy: 95.57% | Grad Norm: 0.2290\nTest Accuracy: 96.05%\nEpoch 25: Avg loss: 1.4931 | Test Accuracy: 96.05% | Grad Norm: 0.2406\nTest Accuracy: 96.27%\nEpoch 26: Avg loss: 1.4928 | Test Accuracy: 96.27% | Grad Norm: 0.2411\nTest Accuracy: 96.26%\nEpoch 27: Avg loss: 1.4916 | Test Accuracy: 96.26% | Grad Norm: 0.2260\nTest Accuracy: 96.19%\nEpoch 28: Avg loss: 1.4921 | Test Accuracy: 96.19% | Grad Norm: 0.2398\nTest Accuracy: 96.24%\nEpoch 29: Avg loss: 1.4905 | Test Accuracy: 96.24% | Grad Norm: 0.2443\nTest Accuracy: 96.37%\nEpoch 30: Avg loss: 1.4905 | Test Accuracy: 96.37% | Grad Norm: 0.2259\n\n\n([1.712379110833009,\n  1.634699333647887,\n  1.6227173241575559,\n  1.6158263737956682,\n  1.6114202886223794,\n  1.6081893152594566,\n  1.5233851760824522,\n  1.509206545372804,\n  1.5068402435978254,\n  1.5036250775456428,\n  1.502133689125379,\n  1.5022221900224686,\n  1.4999016187985739,\n  1.4994844551086426,\n  1.4980750774939855,\n  1.4973419542312623,\n  1.4964445405602456,\n  1.4954652837514877,\n  1.4949333322842915,\n  1.494347569723924,\n  1.494236315647761,\n  1.49370050462087,\n  1.4932476964990298,\n  1.491597293774287,\n  1.4930901816089948,\n  1.4928456391890843,\n  1.4915773904124896,\n  1.4920755220850308,\n  1.490536659638087,\n  1.4905199672579765],\n [81.87,\n  84.19,\n  84.75,\n  85.23,\n  85.27,\n  85.37,\n  94.89,\n  94.95,\n  95.5,\n  95.68,\n  95.75,\n  95.81,\n  95.61,\n  96.11,\n  96.0,\n  95.7,\n  96.15,\n  96.16,\n  96.24,\n  95.53,\n  96.19,\n  96.04,\n  96.27,\n  95.57,\n  96.05,\n  96.27,\n  96.26,\n  96.19,\n  96.24,\n  96.37],\n [0.7243484067550947,\n  0.45651815972085996,\n  0.399189866863298,\n  0.3693671963394121,\n  0.3146082094254294,\n  0.3031961040602153,\n  0.4304137508635906,\n  0.3596677687820174,\n  0.36491132085475586,\n  0.3121707544426844,\n  0.3080823959714832,\n  0.28502727683696094,\n  0.2799889310267698,\n  0.2886388187179246,\n  0.2754983639435184,\n  0.2704686516169565,\n  0.26509288289372707,\n  0.25901263095473304,\n  0.26583254155572733,\n  0.2570741118824896,\n  0.25148435638281935,\n  0.24566580125881313,\n  0.25236080576859626,\n  0.22904242185308724,\n  0.24059044417438222,\n  0.24110372210341244,\n  0.22600588605100547,\n  0.23976487168889318,\n  0.2443153856269968,\n  0.2259297749290105])\n\n\n\n# Initialize network\nnet = Network([784,30, 10])\n\n# Train\ntrain(net, train_loader,epochs=30, mini_batch_size=100, eta=0.0005, test_data=test_loader)\n\nTest Accuracy: 84.24%\nEpoch 1: Avg loss: 1.6588 | Test Accuracy: 84.24% | Grad Norm: 0.8379\nTest Accuracy: 85.10%\nEpoch 2: Avg loss: 1.6136 | Test Accuracy: 85.10% | Grad Norm: 0.5828\nTest Accuracy: 84.85%\nEpoch 3: Avg loss: 1.6037 | Test Accuracy: 84.85% | Grad Norm: 0.4827\nTest Accuracy: 85.91%\nEpoch 4: Avg loss: 1.5972 | Test Accuracy: 85.91% | Grad Norm: 0.4511\nTest Accuracy: 94.97%\nEpoch 5: Avg loss: 1.5284 | Test Accuracy: 94.97% | Grad Norm: 0.5244\nTest Accuracy: 95.59%\nEpoch 6: Avg loss: 1.5049 | Test Accuracy: 95.59% | Grad Norm: 0.4655\nTest Accuracy: 95.36%\nEpoch 7: Avg loss: 1.5004 | Test Accuracy: 95.36% | Grad Norm: 0.4240\nTest Accuracy: 96.11%\nEpoch 8: Avg loss: 1.4973 | Test Accuracy: 96.11% | Grad Norm: 0.4028\nTest Accuracy: 96.09%\nEpoch 9: Avg loss: 1.4950 | Test Accuracy: 96.09% | Grad Norm: 0.3978\nTest Accuracy: 95.84%\nEpoch 10: Avg loss: 1.4930 | Test Accuracy: 95.84% | Grad Norm: 0.3621\nTest Accuracy: 96.12%\nEpoch 11: Avg loss: 1.4914 | Test Accuracy: 96.12% | Grad Norm: 0.3497\nTest Accuracy: 96.30%\nEpoch 12: Avg loss: 1.4902 | Test Accuracy: 96.30% | Grad Norm: 0.3424\nTest Accuracy: 96.39%\nEpoch 13: Avg loss: 1.4887 | Test Accuracy: 96.39% | Grad Norm: 0.3192\nTest Accuracy: 96.69%\nEpoch 14: Avg loss: 1.4876 | Test Accuracy: 96.69% | Grad Norm: 0.3158\nTest Accuracy: 96.72%\nEpoch 15: Avg loss: 1.4866 | Test Accuracy: 96.72% | Grad Norm: 0.2915\nTest Accuracy: 96.28%\nEpoch 16: Avg loss: 1.4861 | Test Accuracy: 96.28% | Grad Norm: 0.2964\nTest Accuracy: 96.50%\nEpoch 17: Avg loss: 1.4849 | Test Accuracy: 96.50% | Grad Norm: 0.2871\nTest Accuracy: 96.65%\nEpoch 18: Avg loss: 1.4845 | Test Accuracy: 96.65% | Grad Norm: 0.2766\nTest Accuracy: 96.60%\nEpoch 19: Avg loss: 1.4836 | Test Accuracy: 96.60% | Grad Norm: 0.2649\nTest Accuracy: 96.65%\nEpoch 20: Avg loss: 1.4827 | Test Accuracy: 96.65% | Grad Norm: 0.2451\nTest Accuracy: 96.43%\nEpoch 21: Avg loss: 1.4825 | Test Accuracy: 96.43% | Grad Norm: 0.2548\nTest Accuracy: 96.83%\nEpoch 22: Avg loss: 1.4818 | Test Accuracy: 96.83% | Grad Norm: 0.2329\nTest Accuracy: 96.74%\nEpoch 23: Avg loss: 1.4816 | Test Accuracy: 96.74% | Grad Norm: 0.2315\nTest Accuracy: 96.81%\nEpoch 24: Avg loss: 1.4805 | Test Accuracy: 96.81% | Grad Norm: 0.2189\nTest Accuracy: 96.81%\nEpoch 25: Avg loss: 1.4807 | Test Accuracy: 96.81% | Grad Norm: 0.2212\nTest Accuracy: 96.80%\nEpoch 26: Avg loss: 1.4801 | Test Accuracy: 96.80% | Grad Norm: 0.1994\nTest Accuracy: 96.88%\nEpoch 27: Avg loss: 1.4796 | Test Accuracy: 96.88% | Grad Norm: 0.2003\nTest Accuracy: 96.83%\nEpoch 28: Avg loss: 1.4798 | Test Accuracy: 96.83% | Grad Norm: 0.2192\nTest Accuracy: 96.83%\nEpoch 29: Avg loss: 1.4791 | Test Accuracy: 96.83% | Grad Norm: 0.1962\nTest Accuracy: 96.95%\nEpoch 30: Avg loss: 1.4788 | Test Accuracy: 96.95% | Grad Norm: 0.2054\n\n\n([1.6587944337526956,\n  1.6135517328977584,\n  1.6037016983032226,\n  1.5972112274567287,\n  1.5283759133617083,\n  1.5048583151896795,\n  1.500364566922188,\n  1.4973335752685866,\n  1.4950114894707998,\n  1.4930032001336415,\n  1.4914346623222032,\n  1.4901753436525662,\n  1.4887443048556646,\n  1.487556106408437,\n  1.4866306614081064,\n  1.4860525583426158,\n  1.4849071377515792,\n  1.4844598679145178,\n  1.4835793728431066,\n  1.4826580402255058,\n  1.4825361109972,\n  1.481778745194276,\n  1.4815686868429183,\n  1.4805133953889211,\n  1.4807098559538523,\n  1.4800608076850572,\n  1.4796317419807117,\n  1.479806029498577,\n  1.4790916587313017,\n  1.4787971489628156],\n [84.24,\n  85.1,\n  84.85,\n  85.91,\n  94.97,\n  95.59,\n  95.36,\n  96.11,\n  96.09,\n  95.84,\n  96.12,\n  96.3,\n  96.39,\n  96.69,\n  96.72,\n  96.28,\n  96.5,\n  96.65,\n  96.6,\n  96.65,\n  96.43,\n  96.83,\n  96.74,\n  96.81,\n  96.81,\n  96.8,\n  96.88,\n  96.83,\n  96.83,\n  96.95],\n [0.837896753618222,\n  0.5827687637227682,\n  0.4826752601597975,\n  0.4511472644569282,\n  0.5243731860804096,\n  0.46551088325063045,\n  0.4239557685628547,\n  0.4028482485086789,\n  0.397803709560286,\n  0.3620867673378315,\n  0.3497052476465283,\n  0.34237169323483263,\n  0.31922445064978383,\n  0.31581735428312013,\n  0.291537828621234,\n  0.2964211914070448,\n  0.28709497524425703,\n  0.27664326199134304,\n  0.2649279448417684,\n  0.24511394909914588,\n  0.2548074126238488,\n  0.2328908924115631,\n  0.23151278675522277,\n  0.21887790198628423,\n  0.22120987124742428,\n  0.19939092685213478,\n  0.20033031164809215,\n  0.21924440801590078,\n  0.19624090372650524,\n  0.20542978161287692])\n\n\n\nF.relu\n\n&lt;function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -&gt; torch.Tensor&gt;\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=10, eta=0.0001, test_data=test_loader)\n\nTest Accuracy: 85.02%\nEpoch 1: Avg loss: 1.6808 | Test Accuracy: 85.02% | Grad Norm: 0.5848\nTest Accuracy: 85.27%\nEpoch 2: Avg loss: 1.6110 | Test Accuracy: 85.27% | Grad Norm: 0.5258\nTest Accuracy: 85.48%\nEpoch 3: Avg loss: 1.6093 | Test Accuracy: 85.48% | Grad Norm: 0.5570\nTest Accuracy: 88.00%\nEpoch 4: Avg loss: 1.5914 | Test Accuracy: 88.00% | Grad Norm: 1.5017\nTest Accuracy: 88.52%\nEpoch 5: Avg loss: 1.5793 | Test Accuracy: 88.52% | Grad Norm: 1.2540\nTest Accuracy: 89.07%\nEpoch 6: Avg loss: 1.5742 | Test Accuracy: 89.07% | Grad Norm: 1.1259\nTest Accuracy: 89.11%\nEpoch 7: Avg loss: 1.5696 | Test Accuracy: 89.11% | Grad Norm: 1.1550\nTest Accuracy: 89.61%\nEpoch 8: Avg loss: 1.5660 | Test Accuracy: 89.61% | Grad Norm: 1.1759\nTest Accuracy: 89.94%\nEpoch 9: Avg loss: 1.5631 | Test Accuracy: 89.94% | Grad Norm: 1.0710\nTest Accuracy: 90.16%\nEpoch 10: Avg loss: 1.5595 | Test Accuracy: 90.16% | Grad Norm: 1.0280\n\n\n([1.6808435128529866,\n  1.6109866456190745,\n  1.6093079151511192,\n  1.5913694785237313,\n  1.5792830767432848,\n  1.5742242393096288,\n  1.5695668534239133,\n  1.5659935484925906,\n  1.5630642044941585,\n  1.559465788523356],\n [85.02, 85.27, 85.48, 88.0, 88.52, 89.07, 89.11, 89.61, 89.94, 90.16],\n [0.5847676788903353,\n  0.5258062318293325,\n  0.5569941287694299,\n  1.501659070342197,\n  1.2540426928454615,\n  1.1258879237975783,\n  1.155011085806159,\n  1.1759249319730467,\n  1.0709939167230804,\n  1.0279926092156686])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=10, eta=0.00005, test_data=test_loader)\n\nTest Accuracy: 90.29%\nEpoch 1: Avg loss: 1.5576 | Test Accuracy: 90.29% | Grad Norm: 1.1653\nTest Accuracy: 90.37%\nEpoch 2: Avg loss: 1.5557 | Test Accuracy: 90.37% | Grad Norm: 1.0235\nTest Accuracy: 90.46%\nEpoch 3: Avg loss: 1.5549 | Test Accuracy: 90.46% | Grad Norm: 0.8970\nTest Accuracy: 90.42%\nEpoch 4: Avg loss: 1.5545 | Test Accuracy: 90.42% | Grad Norm: 1.0247\nTest Accuracy: 90.72%\nEpoch 5: Avg loss: 1.5531 | Test Accuracy: 90.72% | Grad Norm: 1.0628\nTest Accuracy: 90.88%\nEpoch 6: Avg loss: 1.5511 | Test Accuracy: 90.88% | Grad Norm: 0.9794\nTest Accuracy: 90.90%\nEpoch 7: Avg loss: 1.5503 | Test Accuracy: 90.90% | Grad Norm: 0.9258\nTest Accuracy: 90.93%\nEpoch 8: Avg loss: 1.5495 | Test Accuracy: 90.93% | Grad Norm: 1.0355\nTest Accuracy: 91.01%\nEpoch 9: Avg loss: 1.5490 | Test Accuracy: 91.01% | Grad Norm: 0.9989\nTest Accuracy: 91.06%\nEpoch 10: Avg loss: 1.5484 | Test Accuracy: 91.06% | Grad Norm: 0.8950\n\n\n([1.5576463149785995,\n  1.5557187402844428,\n  1.5549181991020837,\n  1.5544501681526501,\n  1.5530901376008988,\n  1.5511410237153371,\n  1.5503380763729413,\n  1.5494966670076051,\n  1.5490306969483694,\n  1.5483644265333811],\n [90.29, 90.37, 90.46, 90.42, 90.72, 90.88, 90.9, 90.93, 91.01, 91.06],\n [1.1653370030086032,\n  1.0234840901001925,\n  0.8970488051590173,\n  1.0247076750978872,\n  1.0628498398400164,\n  0.9794491940609777,\n  0.9257728102990177,\n  1.0355127711586682,\n  0.9988974580161624,\n  0.8950070320304887])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=30, eta=0.00005, test_data=test_loader)\n\nTest Accuracy: 91.10%\nEpoch 1: Avg loss: 1.5476 | Test Accuracy: 91.10% | Grad Norm: 0.8429\nTest Accuracy: 91.20%\nEpoch 2: Avg loss: 1.5472 | Test Accuracy: 91.20% | Grad Norm: 0.7848\nTest Accuracy: 91.24%\nEpoch 3: Avg loss: 1.5463 | Test Accuracy: 91.24% | Grad Norm: 0.9697\nTest Accuracy: 91.33%\nEpoch 4: Avg loss: 1.5453 | Test Accuracy: 91.33% | Grad Norm: 0.8874\nTest Accuracy: 91.39%\nEpoch 5: Avg loss: 1.5446 | Test Accuracy: 91.39% | Grad Norm: 1.0315\nTest Accuracy: 91.49%\nEpoch 6: Avg loss: 1.5441 | Test Accuracy: 91.49% | Grad Norm: 0.8475\nTest Accuracy: 91.49%\nEpoch 7: Avg loss: 1.5437 | Test Accuracy: 91.49% | Grad Norm: 0.8058\nTest Accuracy: 91.52%\nEpoch 8: Avg loss: 1.5442 | Test Accuracy: 91.52% | Grad Norm: 0.8884\nTest Accuracy: 91.49%\nEpoch 9: Avg loss: 1.5430 | Test Accuracy: 91.49% | Grad Norm: 0.8100\nTest Accuracy: 91.58%\nEpoch 10: Avg loss: 1.5420 | Test Accuracy: 91.58% | Grad Norm: 0.7816\n\n\n([1.5476057616670926,\n  1.5471696167389553,\n  1.5463465830485026,\n  1.5452923675576846,\n  1.5446079933842023,\n  1.54406529823939,\n  1.5436700760324795,\n  1.5442081407904624,\n  1.543009335021178,\n  1.542019239405791],\n [91.1, 91.2, 91.24, 91.33, 91.39, 91.49, 91.49, 91.52, 91.49, 91.58],\n [0.8428987947043185,\n  0.7847833687839821,\n  0.9697343087016209,\n  0.8873604827569046,\n  1.0314763956363955,\n  0.8475186883702619,\n  0.8057847412439724,\n  0.8884064122035553,\n  0.8100050882316191,\n  0.7816337433262337])\n\n\n\ntrain(net, train_loader,epochs=10, mini_batch_size=30, eta=0.00002, test_data=test_loader)\n\nTest Accuracy: 91.65%\nEpoch 1: Avg loss: 1.5409 | Test Accuracy: 91.65% | Grad Norm: 0.8363\nTest Accuracy: 91.67%\nEpoch 2: Avg loss: 1.5406 | Test Accuracy: 91.67% | Grad Norm: 0.6521\nTest Accuracy: 91.60%\nEpoch 3: Avg loss: 1.5404 | Test Accuracy: 91.60% | Grad Norm: 0.7502\nTest Accuracy: 91.67%\nEpoch 4: Avg loss: 1.5399 | Test Accuracy: 91.67% | Grad Norm: 0.7862\nTest Accuracy: 91.72%\nEpoch 5: Avg loss: 1.5399 | Test Accuracy: 91.72% | Grad Norm: 0.5888\nTest Accuracy: 91.76%\nEpoch 6: Avg loss: 1.5393 | Test Accuracy: 91.76% | Grad Norm: 0.8160\nTest Accuracy: 91.75%\nEpoch 7: Avg loss: 1.5393 | Test Accuracy: 91.75% | Grad Norm: 0.6232\nTest Accuracy: 91.75%\nEpoch 8: Avg loss: 1.5391 | Test Accuracy: 91.75% | Grad Norm: 0.7121\nTest Accuracy: 91.72%\nEpoch 9: Avg loss: 1.5389 | Test Accuracy: 91.72% | Grad Norm: 0.6047\nTest Accuracy: 91.77%\nEpoch 10: Avg loss: 1.5387 | Test Accuracy: 91.77% | Grad Norm: 0.6256\n\n\n([1.5408721971710524,\n  1.5405561513702075,\n  1.5404061170220376,\n  1.539928627272447,\n  1.5399407841563224,\n  1.539325503985087,\n  1.539281131406625,\n  1.5390899473627409,\n  1.5388786518971125,\n  1.5387439164320629],\n [91.65, 91.67, 91.6, 91.67, 91.72, 91.76, 91.75, 91.75, 91.72, 91.77],\n [0.8363055294336015,\n  0.6521481247455782,\n  0.7501852165585575,\n  0.7862274248023394,\n  0.588820192058458,\n  0.8159551650568143,\n  0.6232069042345509,\n  0.7121345371288179,\n  0.6046759387407957,\n  0.6256342942342423])\n\n\n\n?optim.Adam\n\n\nInit signature:\noptim.Adam(\n    params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]],\n    lr: Union[float, torch.Tensor] = 0.001,\n    betas: Tuple[float, float] = (0.9, 0.999),\n    eps: float = 1e-08,\n    weight_decay: float = 0,\n    amsgrad: bool = False,\n    *,\n    foreach: Optional[bool] = None,\n    maximize: bool = False,\n    capturable: bool = False,\n    differentiable: bool = False,\n    fused: Optional[bool] = None,\n)\nDocstring:     \nImplements Adam algorithm.\n.. math::\n   \\begin{aligned}\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n        &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n            \\:\\textit{maximize}                                                              \\\\\n        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n            v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                                 \\\\\n        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n        &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n        &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max},\n            \\widehat{v_t})                                                                   \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n        &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n   \\end{aligned}\nFor further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\nArgs:\n    params (iterable): iterable of parameters to optimize or dicts defining\n        parameter groups\n    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n        is not yet supported for all our implementations. Please use a float\n        LR if you are not also specifying fused=True or capturable=True.\n    betas (Tuple[float, float], optional): coefficients used for computing\n        running averages of gradient and its square (default: (0.9, 0.999))\n    eps (float, optional): term added to the denominator to improve\n        numerical stability (default: 1e-8)\n    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n        algorithm from the paper `On the Convergence of Adam and Beyond`_\n        (default: False)\n    foreach (bool, optional): whether foreach implementation of optimizer\n        is used. If unspecified by the user (so foreach is None), we will try to use\n        foreach over the for-loop implementation on CUDA, since it is usually\n        significantly more performant. Note that the foreach implementation uses\n        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n        parameters through the optimizer at a time or switch this flag to False (default: None)\n    maximize (bool, optional): maximize the objective with respect to the\n        params, instead of minimizing (default: False)\n    capturable (bool, optional): whether this instance is safe to\n        capture in a CUDA graph. Passing True can impair ungraphed performance,\n        so if you don't intend to graph capture this instance, leave it False\n        (default: False)\n    differentiable (bool, optional): whether autograd should\n        occur through the optimizer step in training. Otherwise, the step()\n        function runs in a torch.no_grad() context. Setting to True can impair\n        performance, so leave it False if you don't intend to run autograd\n        through this instance (default: False)\n    fused (bool, optional): whether the fused implementation is used.\n        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n        are supported. (default: None)\n.. note:: The foreach and fused implementations are typically faster than the for-loop,\n          single-tensor implementation, with fused being theoretically fastest with both\n          vertical and horizontal fusion. As such, if the user has not specified either\n          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n          implementation is relatively new, we want to give it sufficient bake-in time.\n          To specify fused, pass True for fused. To force running the for-loop\n          implementation, pass False for either foreach or fused. \n.. Note::\n    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n.. _Adam\\: A Method for Stochastic Optimization:\n    https://arxiv.org/abs/1412.6980\n.. _On the Convergence of Adam and Beyond:\n    https://openreview.net/forum?id=ryQu7f-RZ\nFile:           ~/miniforge3/envs/stan/lib/python3.11/site-packages/torch/optim/adam.py\nType:           type\nSubclasses:     \n\n\n\n\n0.9**50\n\n0.00515377520732012"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#week-summary",
    "href": "meetups/Meetup-11/meetup-11.html#week-summary",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Week Summary",
    "text": "Week Summary\n\nSpring Break Next week\nHW Due in the week after spring break\nReading: 7.1, 7.2, 8.6\nI’ll read your proposals and give some quick feedback"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#ny-open-statistical-computing-meetup",
    "href": "meetups/Meetup-11/meetup-11.html#ny-open-statistical-computing-meetup",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "NY Open Statistical Computing Meetup",
    "text": "NY Open Statistical Computing Meetup\n\nApril 8th 6:30-8:30+\nGreat Networking/Learning Opportunity"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#parametric-statistics",
    "href": "meetups/Meetup-11/meetup-11.html#parametric-statistics",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Parametric Statistics",
    "text": "Parametric Statistics\n\nParametric statistical models assume data generated by a probability distribution that depends on parameters\n\nLinear regression: \\[\ny_i| \\sim \\mathrm{Normal}(\\theta^T\\mathbf{x}_i + y_0,\\sigma)\n\\]\nHere the parameters are \\(\\theta\\), \\(y_0\\), and \\(\\sigma\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#maximum-likelihood",
    "href": "meetups/Meetup-11/meetup-11.html#maximum-likelihood",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nLikelihood is probability of observations given model coefficients \\[\np(\\mathbf{y}|\\theta,\\mathbf{x}) = \\prod_{i=1}^n p(y_i|\\theta,\\mathbf{x}_i)\n\\]\nUsually work with \\(l(\\mathbf{y},\\mathbf{\\theta}) = \\log(p(\\mathbf{y}|\\theta,\\mathbf{x}))\\): \\[\nl(\\mathbf{y},\\theta) = \\sum_{i=1}^n \\log(p(y_i|\\theta,\\mathbf{x}_i))\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#maximum-likelihood-1",
    "href": "meetups/Meetup-11/meetup-11.html#maximum-likelihood-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\nIf \\(p\\) is a log-concave in \\(\\theta\\) for fixed \\(\\mathbf{y}\\), \\(\\mathbf{x}\\), maximum likelihood is a convex optimization problem:\n\n\\[\n\\mathrm{min}_{\\theta}\\,  -\\sum_{i=1}^n \\log(p(y_i|\\theta,\\mathbf{x}_i))\n\\]\n\nIf \\(p\\) is a log-concave function of \\(\\theta\\), MLE problem is convex"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#link-to-penalty-functions",
    "href": "meetups/Meetup-11/meetup-11.html#link-to-penalty-functions",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Link to Penalty Functions",
    "text": "Link to Penalty Functions\n\nLast lecture: \\[\n\\min_{\\theta} \\sum_{i=1}^n \\phi(x,\\theta)\n\\]\nHere \\(\\phi(\\theta) = \\log(p(y_i|\\theta,\\mathbf{x}_i))\\)\nEach penalty corresponds to a probability model \\[\np(x|\\theta)  = \\frac{e^{-\\phi(x,\\theta)}}{\\int dx' e^{-\\phi(x',\\theta)}}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#probability-and-penalty",
    "href": "meetups/Meetup-11/meetup-11.html#probability-and-penalty",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Probability and Penalty",
    "text": "Probability and Penalty\n\nSquared Residuals corresponds to Gaussian Noise\n\n\\(p(y) = \\frac{e^{-y^2/(2\\sigma^2)}}{\\sqrt{2\\pi\\sigma^2}}\\)\n\nSum of absolute value of residuals (\\(L_1\\)) corresponds to Laplace noise:\n\n\\(p(y) = \\frac{e^{-|y|/a}}{2a}\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#convexity-of-mle",
    "href": "meetups/Meetup-11/meetup-11.html#convexity-of-mle",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Convexity of MLE",
    "text": "Convexity of MLE\n\nFor Gaussian: \\[\n-l(\\theta) = \\left(\\frac{n}{2}\\right)\\log(2\\pi\\sigma^2) + \\frac{\\| \\theta^T X -\\mathbf{y}\\|^2}{2\\sigma^2}\n\\]\nNot convex except if \\(\\sigma\\) fixed\nIf \\(\\theta\\) is fixed is convex in \\(1/\\sigma^2\\)\nCan generalize to covariance estimation"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#covariance-estimation",
    "href": "meetups/Meetup-11/meetup-11.html#covariance-estimation",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Covariance Estimation",
    "text": "Covariance Estimation\n\nHard example in 7.1\nHave many \\(\\mathbf{x}_i\\) of variables, want to estimate covariance under a normal model \\[\n-l(\\Sigma) = \\frac{N}{2}\\log(\\mathrm{det}\\Sigma) + \\frac{1}{2}\\sum_{i=1}^N \\mathbf{x}_i^T\\Sigma^{-1}\\mathbf{x}_i\n\\]\nOnly convex in variable \\(S = \\Sigma^{-1}\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#prior-knowledge",
    "href": "meetups/Meetup-11/meetup-11.html#prior-knowledge",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Prior Knowledge",
    "text": "Prior Knowledge\n\nCan incorporate constraints on parameters as a type of prior knowledge\nFor example, if the parameters had been previously estimated to within a given accuracy\nFor example, trying to learn if a coin is fair or not, \\(p_1\\) is probability of heads\n\n\\(p_1 \\sim \\mathrm{Beta}(5,5)\\)\nYou think coin might not be fair"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#map-estimation",
    "href": "meetups/Meetup-11/meetup-11.html#map-estimation",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "MAP Estimation",
    "text": "MAP Estimation\n\nCan take prior idea even further and start with a Bayesian model\nObservations \\(y\\) and model parameters \\(\\theta\\)\n\\(p(\\theta|y)\\) is probability of parameters \\(\\theta\\) given observations \\(y\\)\nProbability model: \\(p(y|\\theta)\\)\nAlso have prior beliefs \\(p(\\theta)\\)\nBayes Theorem: \\[\np(x|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#map-estimation-1",
    "href": "meetups/Meetup-11/meetup-11.html#map-estimation-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "MAP Estimation",
    "text": "MAP Estimation\n\nHarder problem: Calculate samples of posterior \\(p(\\theta|y)\\)\nSimpler problem: Find value of \\(\\theta\\) that maximimizes \\(p(\\theta,y)\\)\nMaximum a-Posteriori Probability: \\[\n\\mathrm{max}_{\\theta}\\, p(y|\\theta)p(\\theta)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#map-estimation-2",
    "href": "meetups/Meetup-11/meetup-11.html#map-estimation-2",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "MAP Estimation",
    "text": "MAP Estimation\n\nCan take log \\[\n\\mathrm{max}_{\\theta}\\,\\, \\log\\left(p(y|\\theta)\\right) +\\log\\left(p(\\theta)\\right)\n\\]\nIf \\(p(y|\\theta)\\) and \\(p(\\theta)\\) are log-concave, can solve\nEquivalent to maximum likelihood if prior is flat\nPrior looks like a regularization term"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#non-parametric-discrete",
    "href": "meetups/Meetup-11/meetup-11.html#non-parametric-discrete",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Non-Parametric Discrete",
    "text": "Non-Parametric Discrete\n\nConsider an unknown probability distribution \\(p_i\\) over random variable \\(X\\) with values \\(x_i\\)\nObservable/Feature is a function \\(f\\) of \\(x_i\\)\n\\(Ef(X) = \\sum_i f(x_i)p_i\\) describes a convex set\n\nMean, Variance, probability greater than a threshold"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#entropy",
    "href": "meetups/Meetup-11/meetup-11.html#entropy",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Entropy",
    "text": "Entropy\n\nEntropy of a distribution/RV is defined as: \\[\nH(X) = E(-\\log(p(x))) = \\sum_i -p_i\\log(p_i)\n\\]\nConvex in \\(p_i\\)s\nEntropy stands for the amount of uncertainty contained in the random variable \\(X\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#maximum-entropy",
    "href": "meetups/Meetup-11/meetup-11.html#maximum-entropy",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Maximum Entropy",
    "text": "Maximum Entropy\n\nSuppose you have some measurements/knowledge about a random variable \\(X\\)\n\nCould be from expected values of observables\n\\(Ef_k(X) = y_k\\)\n\nMaximum Entropy probability distribution:\n\n\\[\n\\mathrm{min}_{p_i} \\sum_i p_i\\log(p_i) \\\\\n\\sum_{i} f_k(x_i)p_i = y_k\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#maximum-entropy-in-practice",
    "href": "meetups/Meetup-11/meetup-11.html#maximum-entropy-in-practice",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Maximum Entropy in Practice",
    "text": "Maximum Entropy in Practice\n\nEnormously useful modeling framework\nApplication uses ideas from convex optimization\nConverts from measurements to prediction while adding least additional information \\[\np(x) = \\exp\\left(\\sum_{i} \\lambda_i f_i(x)\\right) \\\\\n\\mathrm{min}_{\\lambda} -\\sum_i a_i \\lambda_i + \\int dx p(x|\\lambda)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#kl-divergence",
    "href": "meetups/Meetup-11/meetup-11.html#kl-divergence",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "KL Divergence",
    "text": "KL Divergence\n\nSuppose you have two probability distributions \\(p\\) and \\(q\\)\nThe “surprise” we experience when we use \\(q\\) as a model for a process driven by \\(p\\) is given by the KL-Divergence: \\[\nDL(p||q) = \\sum_i p_i \\log(p_i/q_i)\n\\]\nCommon tool used for model comparison\n\nImagine \\(p_i\\) is true distribution\n\nConvex loss function used in machine learning"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#classification",
    "href": "meetups/Meetup-11/meetup-11.html#classification",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Classification",
    "text": "Classification\n\nHave two sets of points \\(A = \\{\\mathbf{x}_{a1}, \\mathbf{x}_{a2}, \\mathbf{x}_{a3}, \\dots\\}\\) and \\(B = \\{\\mathbf{x}_{b1}, \\mathbf{x}_{b2}, \\mathbf{x}_{b3}, \\dots\\}\\)\nWant to find a function \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) that satisfies: \\[\nf(\\mathbf{x}) &lt; 0\\,\\, \\mathrm{if}\\,\\, \\mathbf{x} \\in A \\\\\nf(\\mathbf{x}) &gt; 0\\,\\, \\mathrm{if}\\,\\, \\mathbf{x} \\in B\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#classification-1",
    "href": "meetups/Meetup-11/meetup-11.html#classification-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Classification",
    "text": "Classification\n\nSuch an \\(f\\) classifies or discriminates the points of \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#classification-2",
    "href": "meetups/Meetup-11/meetup-11.html#classification-2",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Classification",
    "text": "Classification\n\nTo solve classification problem you need a hypothesis set \\(\\mathcal{H}\\) of functions \\(f\\)\nIf \\(\\mathcal{H}\\) too complex for dataset you cannot generalize\n\nImagine a family of functions which you can just specify to have any values\n\nLeads us to consider simple families \\(f\\) initially"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination",
    "text": "Linear Discrimination\n\n\n\nLinear discrimination:\n\\(f(\\mathbf{x}) = \\theta^T\\mathbf{x} + c_0\\)"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination Fails",
    "text": "Linear Discrimination Fails\n\nSets not always linearly separable"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-1",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination Fails",
    "text": "Linear Discrimination Fails\n\nSets not always linearly separable"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-2",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-2",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination Fails",
    "text": "Linear Discrimination Fails\n\nSets not always linearly separable"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-3",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-3",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination Fails",
    "text": "Linear Discrimination Fails\n\nNeed Convex Hull of sets to be disjoint"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-4",
    "href": "meetups/Meetup-11/meetup-11.html#linear-discrimination-fails-4",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Linear Discrimination Fails",
    "text": "Linear Discrimination Fails\n\nNeed Convex Hull of sets to be disjoint"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#support-vector-machines",
    "href": "meetups/Meetup-11/meetup-11.html#support-vector-machines",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\nWhat to do if can’t separate?\nLook for approximate separation\n\nMinimize misclassified points directly is too hard\n\nInstead look to see how bad misclassifications are: \\[\n\\theta^T\\mathbf{x}_{ai} + c_0 \\geq 1 - u_i \\\\\n\\theta^T\\mathbf{x}_{bj} + c_0 \\leq -(1-v_j)\n\\]\nIf the \\(u_i\\) or \\(v_j\\) are 0, classification successful"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#svm-margin",
    "href": "meetups/Meetup-11/meetup-11.html#svm-margin",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "SVM Margin",
    "text": "SVM Margin\n\nInequalities create marginal region\nSupport vectors are points inside"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#svm-optimization-problem",
    "href": "meetups/Meetup-11/meetup-11.html#svm-optimization-problem",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "SVM Optimization Problem",
    "text": "SVM Optimization Problem\n\nMaximize width of slab \\(\\frac{2}{\\|\\theta\\|_2}\\)\n\nSeparates points\n\nPenalize misclassified points\n\n\\(L_1\\) norm of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)\n\n\n\\[\n\\mathrm{min}_{\\mathbf{u},\\mathbf{v},\\theta,c_0} \\|\\theta\\|_2+\\gamma\\left(\\mathbf{1}^T\\mathbf{u} +\\mathbf{1}^T\\mathbf{v}\\right) \\\\\n\\theta^T\\mathbf{x}_{ai} + c_0 \\geq 1 - u_i\\ \\\\\n\\theta^T\\mathbf{x}_{bj} + c_0 \\leq -(1 - v_j)\\ \\\\\n\\mathbf{u} \\succeq 0\\,\\, \\mathbf{v} \\succeq 0\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#svm-optimization-problem-1",
    "href": "meetups/Meetup-11/meetup-11.html#svm-optimization-problem-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "SVM Optimization Problem",
    "text": "SVM Optimization Problem\n\\[\n\\mathrm{min}_{\\mathbf{u},\\mathbf{v},\\theta,c_0} \\|\\theta\\|_2+\\gamma\\left(\\mathbf{1}^T\\mathbf{u} +\\mathbf{1}^T\\mathbf{v}\\right) \\\\\n\\theta^T\\mathbf{x}_{ai} + c_0 \\geq 1 - u_i\\ \\\\\n\\theta^T\\mathbf{x}_{bj} + c_0 \\leq 1 - v_j\\ \\\\\n\\mathbf{u} \\succeq 0\\,\\, \\mathbf{v} \\succeq 0\n\\]\n\n\\(\\gamma\\) balances objective of slab width and misclassified points\nAffine plus norm and linear inequalities so convex"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#implementation",
    "href": "meetups/Meetup-11/meetup-11.html#implementation",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Implementation",
    "text": "Implementation\n\nReformulate a little\nEither \\(u_i=0\\) or \\(u_i = 1-\\theta^T\\mathbf{x}_{ai}-c_0\\)\nEither \\(v_j=0\\) or \\(v_j = 1-\\theta^T\\mathbf{x}_{bj}-c_0\\)\n\n\\[\n\\mathrm{min}_{\\theta,c_0} \\|\\theta\\|_2 + \\gamma\\left(\\sum_i\\mathrm{max}(0,1-\\theta^T\\mathbf{x}_{ai}-c_0) +\\\\\n\\sum_j\\mathrm{max}(0,1-\\theta^T\\mathbf{x}_{bj}-c_0)\\right)\n\\]"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#implementation-1",
    "href": "meetups/Meetup-11/meetup-11.html#implementation-1",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Implementation",
    "text": "Implementation\n\nimport cvxpy as cp\nn = 20\nm = 1000\ntheta = cp.Variable((n,1)) # coefficients\nc0 = cp.Variable() # offset\nloss = cp.sum(cp.pos(1 - cp.multiply(Y, X @ theta - c0))) # u and v\nreg = cp.norm(theta, 1) # width\nlambd = cp.Parameter(nonneg=True) # tradeoff\nprob = cp.Problem(cp.Minimize(loss/m + lambd*reg)) #\n\n\nRescaled loss by number of points\ncp.sum(cp.pos(1 - cp.multiply(Y, X @ theta - c0))) is called hinge-loss\n\\(Y = \\pm 1\\) is the class"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#testing-regularization",
    "href": "meetups/Meetup-11/meetup-11.html#testing-regularization",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Testing Regularization",
    "text": "Testing Regularization\n\n# Compute a trade-off curve and record train and test error.\nTRIALS = 100\ntrain_error = np.zeros(TRIALS)\ntest_error = np.zeros(TRIALS)\nlambda_vals = np.logspace(-2, 0, TRIALS)\ntheta_vals = []\nfor i in range(TRIALS):\n    lambd.value = lambda_vals[i]\n    prob.solve()\n    train_error[i] = (np.sign(X.dot(theta_true) + offset) != np.sign(X.dot(theta.value) - c0.value)).sum()/m\n    test_error[i] = (np.sign(X_test.dot(theta_true) + offset) != np.sign(X_test.dot(theta.value) - c0.value)).sum()/TEST\n    theta_vals.append(theta.value)\n\n\nHere test/train errors are percentage of points classified correctly"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#traintest-error",
    "href": "meetups/Meetup-11/meetup-11.html#traintest-error",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Train/Test Error",
    "text": "Train/Test Error"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#regularization-paths",
    "href": "meetups/Meetup-11/meetup-11.html#regularization-paths",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Regularization Paths",
    "text": "Regularization Paths\n\nfor i in range(n):\n    plt.plot(lambda_vals, [wi[i,0] for wi in theta_vals])\nplt.xlabel(r\"$\\lambda$\", fontsize=16)\nplt.xscale(\"log\")"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#do-more-with-svms",
    "href": "meetups/Meetup-11/meetup-11.html#do-more-with-svms",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Do more with SVMs",
    "text": "Do more with SVMs\n\nSVMs not restricted to linear separation\nCan use nonlinear functions to generate additional features\nLeads to higher dimensional space in which sometimes linear separator is more effective"
  },
  {
    "objectID": "meetups/Meetup-11/meetup-11.html#thanks",
    "href": "meetups/Meetup-11/meetup-11.html#thanks",
    "title": "DATA 609 Meetup 11: Application to Statistics and Machine Learning",
    "section": "Thanks!",
    "text": "Thanks!"
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "This website would not be possible without the quarto package. This course draws inspiration from several open educational resources on the internet, including two books and accompanying materials by Stephen Boyd and Lieven Vandenberghe: Introduction to Applied Linear Algebra and Convex Optimization",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "modules/module5.html",
    "href": "modules/module5.html",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\n\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning\nWe will use CVX for the first time in the next two weeks. If you have time go to https://www.cvxpy.org/tutorial/dcp/ or https://cvxr.rbind.io/cvxr_examples/cvxr_intro/ to begin learning the basics",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "modules/module5.html#homework",
    "href": "modules/module5.html#homework",
    "title": "Module 5 - Convex Sets",
    "section": "",
    "text": "Homework 3 is due in two weeks, Sunday at midnight\n\n\n\nDefinition of Affine and Convex Sets\nExamples of Convex Sets\n\n\n\n\n\nSections 2.1-2.3 of Convex Optimization\nSection 2.1 of Introduction to Algorithms for Machine Learning\nWe will use CVX for the first time in the next two weeks. If you have time go to https://www.cvxpy.org/tutorial/dcp/ or https://cvxr.rbind.io/cvxr_examples/cvxr_intro/ to begin learning the basics",
    "crumbs": [
      "Topics",
      "5 - Convex Sets"
    ]
  },
  {
    "objectID": "modules/module12.html",
    "href": "modules/module12.html",
    "title": "Module 12: Duality",
    "section": "",
    "text": "Learning Objectives\n\nDual Formulation of Optimization Problems\nLagrange Multipliers\nShadow Prices and Sensitivity Analysis\n\n\n\nReadings\nSections 5.1, 5.2, 5.4.3, 5.4.4, 5.5.2, 5.6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "12 - Duality and Classifiers"
    ]
  },
  {
    "objectID": "modules/module9.html",
    "href": "modules/module9.html",
    "title": "Module 9: Convex Optimization Problems and Disciplined Convex Programming",
    "section": "",
    "text": "Homework\nHomework 5 is due in two weeks, Sunday at midnight.\n\n\nLearning Objectives\n\nlog-concave functions\nProblem Transformations\nGeometric Programming\nVector Optimization and Scalarization\n\n\n\nReadings\nSections 4.1-4.2, 4.7, skim 4.3-4.6 of Convex Optimization\nContinue reading CVX users guide: -python -R -julia",
    "crumbs": [
      "Topics",
      "9 - Convex Optimization and Disciplined Convex Programming"
    ]
  },
  {
    "objectID": "modules/module10.html",
    "href": "modules/module10.html",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, interpolation, robust stochastic and worst case fitting.\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module10.html#homework",
    "href": "modules/module10.html#homework",
    "title": "Module 10 - Convex Problems in Data Fitting",
    "section": "",
    "text": "Homework 5 is due this Sunday at midnight\n\n\n\nRegularization- lasso and Huber penalties\nFunction fitting, interpolation, robust stochastic and worst case fitting.\n\n\n\n\nChapter 6 of Convex Optimization",
    "crumbs": [
      "Topics",
      "10 - Data Fitting"
    ]
  },
  {
    "objectID": "modules/module6.html",
    "href": "modules/module6.html",
    "title": "Module 6 - Constructing Convex Sets",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nOperations that preserve convexity\n\nIntersection\nCartesian Product\nAffine Transformation\nPerspective and Linear Fractional Transformation\n\nIntroduce Disciplined Convex Programming\n\n\n\n\n\nSections 2.3 of Convex Optimization\nStart looking at cvx user guides:\n\ncvxpy\ncvxr",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module6.html#homework",
    "href": "modules/module6.html#homework",
    "title": "Module 6 - Constructing Convex Sets",
    "section": "",
    "text": "Homework 3 is due this Sunday at midnight\n\n\n\nOperations that preserve convexity\n\nIntersection\nCartesian Product\nAffine Transformation\nPerspective and Linear Fractional Transformation\n\nIntroduce Disciplined Convex Programming\n\n\n\n\n\nSections 2.3 of Convex Optimization\nStart looking at cvx user guides:\n\ncvxpy\ncvxr",
    "crumbs": [
      "Topics",
      "6 - Convex Sets II"
    ]
  },
  {
    "objectID": "modules/module3.html",
    "href": "modules/module3.html",
    "title": "Module 3: Statistical Applications of Least Squares",
    "section": "",
    "text": "This week we will learn more about the applications of least squares methods in statistics. We will learn about how weighted least squares methods allow us to apply linear regression in more cases, including those with heteroscedastic errors. Then we will show how to fit data using interpolating functions, and autoregressive models for time series. This discussion will lead into feature engineerging. Finally, we will explore validation and regularization, which is our first example of multi-objective least squares.\n\nDeliverables\nHomework 2 will be due Sunday, February 23rd at midnight.\n\n\nLearning Objectives\n\nLeast Squares and Linear Regression\nGeneralized Least Squares\nRecursive/Online Least Squares\nRegularization and Multi-Objective Least Squares\n\n\n\nReadings\nChapters 13 of Introduction to Applied Linear Algebra\nChapter 14 (on classification methods) is interesting but we won’t cover it.\nSections 4.2 and 4.5 of Introduction to Algorithms for Data Mining and Machine Learning",
    "crumbs": [
      "Topics",
      "3 - Least Squares and Statistics"
    ]
  },
  {
    "objectID": "modules/module13.html",
    "href": "modules/module13.html",
    "title": "Module 13: Local Optimization Algorithms",
    "section": "",
    "text": "Homework\nHomework 7 is due in two weeks, Sunday at midnight\n\n\nLearning Objectives\n\nNeural Network Learning\nGradient Descent\nConvergence Rates and Condition Number\nAcceleration Using Momentum\n\n\n\nReadings\n\nChapter 8.1 to 8.4 of Intro to Optimization Algorithms\nWikipedia article on SGD Covers everything but good level\nPankaj Mehta’s Notebook I followed his notebook partially here\nDeep Learning Book 8.3 Parts of this chapter are very advanced\nGilbert Strang’s Linear Algebra and Learning from Data VI.4 and VI.5 Advanced and Scatterbrained, covers too much, but has deep insights\nAccompanying Lectures 22-26 on this playlist",
    "crumbs": [
      "Topics",
      "13 - Local Optimization and Neural Networks"
    ]
  },
  {
    "objectID": "modules/module4.html",
    "href": "modules/module4.html",
    "title": "Module 4 - Constrained Least Squares and Applications",
    "section": "",
    "text": "Homework\nHomework 2 is due this Sunday at midnight\n\n\nLearning Objectives\n\nSolving Least Squares problems with Linear Equality Constraints\nApplications to Portfolio Optimization\nApplications to B-Splines\n\n\n\nReadings\nChapters 16 and 17 of Introduction to Applied Linear Algebra",
    "crumbs": [
      "Topics",
      "4 - Constrained Least Squares and Applications"
    ]
  },
  {
    "objectID": "posts/2025-03-04-Meetup-6-Slides.html",
    "href": "posts/2025-03-04-Meetup-6-Slides.html",
    "title": "Meetup 6: Introduction to Convex Sets",
    "section": "",
    "text": "I have uploaded the slides and video for Meetup 6, which happened last night.\nWe learned about rules for constructing convex sets using intersections, cartesian products, and transformations by affine, linear fractional, and perspective functions.\n\nClick here for slides\nWatch Meetup 6 Here"
  },
  {
    "objectID": "posts/2025-02-03-Meetup-2-Slides.html",
    "href": "posts/2025-02-03-Meetup-2-Slides.html",
    "title": "Meetup 2: Least Squares Optimization",
    "section": "",
    "text": "Meetup 2 is tonight at 6:45PM. We will introduce least squares optimization, which is the most commonly used and possibly most important type of optimization problem. We will go through a few applications to illustrate least squares in its simplest formulation, and discuss some practical issues with solving least squares problems.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 2 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-01-27-Meetup-1-Slides.html",
    "href": "posts/2025-01-27-Meetup-1-Slides.html",
    "title": "Meetup 1: Introduction to Optimization",
    "section": "",
    "text": "Meetup 1 is tonight at 6:45PM. We are going to talk about what optimization is, some of its applications, the course structure, and some very basic facts about its theory.\nHere is the information you need for the first meetup:\n\nGo through the course overview materials, and complete the week 1 readings.\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\nClick here for slides"
  },
  {
    "objectID": "posts/2025-04-07-Week-11-Stats-and-ML.html",
    "href": "posts/2025-04-07-Week-11-Stats-and-ML.html",
    "title": "Week 11- Stats and Machine Learning",
    "section": "",
    "text": "Welcome to Week 11. During this week we will cover the beginning of Chapter 7 of CVX-book, which focuses on both parametric and non-parametric statistics applications of convex optimization, and section 8.6 of CVX-book, which focuses on classication. We will revisit maximum likelihood methods and introduce maximum a posteriori methods. We will talk about entropy maximimization and the KL-divergence, and then introduce support vector machines for classification.\nYour sixth homework assignment is due three weeks from today, Sunday April 27th at midnight.\nHere are more details on what you need to do this week:\n\nGo through module 11 and complete the week 11 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nFinish working on Lab 6"
  },
  {
    "objectID": "posts/2025-02-16-Week-4-Constrained-Least-Squares.html",
    "href": "posts/2025-02-16-Week-4-Constrained-Least-Squares.html",
    "title": "Week 4- Constrained Least Squares",
    "section": "",
    "text": "This week we will learn how to deal with linear equality constraints in least squares optimization problems. There will be a discussion of Lagrange multipliers, and application examples will include B-splines and portfolio optimization. Constraints are a critical part of both optimization problems and modeling in general.\nYour second homework assignment will be due Sunday February 23rd.\nHere are more details on what you need to do this week:\n\nGo through module 4 and complete the week 4 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nContinue working on Lab 2"
  },
  {
    "objectID": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "href": "posts/2025-01-26-Welcome_to_DATA_609.html",
    "title": "Welcome to DATA 609",
    "section": "",
    "text": "Welcome to DATA 609, Math Modeling Techniques.\nClick this post now to get started!\nHere are the first steps:\n\nRead the syllabus.\nGo through the course overview materials, and complete the week 1 readings.\nJoin our Slack channel:\n\nAttend our first meetup on Monday at 6:45 p.m. ET on Zoom\n\nI recorded a short video which introduces me and gives you more background on the course:\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-02-Week-2-Least-Squares.html",
    "href": "posts/2025-02-02-Week-2-Least-Squares.html",
    "title": "Week 2- Least Squares",
    "section": "",
    "text": "This week we will talk about the most basic and most important class of optimization problems, least squares optimization. Whenever you fit a linear regression, you are solving a least squares problems, and least squares methods have applications across all quantitative fields. Although least squares methods are simple, they should not be dismissed- the fact that we can solve them rapidly, reliably means that it is useful to know how to formulate them in applied situations and how to solve them.\nIn this week we will go over the basics and a few simple applications, while the two weeks after this will explore data fitting examples and how to handle more complicated cases.\nYour first homework assignment will be due next Sunday.\nHere are more details on what you need to do this week:\n\nGo through module 2 and complete the week 2 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nContinue working on Lab 1"
  },
  {
    "objectID": "posts/2025-04-21-Meetup-12-Duality.html",
    "href": "posts/2025-04-21-Meetup-12-Duality.html",
    "title": "Meetup 12: Duality",
    "section": "",
    "text": "I have uploaded the slides for Meetup 12\nWe are going to talk about the concept of duality and how to use it to learn about your optimization problem.\n\nClick here for slides\nClick here to watch the meetup video"
  },
  {
    "objectID": "posts/2025-04-07-Meetup-11-Statistics.html",
    "href": "posts/2025-04-07-Meetup-11-Statistics.html",
    "title": "Meetup 11: Statistics",
    "section": "",
    "text": "I have uploaded the slides for Meetup 11\nWe are going to talk about applications of convex optimization to statistics and classification.\n\nClick here for slides\nClick here to watch the meetup video"
  },
  {
    "objectID": "posts/2025-03-24-Meetup-9-Slides.html",
    "href": "posts/2025-03-24-Meetup-9-Slides.html",
    "title": "Meetup 9: Convex Optimization Problems",
    "section": "",
    "text": "I have uploaded the slides for Meetup 9.\nWe will discuss log-concave functions, a topic which was missed the past two weeks, and then discuss methods for transforming optimization problems, geometric programs, and multicriterion optimization.\n\nClick here for slides"
  },
  {
    "objectID": "posts/2025-02-10-Data-Fitting.html",
    "href": "posts/2025-02-10-Data-Fitting.html",
    "title": "Week 3- Data Fitting",
    "section": "",
    "text": "This week, we will discuss applications of least squares methods to statistics and data fitting. This will include weighted least squares methods for linear regression, function interpolation, auto-regressive models, and feature engineering, and generalization/regularization.\nYour second homework assignment will be due Sunday February 23rd.\nHere are more details on what you need to do this week:\n\nGo through module 3 and complete the week 3 readings.\nAttend our second meetup on Monday at 6:45 p.m. ET on Zoom\nBegin working on Lab 2"
  },
  {
    "objectID": "posts/2025-03-18-Meetup-8-Slides.html",
    "href": "posts/2025-03-18-Meetup-8-Slides.html",
    "title": "Meetup 8: Constructing Convex Functions",
    "section": "",
    "text": "I have uploaded the slides for Meetup 8.\nWe discussed rules for constructing convex functions and showing that functions are convex. In particular, we learned that positive weighted sums of convex functions are convex (both finite and infinite), that this extends to integration, and that maximization of convex functions (both finite and infinite) both preserve convexity. Then we discussed composition, presenting both scalar and vector composition rules.\n\nClick here for slides\nWatch Meetup 8 Here"
  },
  {
    "objectID": "posts/2025-02-14-Linear-Algebra-In_R.html",
    "href": "posts/2025-02-14-Linear-Algebra-In_R.html",
    "title": "Coding Vignette- Linear Algebra in R",
    "section": "",
    "text": "For those of you that prefer to work in R, I decided to make a video going through the basics of matrix manipulations and linear algebra in R. At the end I redo one of the examples from the last meetup. Although matrices aren’t commonly used in introductory R classes, they do become quite prominent for more advanced applications (consider tidymodels which can return matrices where the entries are different models).\nClick here to watch the video on youtube\nThe linear algebra routines are all in base R: Click Here for a Tutorial\nI have also included the R script with the code that I entered"
  },
  {
    "objectID": "posts/2025-02-17-Meetup-4-Slides.html",
    "href": "posts/2025-02-17-Meetup-4-Slides.html",
    "title": "Meetup 4: Least Squares Data Fitting",
    "section": "",
    "text": "I have uploaded the slides for Meetup 4, which happened tonight at 6:45PM\nWe discussed constrained least squares and went through a detailed example that showed how to fit B-splines to data using that technique.\n\nClick here for slides\nClick here"
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Instructor: George Hagstrom, Ph.D. Class Meetup: Monday 7:00-8:00 Eastern Office Hours: By appointment Email: george.hagstrom@cuny.edu\n\nDescription\nOptimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.\nIn this course you will learn optimization theory through its practical applications to statistics and machine learning. The first half of the course will cover convex optimization (including least squares and linear programming as special cases), and the second half will cover methods for non-convex problems, primarily stochastic gradient descent and Markov-Chain Monte Carlo Methods. Applications will include large-scale linear and logistic regression, regularization, maximum likelihood methods, support vector machines, neural networks, Bayesian statistics, and optimization problems arising in an operations research or other business context.\n\n\nCourse Learning Outcomes\nBy the end of the course, students should be able to:\n\nUnderstand how to formulate major statistical and machine learning algorithms as optimization problems\nLearn how to recognize and solve least squares, linear programming, and convex optimization problems.\nLearn how to represent convex optimization problems in the CVX package\nUnderstand the basics of algorithmic complexity theory and use it to understand how quickly different algorithms will converge to the solution of an optimization problem\nImplement stochastic gradient descent for neural networks and other non-convex problems, understand trade-offs in algorithm design\n\n\n\nProgram Learning Outcomes\n\nBusiness Understanding. Apply frameworks and processes to build out data analytics solutions from understanding of business goals.\nSolid foundational data programming skills, using industry standard tools, essential algorithms, and design patterns for working with structured data, unstructured data and big data.\nSolid foundational math and statistics skills, with emphasis on linear algebra, probability, Bayesian statistics, and numerical methods.\nData understanding. Collect, describe, model, explore and verify data.\nData preparation. Selecting, cleaning, constructing, integrating, and formatting data.\nOptimization Modeling. Selecting optimization modeling techniques, generating test designs, building and assessing models.\nModel implementation and deployment.\nCommunicating results.\n\n\n\nGrading\n\nLabs (80%)\nProject (20%)\n\n\nGrade Distribution\n\n\n\n\n\n\n\n\n\nQuality of Performance\nLetter Grade\nRange %\nGPA\n\n\n\n\nExcellent - work is of exceptional quality\nA\n93 - 100\n4\n\n\nExcellent\nA-\n90 - 92.9\n3.7\n\n\nGood - work is above average\nB+\n87 - 89.9\n3.3\n\n\nSatisfactory\nB\n83 - 86.9\n3\n\n\nBelow Average\nB-\n80 - 82.9\n2.7\n\n\nPoor\nC+\n77 - 79.9\n2.3\n\n\nPoor\nC\n70 - 76.9\n2\n\n\nFailure\nF\n&lt; 70\n0\n\n\n\n\n\n\nHow This Course Works\nThis course is conducted entirely online. Each week, you will have various resources made available, including weekly readings from the textbooks and occasionally additional readings provided by the instructor. A homework assignment will be due every other week, see the schedule for details). There will also be a final project required. You are expected to complete all assignments by their due dates.\nYou are expected to attend or watch every Meetup. I highly recommend attending the Meetups live if possible but understand that may not be possible for everyone. Recordings will be made available by the next morning on the Schedule page. In addition to highlighting key concepts from each learning module, some topics will be discussed that are not in the textbook. Moreover, we regularly make announcements in the Meetups that will be important to being successful in this course.\n\n\nTextbooks and Course Materials\nThis course makes use of several textbooks. I have attempted when possible to choose resources which are freely available.\n\nStephen Boyd and Lieven Vandenberghe. Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares\nLieven Vandenberghe and Stephen Boyd. Convex Optimization\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning\n\n\nOptional\n\nYang Xin-She. Introduction to algorithms for data mining and machine learning. Academic press, 2019.\nDavid Mackay Information Theory, Inference, and Learning Algorithms\n\n\n\n\nAccessibility and Accommodations\nThe CUNY School of Professional Studies is firmly committed to making higher education accessible to students with disabilities by removing architectural barriers and providing programs and support services necessary for them to benefit from the instruction and resources of the University. Early planning is essential for many of the resources and accommodations provided. Please see: http://sps.cuny.edu/student_services/disabilityservices.html\n\n\nOnline Etiquette and Anti-Harassment Policy\nThe University strictly prohibits the use of University online resources or facilities, including Brightspace, for the purpose of harassment of any individual or for the posting of any material that is scandalous, libelous, offensive or otherwise against the University’s policies. Please see: http://media.sps.cuny.edu/filestore/8/4/9_d018dae29d76f89/849_3c7d075b32c268e.pdf\n\n\nAcademic Integrity\nAcademic dishonesty is unacceptable and will not be tolerated. Cheating, forgery, plagiarism and collusion in dishonest acts undermine the educational mission of the City University of New York and the students’ personal and intellectual growth. Please see: http://media.sps.cuny.edu/filestore/8/3/9_dea303d5822ab91/839_1753cee9c9d90e9.pdf\n\n\nStudent Support Services\nIf you need any additional help, please visit Student Support Services: http://sps.cuny.edu/student_resources/",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "DATA 609 Spring 2025",
    "section": "",
    "text": "title: “DATA609 - Math Modeling Techniques” editor_options: chunk_output_type: console",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#meetup-link",
    "href": "course/schedule.html#meetup-link",
    "title": "DATA 609 Spring 2025",
    "section": "Meetup Link:",
    "text": "Meetup Link:\nClick Here to Join the Meetups on Zoom\n\nCourse Schedule\n\n\nDate\nStart Time\nModule\nSlides\nVideo\n\n\n\n\nJan 27\n06:45PM\nIntroduction to Optimization Theory\nMeetup 1 Slides\nMeetup 1 Video\n\n\nFeb 3\n06:45PM\nLeast Squares Optimization\nMeetup 2 Slides\nMeetup 2 Video\n\n\nFeb 10\n06:45PM\nLeast Squares and Statistics\nMeetup 3 Slides\nMeetup 3 Video\n\n\nFeb 17\n06:45PM\nConstrained Least Squares and Applications\nMeetup 4 Slides\nMeetup 4 Video\n\n\nFeb 24\n06:45PM\nConvex Sets\nMeetup 5 Slides\nMeetup 5 Video\n\n\nMar 3\n06:45PM\nConvex Sets II\nMeetup 6 Slides\nMeetup 6 Video\n\n\nMar 10\n06:45PM\nConvex Functions\nMeetup 7 Slides\nMeetup 7 Video\n\n\nMar 17\n06:45PM\nConvex Functions II\nMeetup 8 Slides\nMeetup 8 Video\n\n\nMar 24\n06:45M\nConvex Optimization Problems\nMeetup 9 Slides\nMeetup 9 Video\n\n\nMar 31\n06:45PM\nData Fitting\nMeetup 10 Slides\nMeetup 10 Video\n\n\nApr 7\n06:45PM\nApplications to Statistics and Machine Learning\nMeetup 11 Slides\nMeetup 11 Video\n\n\nApr 14\n\nNo Meetup (Spring Break)\n\n\n\n\nApr 21\n06:45PM\nDuality\nMeetup 12 Slides\nMeetup 12 Video\n\n\nApr 28\n06:45PM\nGradient Descent and Neural Networks\nMeetup 13 Slides\nMeetup 13 Video\n\n\nMay 5\n06:45PM\nStochastic Gradient Descent\n\n\n\n\nMay 12\n06:45PM\nTraining Deep Neural Networks",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/overview.html",
    "href": "course/overview.html",
    "title": "DATA 609 - Math Modeling Techniques",
    "section": "",
    "text": "Optimization underlies nearly everything in statistics and machine learning, and has a wealth of applications across many different applied fields. Learning how to recognize and solve optimization problems will allow you to select the best algorithms to solve a given data science problem, give you a deeper understanding of statistics and machine learning tools, and enable you to find reformulations or approximations of problems that are substantially easier or more useful to solve.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  }
]